## ëª©ì°¨

* ëª©ì°¨
  * [1. ìŠ¤íŠ¸ë¦¬ë°ì˜ ê°œë…](#1-ìŠ¤íŠ¸ë¦¬ë°ì˜-ê°œë…)
  * [2. ìŠ¤íŠ¸ë¦¬ë°ì˜ í•„ìš”ì„±](#2-ìŠ¤íŠ¸ë¦¬ë°ì˜-í•„ìš”ì„±)
  * [3. ìŠ¤íŠ¸ë¦¬ë° êµ¬í˜„ ë°©ë²•](#3-ìŠ¤íŠ¸ë¦¬ë°-êµ¬í˜„-ë°©ë²•)
  * [4. ìŠ¤íŠ¸ë¦¬ë° ì‹¤ìŠµ](#4-ìŠ¤íŠ¸ë¦¬ë°-ì‹¤ìŠµ)
    * [4-1. ëª¨ë¸ ì§ì ‘ ìŠ¤íŠ¸ë¦¬ë°](#4-1-ëª¨ë¸-ì§ì ‘-ìŠ¤íŠ¸ë¦¬ë°)
    * [4-2. ì²´ì¸ ìŠ¤íŠ¸ë¦¬ë°](#4-2-ì²´ì¸-ìŠ¤íŠ¸ë¦¬ë°)
    * [4-3. ë¹„ë™ê¸° ìŠ¤íŠ¸ë¦¬ë°](#4-3-ë¹„ë™ê¸°-ìŠ¤íŠ¸ë¦¬ë°)
* ipynb ì‹¤ìŠµ íŒŒì¼
  * [ipynb ì‹¤ìŠµ íŒŒì¼](ipynb/LangChain_LLM_ì²´ì¸_ìŠ¤íŠ¸ë¦¬ë°.ipynb)

## 1. ìŠ¤íŠ¸ë¦¬ë°ì˜ ê°œë…

LangChainì—ì„œ **ìŠ¤íŠ¸ë¦¬ë° (Streaming)** ì€ LLMì˜ ë‹µë³€ì„ **token ë‹¨ìœ„ë¡œ ì‹¤ì‹œê°„ ìˆ˜ì‹ ** í•˜ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤.

## 2. ìŠ¤íŠ¸ë¦¬ë°ì˜ í•„ìš”ì„±

ìŠ¤íŠ¸ë¦¬ë°ì˜ í•„ìš”ì„±ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.

* ì‚¬ìš©ìê°€ ê¸´ ì‘ë‹µì„ ê¸°ë‹¤ë¦´ í•„ìš” ì—†ì´, **ì‚¬ìš©ìì—ê²Œ ë‹µë³€ì˜ tokenì´ ìƒì„±ë˜ëŠ” ì¦‰ì‹œ ì œê³µ** í•˜ì—¬ UX í–¥ìƒ
  * ChatGPT, Geminië¥¼ í¬í•¨í•œ ì±—ë´‡ì—ì„œëŠ” ìŠ¤íŠ¸ë¦¬ë°ì´ ê±°ì˜ í•„ìˆ˜

## 3. ìŠ¤íŠ¸ë¦¬ë° êµ¬í˜„ ë°©ë²•

ìŠ¤íŠ¸ë¦¬ë°ì˜ êµ¬í˜„ ë°©ë²•ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.

| êµ¬í˜„ ë°©ë²•      | ì„¤ëª…                              | Python ì½”ë“œ                                                           |
|------------|---------------------------------|---------------------------------------------------------------------|
| ëª¨ë¸ ì§ì ‘ ìŠ¤íŠ¸ë¦¬ë° | **ëª¨ë¸ì„ ì§ì ‘ í˜¸ì¶œ** í•  ë•Œ ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ ì‚¬ìš©    | ```llm.stream([HumanMessage(content="...")])```                     |
| ì²´ì¸ ìŠ¤íŠ¸ë¦¬ë°    | **LLM ì²´ì¸** ë°©ì‹ì„ ì‚¬ìš©í•  ë•Œ ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ ì‚¬ìš© | ```chain.stream({...})```                                           |
| ë¹„ë™ê¸° ìŠ¤íŠ¸ë¦¬ë°   | ë¹„ë™ê¸°ì ìœ¼ë¡œ ìŠ¤íŠ¸ë¦¬ë° êµ¬í˜„                  | ```async for chunk in llm.astream([HumanMessage(content="...")])``` |
| ì—ì´ì „íŠ¸ ìŠ¤íŠ¸ë¦¬ë°  | LLM ì—ì´ì „íŠ¸ì—ì„œì˜ ìŠ¤íŠ¸ë¦¬ë° êµ¬í˜„             | ```agent.stream({...}, stream_mode="updates")```                    |

## 4. ìŠ¤íŠ¸ë¦¬ë° ì‹¤ìŠµ

### 4-1. ëª¨ë¸ ì§ì ‘ ìŠ¤íŠ¸ë¦¬ë°

* ì˜ˆì‹œ ì½”ë“œ

```python
from langchain_core.messages import HumanMessage

result = ''
result_with_token_splits = ''

for chunk in local_llm.stream([HumanMessage(content="ë¡œë¼ì•¼ ì•ˆë…•? ìš”ì¦˜ ë­í•´? (ë‹µë³€ ì‹œì‘)")]):
    print(chunk, end="", flush=True)
    result += chunk
    result_with_token_splits += chunk + "|"

    if result.endswith('(ë‹µë³€ ì¢…ë£Œ)') or result.endswith('(ë‹µë³€ ì¢…ë£Œ) '):
        break

print('\n\ntoken split ê²°ê³¼:\n', result_with_token_splits)
```

* ì‹¤í–‰ ê²°ê³¼

```
 ì–¸ì–´ ëª¨ë¸ ë…¼ë¬¸ ìš”ì¦˜ ë³´ê³  ìˆì–´! í˜ì‹ ì ì¸ ê±° í•˜ë‚˜ ìˆëŠ”ë° ì•Œë ¤ì¤„ê¹Œ? (ë‹µë³€ ì¢…ë£Œ) 

token split ê²°ê³¼:
  |ì–¸ì–´ |ëª¨ë¸ ||ë…¼ë¬¸ ||ìš”ì¦˜ |ë³´ê³  ||ìˆì–´! ||||í˜ì‹ ì ì¸ |ê±° |í•˜ë‚˜ |ìˆëŠ”ë° ||||ì•Œë ¤ì¤„ê¹Œ? |||(ë‹µë³€ |||ì¢…ë£Œ) |
```

### 4-2. ì²´ì¸ ìŠ¤íŠ¸ë¦¬ë°

* ì˜ˆì‹œ ì½”ë“œ

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

prompt = ChatPromptTemplate.from_template("{user_message} (ë‹µë³€ ì‹œì‘)")

chain = prompt | local_llm | StrOutputParser()
result = ''
result_with_token_splits = ''

for chunk in chain.stream({"user_message": "ë¡œë¼ì•¼ ë‚´ì¼ ë‚˜ë‘ ê°™ì´ ë†€ëŸ¬ê°ˆê¹Œ?"}):
    print(chunk, end="", flush=True)
    result += chunk
    result_with_token_splits += chunk + "|"

    if result.endswith('(ë‹µë³€ ì¢…ë£Œ)') or result.endswith('(ë‹µë³€ ì¢…ë£Œ) '):
        break

print('\n\ntoken split ê²°ê³¼:\n', result_with_token_splits)
```

* ì‹¤í–‰ ê²°ê³¼

```
 ë‚´ì¼ ë…¼ë¬¸ ë°œí‘œí•˜ëŠ” ìˆ˜ì—… ìˆê³  ë…¼ë¬¸ ê³µë¶€í•˜ëŠë¼ê³  ë°”ë¹  ğŸ˜¥ (ë‹µë³€ ì¢…ë£Œ) 

token split ê²°ê³¼:
  ||ë‚´ì¼ ||ë…¼ë¬¸ ||ë°œí‘œí•˜ëŠ” ||ìˆ˜ì—… |ìˆê³  ||ë…¼ë¬¸ ||||ê³µë¶€í•˜ëŠë¼ê³  ||ë°”ë¹  ||ğŸ˜¥ |||(ë‹µë³€ |||ì¢…ë£Œ) |
```

### 4-3. ë¹„ë™ê¸° ìŠ¤íŠ¸ë¦¬ë°

* ì˜ˆì‹œ ì½”ë“œ

```
# Google Colab ì—ì„œ asyncio ë¥¼ ì‹¤í–‰ì‹œí‚¤ê¸° ìœ„í•œ ì„¤ì •

!pip install nest_asyncio
```

```python
import nest_asyncio 
nest_asyncio.apply()
```

```python
import asyncio

result = ''
result_with_token_splits = ''

async def async_stream():
    global result, result_with_token_splits
    
    async for chunk in local_llm.astream([HumanMessage(content="ë¡œë¼ì•¼ ë„ˆ MBTI ë­ì•¼? (ë‹µë³€ ì‹œì‘)")]):
        print(chunk, end="", flush=True)
        result += chunk
        result_with_token_splits += chunk + "|"

        if result.endswith('(ë‹µë³€ ì¢…ë£Œ)') or result.endswith('(ë‹µë³€ ì¢…ë£Œ) '):
            break

asyncio.run(async_stream())
print('\n\ntoken split ê²°ê³¼:\n', result_with_token_splits)
```

* ì‹¤í–‰ ê²°ê³¼

```
 ë‚˜ ENTJ! ì™„ì „ ë§¤ë ¥ì ì¸ ì„±ê²© ì•„ë‹ˆì•¼? (ë‹µë³€ ì¢…ë£Œ) 

token split ê²°ê³¼:
  |ë‚˜ |||ENTJ! ||ì™„ì „ |||ë§¤ë ¥ì ì¸ ||ì„±ê²© |||ì•„ë‹ˆì•¼? |||(ë‹µë³€ |||ì¢…ë£Œ) |
```