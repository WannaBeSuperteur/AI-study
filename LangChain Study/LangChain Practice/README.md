# LangChain ì‹¤ìŠµ

## ëª©ì°¨

* [1. ê¸°ë³¸ ìš”êµ¬ì‚¬í•­](#1-ê¸°ë³¸-ìš”êµ¬ì‚¬í•­)
* [2. ê°œë°œ ì¼ì •](#2-ê°œë°œ-ì¼ì •)
* [3. êµ¬í˜„ ë‚´ìš© ìƒì„¸](#3-êµ¬í˜„-ë‚´ìš©-ìƒì„¸)
  * [3-1. í•™ìŠµ ë°ì´í„°ì…‹ ìƒì„± ê³¼ì •](#3-1-í•™ìŠµ-ë°ì´í„°ì…‹-ìƒì„±-ê³¼ì •) 
* [4. LLM ì„ íƒ](#4-llm-ì„ íƒ)
  * [4-1. í•œêµ­ì–´ LLM ì„ íƒ ì´ìœ ](#4-1-í•œêµ­ì–´-llm-ì„ íƒ-ì´ìœ )
  * [4-2. í•œêµ­ì–´ LLM ì„±ëŠ¥ ìƒì„¸ ë¹„êµ ê²°ê³¼](#4-2-í•œêµ­ì–´-llm-ì„±ëŠ¥-ìƒì„¸-ë¹„êµ-ê²°ê³¼)
* [5. ì´ìŠˆ ì‚¬í•­ ë° í•´ê²° ë°©ë²•](#5-ì´ìŠˆ-ì‚¬í•­-ë°-í•´ê²°-ë°©ë²•)
  * [5-1. EOS token í•™ìŠµ ì•ˆë¨](#5-1-eos-token-í•™ìŠµ-ì•ˆë¨)
  * [5-2. LLM Fine-Tuning í›„, ì‘ë‹µì´ ì œëŒ€ë¡œ ìƒì„±ë˜ì§€ ì•ŠìŒ](#5-2-llm-fine-tuning-í›„-ì‘ë‹µì´-ì œëŒ€ë¡œ-ìƒì„±ë˜ì§€-ì•ŠìŒ)
  * [5-3. LLM output ì—ì„œ ì²˜ìŒì— EOS token ë°œìƒ](#5-3-llm-output-ì—ì„œ-ì²˜ìŒì—-eos-token-ë°œìƒ)
  * [5-4. Fine-Tuning ëœ LLM ë¡œë”© ì‹œ tensor size ë¶ˆì¼ì¹˜](#5-4-fine-tuning-ëœ-llm-ë¡œë”©-ì‹œ-tensor-size-ë¶ˆì¼ì¹˜)
* [6. ì°¸ê³ ](#6-ì°¸ê³ )
  * [6-1. Quantization ì ìš©/ë¯¸ ì ìš© ì‹œ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì°¨ì´](#6-1-quantization-ì ìš©ë¯¸-ì ìš©-ì‹œ-gpu-ë©”ëª¨ë¦¬-ì‚¬ìš©ëŸ‰-ì°¨ì´) 

## 1. ê¸°ë³¸ ìš”êµ¬ì‚¬í•­

* ë‹¤ìŒ tool call ì„ í•˜ëŠ” ê°„ë‹¨í•œ LLM ì—ì´ì „íŠ¸ ê°œë°œ
  * íŠ¹ì • ë‚ ì§œë¡œë¶€í„° ```N```ì¼ ì „/í›„ì˜ ë‚ ì§œ ê³„ì‚°
  * íŠ¹ì • ë‚ ì§œì˜ ìš”ì¼ ê³„ì‚° ë° ë°˜í™˜
* êµ¬í˜„ í•„ìˆ˜ ì‚¬í•­
  * Open Source LLM ì„ ```trl``` ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì´ìš©í•˜ì—¬ Fine-Tuning  
  * LangChain ê¸°ë°˜ìœ¼ë¡œ êµ¬í˜„
  * [LangChain ì—ì´ì „íŠ¸ tool call ë¬¸ì„œ](../LangChain_ì—ì´ì „íŠ¸_tool_call.md) ë¥¼ ì°¸ê³ í•˜ì—¬ tool call ë°©ì‹ìœ¼ë¡œ êµ¬í˜„
  * ì‚¬ìš©ìì™€ì˜ ì´ì „ ëŒ€í™”ë¥¼ ê¸°ë¡í•˜ëŠ” [ë©”ëª¨ë¦¬ (Memory)](../LangChain_ë©”ëª¨ë¦¬.md) ê¸°ëŠ¥ í¬í•¨

## 2. ê°œë°œ ì¼ì •

* ì „ì²´ ê°œë°œ ì¼ì •
  * **2026.02.19 (ëª©) - 02.28 (í† ), 9 days**
* ìƒì„¸ ê°œë°œ ì¼ì •

| êµ¬ë¶„         | ê³„íš ë‚´ìš©                              | ì¼ì •                     | branch                                   | issue                                                          | ìƒíƒœ |
|------------|------------------------------------|------------------------|------------------------------------------|----------------------------------------------------------------|----|
| ğŸ“ƒ ë¬¸ì„œí™”     | ê¸°ë³¸ ìš”êµ¬ì‚¬í•­ ë° ê°œë°œ ì¼ì • ë¬¸ì„œí™”                | 02.19 ëª© (1d)           |                                          |                                                                | âœ…  |
| ğŸ§  ëª¨ë¸ ì„ íƒ   | ì ì ˆí•œ í•œêµ­ì–´ LLM ìˆœìœ„ ì‚°ì¶œ (ìµœì‹  Open-Source) | 02.19 ëª© (1d)           |                                          |                                                                | âœ…  |
| ğŸ”¨ ëª¨ë¸ êµ¬í˜„   | ë„êµ¬ í•¨ìˆ˜ êµ¬í˜„ (ì „/í›„ ë‚ ì§œ ê³„ì‚°, ìš”ì¼ ê³„ì‚°)        | 02.20 ê¸ˆ (1d)           | ```LangChain-practice-001-tool```        | [issue](https://github.com/WannaBeSuperteur/AI-study/issues/1) | âœ…  |
| ğŸ”¨ ëª¨ë¸ êµ¬í˜„   | ë©”ëª¨ë¦¬ êµ¬í˜„ (ë„êµ¬ í•¨ìˆ˜ì™€ ë™ì¼ ë°©ì‹)              | 02.20 ê¸ˆ (1d)           | ```LangChain-practice-002-memory```      | [issue](https://github.com/WannaBeSuperteur/AI-study/issues/2) | âœ…  |
| ğŸ§  ëª¨ë¸ ì„ íƒ   | LLM í•™ìŠµ (Fine-Tuning) ëŒ€ìƒ LLM ìµœì¢… ì„ íƒ  | 02.20 ê¸ˆ (1d)           | ```LangChain-practice-003-fine-tuning``` | [issue](https://github.com/WannaBeSuperteur/AI-study/issues/3) | âœ…  |
| ğŸ“ ë°ì´í„°ì…‹ ì œì‘ | LLM í•™ìŠµ ë°ì´í„°ì…‹ ì œì‘                     | 02.20 ê¸ˆ (1d)           | ```LangChain-practice-003-fine-tuning``` | [issue](https://github.com/WannaBeSuperteur/AI-study/issues/3) | âœ…  |
| ğŸ§ª ëª¨ë¸ í•™ìŠµ   | LLM í•™ìŠµ (Fine-Tuning) ì‹¤ì‹œ            | 02.20 ê¸ˆ - 02.22 ì¼ (3d) | ```LangChain-practice-003-fine-tuning``` | [issue](https://github.com/WannaBeSuperteur/AI-study/issues/3) | âœ…  |
| âš™ ê¸°ëŠ¥ êµ¬í˜„    | LLM ì—ì´ì „íŠ¸ ê¸°ëŠ¥ êµ¬í˜„                     | 02.22 ì¼ - 02.26 ëª© (5d) | ```LangChain-practice-004-agent```       | [issue](https://github.com/WannaBeSuperteur/AI-study/issues/4) | âœ…  |
| âš™ ê¸°ëŠ¥ êµ¬í˜„    | LLM ì—ì´ì „íŠ¸ ê¸°ëŠ¥ êµ¬í˜„ (tool call ì¬ êµ¬í˜„)    | 02.23 ì›” - 02.26 ëª© (5d) | ```LangChain-practice-005-tool-call```   | [issue](https://github.com/WannaBeSuperteur/AI-study/issues/4) | âœ…  |
| ğŸ” ìµœì¢… ê²€í†    | ìµœì¢… QA (ë²„ê·¸ ìœ ë¬´ ê²€ì‚¬)                   | 02.28 í†  (1d)           |                                          |                                                                | â¬œ  |
| ğŸ“ƒ ë¬¸ì„œí™”     | í”„ë¡œì íŠ¸ ë¬¸ì„œ ì •ë¦¬ ë° ë§ˆë¬´ë¦¬                   | 02.28 í†  (1d)           |                                          |                                                                | â¬œ  |

## 3. êµ¬í˜„ ë‚´ìš© ìƒì„¸

* ì „ì²´ êµ¬ì¡°

TBU

* í•™ìŠµ ë°ì´í„°ì…‹
  * [toolcall_training_data.csv](toolcall_training_data.csv)

| ì»¬ëŸ¼                     | ì„¤ëª…                                  | LLM í•™ìŠµ ë°ì´í„°ë¡œ ì‚¬ìš©                                                       |
|------------------------|-------------------------------------|----------------------------------------------------------------------|
| ```user_input```       | ì‚¬ìš©ìì˜ ìµœì´ˆ ì…ë ¥ (ë‚ ì§œ ë˜ëŠ” ìš”ì¼ ê³„ì‚° ìš”ì²­)         | - **Tool Call ì‹¤ì‹œ** LLMì˜ **ì…ë ¥** ë°ì´í„°<br>- **ìµœì¢… ë‹µë³€ ìƒì„±** LLMì˜ **ì…ë ¥** ë°ì´í„° |
| ```tool_call_output``` | LangChainì˜ tool callì„ ìœ„í•œ LLM output | **Tool Call ì‹¤ì‹œ** LLMì˜ **ì¶œë ¥** ë°ì´í„°                                     |
| ```tool_call_result``` | tool call ê²°ê³¼ë¡œ ë°˜í™˜ë˜ëŠ” ê°’ (ë‚ ì§œ ë˜ëŠ” ìš”ì¼)     | **ìµœì¢… ë‹µë³€ ìƒì„±** LLMì˜ **ì…ë ¥** ë°ì´í„°                                         |
| ```final_output```     | tool call ê²°ê³¼ë¥¼ í•´ì„í•œ LLMì˜ ìµœì¢… ë‹µë³€        | **ìµœì¢… ë‹µë³€ ìƒì„±** LLMì˜ **ì¶œë ¥** ë°ì´í„°                                         |

### 3-1. í•™ìŠµ ë°ì´í„°ì…‹ ìƒì„± ê³¼ì •

* ìš”ì•½
  * ChatGPT ë¥¼ ì´ìš©í•˜ì—¬ 200 rows ê·œëª¨ì˜ í•™ìŠµ ë°ì´í„°ì…‹ì„ ë¹ ë¥´ê²Œ ìƒì„±
* ì‚¬ìš© ëª¨ë¸
  * ChatGPT 5.2 Thinking
* ì‚¬ìš© í”„ë¡¬í”„íŠ¸
  * ìµœì´ˆ í”„ë¡¬í”„íŠ¸ë¡œ csv íŒŒì¼ ìƒì„± ì´í›„ì—ë„, ìƒì„±ëœ csv íŒŒì¼ì˜ ì˜¤ë¥˜ë¥¼ ì§€ì†ì ìœ¼ë¡œ ê°œì„  ìš”ì²­

```
ìœ„ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬, ì˜ˆë¥¼ ë“¤ì–´ ë‹¤ìŒê³¼ ê°™ì€ ì‹ìœ¼ë¡œ 200ê°œì˜ í–‰ì´ ìˆëŠ” í•™ìŠµ ë°ì´í„°ë¥¼ csv í˜•ì‹ìœ¼ë¡œ ë§Œë“¤ì–´ì¤˜.
 - ì‚¬ìš©ì ì§ˆë¬¸ ì˜ˆì‹œ (ìµœëŒ€í•œ ë‹¤ì–‘í•˜ê²Œ) (ì—´ ì´ë¦„: user_input)
   - 2026ë…„ 2ì›” 20ì¼ì´ ë¬´ìŠ¨ ìš”ì¼ì´ì•¼?
   - 2026ë…„ 1ì›” 15ì¼ì˜ ìš”ì¼ì„ ì•Œë ¤ì¤˜
   - 2026ë…„ 12ì›” 25ì¼ë¶€í„° 10ì¼ í›„ëŠ” ì–¸ì œì•¼?
   - 2025ë…„ 1ì›” 1ì¼ë¶€í„° 100ì¼ í›„ ë‚ ì§œë¥¼ ì–¸ì œì¸ì§€ ì•Œë ¤ì¤˜
 - Tool Callì„ í•  ìˆ˜ ìˆëŠ” LLM ì‘ë‹µ (ì—´ ì´ë¦„: tool_call_output)
 - Tool Call ê²°ê³¼ (ì—´ ì´ë¦„: tool_call_result)
 - Tool Call í•¨ìˆ˜ë¥¼ ë°›ì•„ì„œ ìµœì¢… ì‘ë‹µ (ì—´ ì´ë¦„: final_output)
   - 2026ë…„ 2ì›” 20ì¼ì€ ê¸ˆìš”ì¼ì…ë‹ˆë‹¤. í˜¹ì‹œ ì¶”ê°€ ì§ˆë¬¸ ìˆìœ¼ì‹ ê°€ìš”?
   - ê·¸ë‚ ì€ ëª©ìš”ì¼ì´ì—ìš”! í˜¹ì‹œ ë” ê¶ê¸ˆí•œ ê±° ìˆì–´ìš”?
   - 2027ë…„ 1ì›” 4ì¼! ë§ì£ ?
   - 2025ë…„ 4ì›” 11ì¼ì¸ë° ê·¸ë‚  ë¬´ìŠ¨ ì´ë²¤íŠ¸ ìˆì–´ìš”? ê¶ê¸ˆí•´ìš”!
```

```
ì—¬ê¸°ì„œ tool_call_result ì—´ì˜ ê°’ì´ ë‚ ì§œ ê³„ì‚° ê²°ê³¼ì¸ ê²½ìš° ì˜ˆë¥¼ ë“¤ì–´ '2026ë…„ 2ì›” 20ì¼'ì´ ì•„ë‹Œ '2026ë…„ 02ì›” 20ì¼'ì²˜ëŸ¼ í‘œì‹œë˜ëŠ”ë°, ì´ë¥¼ '2026ë…„ 2ì›” 20ì¼' í˜•ì‹ìœ¼ë¡œ ìˆ˜ì •í•´ ì¤˜.
```

```
ë‹¤ìŒê³¼ ê°™ì€ ì˜¤ë¥˜ê°€ ê³„ì† ìˆëŠ”ë°, ì´ë¥¼ ìˆ˜ì •í•´ì¤˜.
 - ìš”ì¼ ê³„ì‚° ê²°ê³¼ ì¤‘ 'Oìš”ì¼ë„¤ìš”' -> 'Oìš”ì¼ì´ë„¤ìš”' ë¡œ ìˆ˜ì •
 - final_output ì»¬ëŸ¼ì—ì„œ 'ì°¸ê³ ë¡œ ë‚ ì§œ í˜•ì‹ì€'ê³¼ ê°™ì€ ë‚ ì§œ í˜•ì‹ì— ëŒ€í•œ ì–¸ê¸‰ì€ ìƒëµí•œë‹¤.
```

```
ë‹¤ìŒê³¼ ê°™ì€ ì˜¤ë¥˜ê°€ ê³„ì† ìˆëŠ”ë°, ì´ë¥¼ ìˆ˜ì •í•´ì¤˜.
 - Nì¼ ì „ì˜ ê²½ìš° -Nì„ ì¸ìˆ˜ë¡œ ë„£ê³  Nì¼ í›„ì˜ ê²½ìš° Nì„ ì¸ìˆ˜ë¡œ ë„£ì–´ì•¼ í•˜ëŠ”ë°, ë°˜ëŒ€ë¡œ ë˜ì–´ ìˆìŒ
```

## 4. LLM ì„ íƒ

* í•œêµ­ì–´ LLM ëª¨ë¸ ì„ íƒ
  * **ìµœì¢… ì±„íƒ: [Midm-2.0-Mini-Instruct](https://huggingface.co/K-intelligence/Midm-2.0-Mini-Instruct)**
  * [Dnotitia LLM í•œêµ­ì–´ ë¦¬ë”ë³´ë“œ (í•œêµ­ ëª¨ë¸)](https://leaderboard.dnotitia.com/?filter=korea) ì°¸ê³ 

### 4-1. í•œêµ­ì–´ LLM ì„ íƒ ì´ìœ 

**1. í•œêµ­ì–´ LLM ì„ íƒ ì ˆì°¨**

* **1.** [Dnotitia LLM í•œêµ­ì–´ ë¦¬ë”ë³´ë“œ (í•œêµ­ ëª¨ë¸)](https://leaderboard.dnotitia.com/?filter=korea) ì—ì„œ **LLM ë¸Œëœë“œëª… (ì˜ˆ: ```kakakcorp/kanana```)** ì¶”ì¶œ
  * ì¶”ì¶œ ê²°ê³¼ (ì´ 8ê°œ ë¸Œëœë“œ)
    * ```naver/HyperCLOVAX``` + ```naver/HCX```
    * ```kakaocorp/kanana```
    * ```LGAI-EXAONE```
    * ```skt/A.X```
    * ```KT/Midm-2.0```
    * ```dnotitia/DNA-2.0```
    * ```upstage/solar```
    * ```trillionlabs/Tri```

* **2.** ì¶”ì¶œí•œ LLM ë¸Œëœë“œëª…ì„ HuggingFace ë¡œ ê²€ìƒ‰í•´ì„œ, í•´ë‹¹ ë¸Œëœë“œì˜ **ëª¨ë“  ëª¨ë¸** íƒìƒ‰
  * ì´ë•Œ **5B ì´í•˜** ì˜ ëª¨ë¸ íƒìƒ‰

| LLM ë¸Œëœë“œ                                   | HuggingFace íƒìƒ‰ ê²°ê³¼ (5B ì´í•˜ ëª¨ë¸)                                                                                                                                           | [ë¸Œëœë“œ ìˆœìœ„ (ì°¸ê³ )](#3-2-í•œêµ­ì–´-llm-ì„±ëŠ¥-ìƒì„¸-ë¹„êµ-ê²°ê³¼) |
|-------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------|
| ```naver/HyperCLOVAX``` + ```naver/HCX``` | [HyperCLOVAX-SEED-Text-Instruct-1.5B](https://huggingface.co/naver-hyperclovax/HyperCLOVAX-SEED-Text-Instruct-1.5B)                                                    |                                         |
| ```kakaocorp/kanana```                    | [kanana-1.5-2.1b-instruct-2505](https://huggingface.co/kakaocorp/kanana-1.5-2.1b-instruct-2505)                                                                        | 2ìœ„                                      |
| ```LGAI-EXAONE```                         | - [EXAONE-Deep-2.4B](https://huggingface.co/LGAI-EXAONE/EXAONE-Deep-2.4B)<br>- [EXAONE-3.5-2.4B-Instruct](https://huggingface.co/LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct) |                                         |
| ```skt/A.X```                             | [ko-gpt-trinity-1.2B-v0.5](https://huggingface.co/skt/ko-gpt-trinity-1.2B-v0.5)                                                                                        | 3ìœ„                                      |
| ```KT/Midm-2.0```                         | [Midm-2.0-Mini-Instruct](https://huggingface.co/K-intelligence/Midm-2.0-Mini-Instruct)                                                                                 | **1ìœ„** (âœ… ìµœì¢… ì±„íƒ)                        |
| ```dnotitia/DNA-2.0```                    | [DNA-2.0-4B](https://huggingface.co/dnotitia/DNA-2.0-4B)                                                                                                               |                                         |
| ```upstage/solar```                       | (5B ë¯¸ë§Œ LLM ì—†ìŒ)                                                                                                                                                         |                                         |
| ```trillionlabs/Tri```                    | [Tri-1.9B-Base](https://huggingface.co/trillionlabs/Tri-1.9B-Base)                                                                                                     |                                         |

* **3.** ìœ„ í•œêµ­ì–´ ëª¨ë¸ ë¦¬ë”ë³´ë“œë¥¼ ê¸°ì¤€ìœ¼ë¡œ **ë¸Œëœë“œ ë³„ LLM ì„±ëŠ¥ ì¶”ì´** ë¥¼ ë¹„êµ
  * í•´ë‹¹ ë¹„êµ ê²°ê³¼ì— ê·¼ê±°í•˜ì—¬ ìµœì¢…ì ìœ¼ë¡œ **5.0B ë¯¸ë§Œ íŒŒë¼ë¯¸í„° ê°œìˆ˜ êµ¬ê°„** ì—ì„œ ê°€ì¥ ì„±ëŠ¥ì´ ì¢‹ì„ ë²•í•œ ë¸Œëœë“œì˜ ëª¨ë¸ ì„ ì •
  * í•´ë‹¹ ì„ íƒí•œ ëª¨ë¸ì´ **OOM, ê¶Œí•œ ì˜¤ë¥˜ ë“± ì˜¤ë¥˜** ë°œìƒ ì‹œ, ê·¸ ë‹¤ìŒìœ¼ë¡œ ì„±ëŠ¥ì´ ì¢‹ì€ ëª¨ë¸ì„ **ì˜¤ë¥˜ ì—†ëŠ” ëª¨ë¸ì´ ì²˜ìŒìœ¼ë¡œ ë‚˜íƒ€ë‚  ë•Œê¹Œì§€** ì„ íƒ
  * ì•„ë˜ì˜ [4-2. í•œêµ­ì–´ LLM ì„±ëŠ¥ ìƒì„¸ ë¹„êµ ê²°ê³¼](#4-2-í•œêµ­ì–´-llm-ì„±ëŠ¥-ìƒì„¸-ë¹„êµ-ê²°ê³¼) ì°¸ê³ 

**2. í•´ë‹¹ ì ˆì°¨ë¡œ ì§„í–‰í•œ ì´ìœ **

* Local PC (12GB GPU) ì—ì„œ LLMì„ Fine-Tuning í•  ë•Œ, **5B ì´ˆê³¼ì˜ LLMì˜ ê²½ìš° OOM (Out of Memory) ê°€ëŠ¥ì„±**
  * ë©”ëª¨ë¦¬ ë° ì—°ì‚°ëŸ‰ ì ˆì•½ì„ ìœ„í•´, Fine-Tuning ì‹œ **[LoRA](../../AI%20Basics/LLM%20Basics/LLM_ê¸°ì´ˆ_Fine_Tuning_LoRA_QLoRA.md) + [Quantization](../../AI%20Basics/LLM%20Basics/LLM_ê¸°ì´ˆ_Quantization.md)** ì ìš© ì‹œì—ë„ OOM ê°€ëŠ¥ ì˜ˆìƒ

* ë¦¬ë”ë³´ë“œ ì„ íƒ ì´ìœ 
  * ë¦¬ë”ë³´ë“œ ì„¤ëª…ì— ë”°ë¥´ë©´ ì•„ë˜ì™€ ê°™ì´ **RAG, íˆ´ ì½œë§ ë“±ì˜ ì„±ëŠ¥ì„ ì •ë°€í•˜ê²Œ í‰ê°€** í•˜ë¯€ë¡œ, 
  * Tool Call ì´ ê°€ëŠ¥í•œ LLM Agent ì„±ëŠ¥ í‰ê°€ì§€í‘œë¡œì„œ ì í•©

> ë””ë…¸í‹°ì‹œì•„ëŠ” í•œêµ­ì–´ ê¸°ë°˜ ì¶”ë¡ , RAG, íˆ´ ì½œë§ ë“±ì˜ ì„±ëŠ¥ì„ ì •ë°€í•˜ê²Œ í‰ê°€í•˜ê¸° ìœ„í•´ ìì²´ ë²¤ì¹˜ë§ˆí¬ë¥¼ êµ¬ì¶•í–ˆìŠµë‹ˆë‹¤.

### 4-2. í•œêµ­ì–´ LLM ì„±ëŠ¥ ìƒì„¸ ë¹„êµ ê²°ê³¼

* ìµœì¢… ë¹„êµ ê²°ê³¼ (ë¸Œëœë“œ ë³„ ìˆœìœ„)
  * **KT Midm (1ìœ„)** > Kakao Kanana (2ìœ„) > SKT A.X (3ìœ„)
  * 4ìœ„ ì´í›„
    * Dnotitia > TrillionLabs > Naver > Upstage > LG EXAONE 

![image](../images/LangChain_Practice_1.PNG)

* ì „ì²´ ë¦¬ìŠ¤íŠ¸
  * ì¶œì²˜: [Dnotitia LLM í•œêµ­ì–´ ë¦¬ë”ë³´ë“œ (í•œêµ­ ëª¨ë¸)](https://leaderboard.dnotitia.com/?filter=korea) (2026.02.19)
  * **ì´ 27ê°œ** LLM í™•ì¸

| LLM                                           | ë¸Œëœë“œ/íšŒì‚¬             | LLM í¬ê¸° (íŒŒë¼ë¯¸í„° ê°œìˆ˜)     | LeaderBoard ì ìˆ˜ |
|-----------------------------------------------|--------------------|----------------------|----------------|
| ```naver/HyperCLOVAX-SEED-Think-14B```        | ```naver```        | 14.0B                | 0.729          |
| ```naver/HCX-DASH-002```                      | ```naver```        | (í™•ì¸ ë¶ˆê°€)              | 0.570          |
| ```naver/HCX-003```                           | ```naver```        | (í™•ì¸ ë¶ˆê°€)              | 0.614          |
| ```naver/HCX-005```                           | ```naver```        | (í™•ì¸ ë¶ˆê°€)              | 0.700          |
| ```naver/HCX-007```                           | ```naver```        | (í™•ì¸ ë¶ˆê°€)              | 0.852          |
| ```kakaocorp/kanana-1.5-8b-instruct-2505```   | ```kakaocorp```    | 8.0B                 | 0.703          |
| ```kakaocorp/kanana-1.5-15.7b-a3b-instruct``` | ```kakaocorp```    | 15.7B                | 0.719          |
| ```LGAI-EXAONE/EXAONE-3.5-32B```              | ```LGAI-EXAONE```  | 32.0B                | 0.784          |
| ```LGAI-EXAONE/EXAONE-Deep-32B```             | ```LGAI-EXAONE```  | 32.0B                | 0.707          |
| ```LGAI-EXAONE/EXAONE-4.0-32B```              | ```LGAI-EXAONE```  | 32.0B                | 0.719          |
| ```LGAI-EXAONE/K-EXAONE-236B-A23B```          | ```LGAI-EXAONE```  | 23.0B (total 236.0B) | 0.831          |
| ```skt/A.X-3.1-Light```                       | ```skt/A.X```      | 7.0B                 | 0.692          |
| ```skt/A.X-3.1```                             | ```skt/A.X```      | 35.0B                | 0.781          |
| ```skt/A.X-4.0-Light```                       | ```skt/A.X```      | 7.0B                 | 0.680          |
| ```skt/A.X-4.0```                             | ```skt/A.X```      | 72.0B                | 0.874          |
| ```skt/A.X-K1```                              | ```skt/A.X```      | 519.0B               | 0.881          |
| ```KT/Midm-2.0-Mini-Instruct```               | ```KT/Midm-2.0```  | 2.3B                 | 0.551          |
| ```KT/Midm-2.0-Base-Instruct```               | ```KT/Midm-2.0```  | 8.0B                 | 0.739          |
| ```dnotitia/DNA-2.0-14B```                    | ```dnotitia```     | 15.0B                | 0.768          |
| ```dnotitia/DNA-2.0-30B-A3B```                | ```dnotitia```     | 3.0B (total 31.0B)   | 0.808          |
| ```dnotitia/DNA-2.0-235B-A22B```              | ```dnotitia```     | 22.0B (total 235.0B) | 0.865          |
| ```upstage/solar-pro```                       | ```upstage```      | 22.0B                | 0.660          |
| ```upstage/solar-pro2```                      | ```upstage```      | 30.9B                | 0.852          |
| ```upstage/Solar-Open-100B```                 | ```upstage```      | 102.0B               | 0.811          |
| ```upstage/solar-pro3```                      | ```upstage```      | 12.0B (total 102.0B) | 0.842          |
| ```trillionlabs/Tri-7B```                     | ```trillionlabs``` | 8.0B                 | 0.685          |
| ```trillionlabs/Tri-21B```                    | ```trillionlabs``` | 21.0B                | 0.793          |

## 5. ì´ìŠˆ ì‚¬í•­ ë° í•´ê²° ë°©ë²•

### 5-1. EOS token í•™ìŠµ ì•ˆë¨

* ë¬¸ì œ ìƒí™©
  * LLM í•™ìŠµ ì‹œ EOS tokenì´ í•™ìŠµì´ ë˜ì§€ ì•Šì•„ì„œ, **inference ì‹œ EOS token ì´ ìƒì„±ë˜ì§€ ì•Šì•„ì„œ token limit ê¹Œì§€ ê³„ì† ìƒì„±**
* ë¬¸ì œ ì›ì¸
  * LLM tokenizer ì˜ ```pad_token``` ì´ ```eos_token``` ê³¼ ê°™ì€ ê²½ìš°,
  * ```DataCollatorForCompletionOnlyLM``` ì— ì˜í•´ **í•´ë‹¹ í† í°ì´ ```-100``` ìœ¼ë¡œ ë¼ë²¨ë§ë˜ì–´ í•™ìŠµ ë¶ˆê°€**
* í•´ê²° ë°©ë²•
  * tokenizer ì˜ ```pad_token``` ê³¼ ```eos_token``` ì´ ì„œë¡œ ë™ì¼í•  ê²½ìš°, **ì„œë¡œ ë‹¤ë¥´ê²Œ ì„¤ì •** 

```python
if tokenizer.pad_token == tokenizer.eos_token:
    tokenizer.pad_token = '<pad>'
```

### 5-2. LLM Fine-Tuning í›„, ì‘ë‹µì´ ì œëŒ€ë¡œ ìƒì„±ë˜ì§€ ì•ŠìŒ

* ë¬¸ì œ ìƒí™©
  * [5-1. EOS token í•™ìŠµ ì•ˆë¨](#5-1-eos-token-í•™ìŠµ-ì•ˆë¨) í•´ê²° ì´í›„
  * LLM Fine-Tuning í›„, í•´ë‹¹ LLMìœ¼ë¡œë¶€í„° **ì‘ë‹µì´ ì œëŒ€ë¡œ ìƒì„±ë˜ì§€ ì•ŠìŒ**
    * ì‘ë‹µ ì‹œì‘ ì‹œì ì— ```eos_token``` ì´ ìì£¼ ë“±ì¥í•˜ì—¬, LLMì— ì˜í•´ ì¶”ê°€ ìƒì„±ë˜ëŠ” ë‚´ìš©ì´ ì‚¬ì‹¤ìƒ ì—†ëŠ” ê²½ìš°ê°€ ë§ìŒ
* ë¬¸ì œ ì›ì¸ **(ì¶”ì •)**
  * LLMì˜ í•™ìŠµ ë°ì´í„°ëŠ” ```(ì…ë ¥ ë°ì´í„°) ### Answer: (ì¶œë ¥ ë°ì´í„°)``` ê¼´ 
  * LLMì´ ê·¸ ì¤‘ê°„ì˜ ```response_template``` (= ``` ### Answer:```) ì— í•´ë‹¹í•˜ëŠ” ë¶€ë¶„ì„ **í•™ìŠµ ì‹œ ë¬´ì‹œ** í•˜ëŠ” ê²ƒìœ¼ë¡œ ì¶”ì •
    * ê·¼ê±°: ê¸°ì¡´ ë¬¸ì¥ì— ì´ì–´ì§€ëŠ” ë“¯í•œ ë‚´ìš©ì´ ìƒì„±ë˜ê³¤ í•¨ (ì˜ˆ: ```ì•ˆë…• ë°˜ê°€ì›Œ``` â†’ ```!```ê°€ ì¶”ê°€ ìƒì„±) 
* í•´ê²° ë°©ë²•
  * ê¸°ì¡´ Oh-LoRA í”„ë¡œì íŠ¸ë¥¼ ì°¸ê³ í•˜ì—¬, **ë‹µë³€ ì‹œì‘ ì§€ì  = ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ ë ì§€ì ** ì— ```(ë‹µë³€ ì‹œì‘)``` ì¶”ê°€

```python
dataset_df['text'] = dataset_df.apply(
    lambda x: f"{x['input_data']}{ANSWER_PREFIX} {ANSWER_START_MARK} {x['output_data']}{tokenizer.eos_token}",
    axis=1
)
dataset = generate_llm_trainable_dataset(dataset_df)
```

```python
def on_epoch_end(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):
    global lora_llm, tokenizer

    print('=== INFERENCE TEST ===')

    for valid_input in self.valid_dataset:
        valid_input_text = valid_input['text'].split(ANSWER_PREFIX)[0] + f' {ANSWER_PREFIX}'
```

### 5-3. LLM output ì—ì„œ ì²˜ìŒì— EOS token ë°œìƒ

* ë¬¸ì œ ìƒí™©
  * LLM ì´ ìƒì„±í•˜ëŠ” output ì—ì„œ ì²˜ìŒì— EOS token ë°œìƒí•˜ì—¬, **ì‚¬ì‹¤ìƒ ì•„ë¬´ê²ƒë„ ìƒì„±ë˜ì§€ ì•ŠìŒ**
* í•´ê²° ë°©ë²•
  * LLM ì„ ì´ìš©í•œ ìƒì„± ì‹œ, ì•„ë˜ì™€ ê°™ì´ **ìµœì†Œ ìƒì„± token ê°œìˆ˜** ë¥¼ ì§€ì •

```python
outputs = lora_llm.generate(**inputs,
                            max_length=self.max_length,
                            do_sample=True,
                            temperature=0.6,
                            eos_token_id=tokenizer.eos_token_id,
                            pad_token_id=tokenizer.pad_token_id,
                            min_new_tokens=5)                      # ì²˜ìŒì— ë°”ë¡œ EOS token ì´ ìƒì„±ë˜ëŠ” ê²ƒ ë°©ì§€
```

### 5-4. Fine-Tuning ëœ LLM ë¡œë”© ì‹œ tensor size ë¶ˆì¼ì¹˜

* ë¬¸ì œ ìƒí™©
  * Fine-Tuning ëœ LLM ë¡œë”© ì‹œ, tensor í¬ê¸°ê°€ ë¶ˆì¼ì¹˜í•˜ì—¬ ë‹¤ìŒê³¼ ê°™ì€ ì˜¤ë¥˜ ë°œìƒ

```
RuntimeError: Error(s) in loading state_dict for LlamaForCausalLM:
        size mismatch for model.embed_tokens.weight: copying a param with shape torch.Size([131384, 1792]) from checkpoint, the shape in current model is torch.Size([131392, 1792]).
        size mismatch for lm_head.weight: copying a param with shape torch.Size([131384, 1792]) from checkpoint, the shape in current model is torch.Size([131392, 1792]).
```

* ë¬¸ì œ ì›ì¸
  * Fine-Tuning ëœ LLMê³¼ ì›ë³¸ Mi:dm-2.0 LLM ê°„ **tokenizer ì˜ vocab size ë¶ˆì¼ì¹˜**
* í•´ê²° ë°©ë²•
  * **config ì—ì„œ ```vocab_size```ë¥¼ ìˆ˜ì •** (Fine-Tuning ëœ LLM ì˜ vocab size)
  * ```ignore_mismatched_sizes=True``` ë¡œ **í¬ê¸° ë¶ˆì¼ì¹˜ ì‹œì—ë„ ì˜¤ë¥˜ ë¯¸ ë°œìƒ** í•˜ë„ë¡ ìˆ˜ì • + í›„ì²˜ë¦¬

```python
config = AutoConfig.from_pretrained(ORIGINAL_MIDM_LLM_PATH)
config.vocab_size = len(tokenizer)  # new vocab size

llm = AutoModelForCausalLM.from_pretrained(
    llm_path,
    config=config,
    trust_remote_code=True,
    torch_dtype=torch.float16,
    ignore_mismatched_sizes=True
)
```

## 6. ì°¸ê³ 

### 6-1. Quantization ì ìš©/ë¯¸ ì ìš© ì‹œ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì°¨ì´

* ì ìš© Quantization
  * **BitsAndBytesConfig**

```python
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type='nf4',
    bnb_4bit_compute_dtype='bfloat16'
)
```

* Quantization ì ìš© vs. ë¯¸ ì ìš© ì‹œ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
  * 2ê°œì˜ [Midm-2.0-Mini-Instruct](https://huggingface.co/K-intelligence/Midm-2.0-Mini-Instruct) Fine-Tuning ëœ LLM ë¡œë”© ì‹œ ê¸°ì¤€

| Quantization ì ìš© ì‹œ | Quantization ë¯¸ ì ìš© ì‹œ | ì°¨ì´        |
|-------------------|---------------------|-----------|
| 9,970 MB          | 6,141 MB            | ğŸ”» 38.4 % |
