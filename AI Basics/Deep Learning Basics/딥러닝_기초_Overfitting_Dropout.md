## 목차
1. bias와 variance

2. 딥러닝에서의 오버피팅 (Overfitting)

3. Dropout의 개념

4. Dropout의 효과

## bias와 variance
**bias** 는 딥러닝 모델의 예측값과 실제 데이터 값의 차이를 나타내고, **variance**는 다양한 데이터에 대해 모델이 예측할 때, 그 값의 편차를 나타낸다.

![오버피팅 예시](./images/Overfitting_1.PNG)

예를 들어 위 그림에서 보라색과 흰색을 분류하는 모델이 있다고 할 때, 각 그림에 따른 bias와 variance를 나타내면 다음과 같다.

|그림|(A)|(B)|(C)|
|---|---|---|---|
|bias|높음|중간|낮음|
|variance|낮음|중간|높음|

bias와 variance 모두 모델의 오차이므로, 딥 러닝 모델이 목표로 하는 것은 **bias와 variance의 trade-off**를 고려하여 최적으로 학습하는 것이다. (한쪽이 너무 커지면 안 된다.)

## 딥러닝에서의 오버피팅 (Overfitting)
**오버피팅 (Overfitting)** 이란, 위 그림의 (C) 와 같이 딥러닝/머신러닝 모델의 학습 횟수가 과도하게 많아서 해당 모델의 variance가 높아지는 현상을 말한다.

위 그림 중 (A) 는 **언더피팅 (Underfitting)**, (B) 는 알맞게 학습된 모델, (C)는 **오버피팅**이다.

딥러닝 모델의 일반적인 학습 과정에 따른 bias와 variance의 변화를 나타내면 다음 그림과 같다.

![오버피팅 모델 학습 진행 그래프](./images/Overfitting_2.PNG)

오버피팅의 해결 방법은 다음과 같다.
* 학습 데이터의 샘플 개수를 늘린다.
  * 이미지 데이터의 경우, Data Augmentation 을 사용할 수 있다.
* 딥러닝 모델의 퍼셉트론 개수를 줄인다.
  * 통계학에서도 오버피팅 해결을 위해 불필요한 변수를 제거하기도 한다.
* Regularization (정규화) 를 이용한다.
* 드롭아웃 (Dropout) 을 이용한다.

## Dropout의 개념
**드롭아웃 (Dropout)** 이란, 딥러닝 모델에서 퍼셉트론 간의 연결선 (인간 뇌의 뉴런) 을 일정한 확률로 제거하는 것을 말한다. 예를 들어 Dropout rate가 0.25 이면, 모든 뉴런이 **완전히 랜덤하게** 25%의 확률로 제거되는 것을 의미한다.

![드롭아웃 예시](./images/Dropout_1.PNG)

미니배치 (Mini-batch) 단위로 딥러닝 모델을 학습시킬 경우, 학습 과정 전체가 아닌 **각 Minibatch마다 랜덤하게** 연결선이 제거되며, 제거 비율이 아닌 **확률**의 개념이기 때문에 정확히 해당 확률만큼의 연결선이 제거되지는 않는다. (제거되는 연결선의 비율은 해당 확률과 다소 차이가 있을 수 있다.)

모델의 test 시에는 Dropout rate에 따라, 모든 뉴런으로 입력되는 **(직전 뉴런의 입력값) * (가중치) 의 값에 (1 - (Dropout rate)) 를 곱하여** 적용한다. (단, bias 값이 있을 경우, 그 값에는 적용하지 않는다.)

## Dropout의 효과
Dropout은 다음과 같은 효과가 있다.
* Overfitting 방지
  * 매 mini-batch를 학습시킬 때마다 신경망의 연결선이 랜덤하게 제거되기 때문에, 출력값과 큰 상관관계가 있는 feature의 큰 가중치에 의해 **다른 feature들이 학습되지 않는 현상을 방지**할 수 있다.