## 목차
1. kNN 알고리즘이란?

2. kNN 알고리즘 응용 : feature를 정규분포로 표준화

3. kNN 알고리즘 응용 : weighted kNN


## 1. kNN 알고리즘이란?
**kNN (K Nearest Neighbors) 알고리즘**은 지도 학습 (Supervised Learning) 알고리즘 중 하나이다.

알고리즘은 **특정 point와 가장 거리가 가까운 K개의 값** 중에서 분류 빈도가 가장 큰 것을 선택하는 것이다.

예를 들어 다음과 같은 데이터 테이블이 있다고 할 때, ? 에 들어갈 class는? (단, k=4)

|x|y|z|class|
|---|---|---|---|
|3|4|5|A|
|3|3|6|A|
|3|2|5|A|
|2|4|1|B|
|2|3|3|B|
|2|2|3|B|
|4|1|0|C|
|4|2|-1|C|
|4|3|1|C|
|2|4|5|?|

(2, 4, 5) 와 가장 가까운 점 4개를 찾으면 다음과 같다. (거리를 볼드체로 표시)

|x|y|z|class|거리|
|---|---|---|---|---|
|3|4|5|A|**1.00**|
|3|3|6|A|**1.73**|
|3|2|5|A|**2.24**|
|2|4|1|B|4.00|
|2|3|3|B|**2.24**|
|2|2|3|B|2.83|
|4|1|0|C|6.16|
|4|2|-1|C|6.63|
|4|3|1|C|4.58|
|2|4|5|A||

(2, 4, 5) 와 가장 가까운 점 4개는 (3, 4, 5), (3, 3, 6), (3, 2, 5), (2, 3, 3) 이고, 이들 중 3개가 A, 1개가 B, 0개가 C이므로, (2, 4, 5)의 최종적인 class 분류는 A이다.

## 2. kNN 알고리즘 응용 : feature를 정규분포로 표준화
단순히 변수의 값 자체를 기준으로 kNN 알고리즘을 작동시키는 경우, **표준편차가 큰 변수의 영향을 크게 받는다**는 문제가 존재한다.

따라서 실무적으로는 각 feature들을 먼저 정규분포로 표준화한 후 kNN 알고리즘을 사용한다.

예를 들어 다음 테이블에서 ? 에 들어갈 class는? (단, k=3)

|x|y|z|class|
|---|---|---|---|
|5|3|40|A|
|4|4|90|A|
|4|3|60|A|
|-1|8|75|B|
|-2|7|30|B|
|-1|9|55|B|
|5|2|70|?|

단순히 거리만을 계산했을 때의 결과는 다음과 같다.

|x|y|z|class|거리|
|---|---|---|---|---|
|5|3|40|A|30.02|
|4|4|90|A|20.12|
|4|3|60|A|**10.10**|
|-1|8|75|B|**9.85**|
|-2|7|30|B|40.91|
|-1|9|55|B|**17.61**|
|5|2|70|?||

점 (5, 2, 70) 과 가장 가까운 3개의 점은 (4, 3, 60), (-1, 8, 75), (-1, 9, 55) 이고, 이들 중 1개가 A, 2개가 B이므로 (5, 2, 70) 의 최종적인 class 분류는 B가 된다.

그러나 실제로 보면, class 분류는 z보다는 x, y 값에 의해서 결정되는 것을 확인할 수 있으므로, 따라서 이 점의 분류는 A가 되어야 한다. 그러므로 이 결과는 다소 부정확하다고 할 수 있다.

이 문제의 해결 방법이 바로 데이터를 다음 공식에 따라 정규화하는 것이다.

**Z = (X - (평균)) / (표준편차)**

x, y, z 변수의 평균과 표준편차를 구하면 다음과 같다. (단, class 값이 있는 6개의 데이터만 고려)

||x|y|z|
|---|---|---|---|
|평균|1.50|5.67|58.33|
|표준편차|2.87|2.43|20.14|

따라서 이 공식을 통해 정규화한 후의 결과는 다음과 같다.

|x|y|z|class|거리|
|---|---|---|---|---|
|1.22|-1.10|-0.91|A|**1.55**|
|0.87|-0.69|1.57|A|**1.34**|
|0.87|-1.10|0.08|A|**0.73**|
|-0.87|0.96|0.83|B|3.25|
|-1.22|0.55|-1.41|B|3.76|
|-0.87|1.37|-0.17|B|3.64|
|1.22|-1.51|0.58|?||

점 (5, 2, 70) 은 점 (1.22, -1.51, 0.58) 로 변환되었고, 이 점과 거리가 가장 가까운 3개의 점은 모두 class가 A이다.

따라서 이 점은 class A로 정상적으로 분류되는 결과를 얻는다.

## 3. kNN 알고리즘 응용 : weighted kNN
kNN 알고리즘을 조금 응용하면, 주변의 K개의 각 포인트마다 동일한 가중치가 아닌, **거리에 반비례**하는 가중치를 줄 수 있다. (이외에도 필요에 따라 다양한 방법이 가능하다.)

맨 앞의 사례에 거리에 반비례 (=거리의 역수) 하는 가중치를 적용하면 다음과 같다.

|x|y|z|class|거리|가중치|
|---|---|---|---|---|---|
|3|4|5|A|**1.00**|**1.00**|
|3|3|6|A|**1.73**|**0.58**|
|3|2|5|A|**2.24**|**0.45**|
|2|4|1|B|4.00||
|2|3|3|B|**2.24**|**0.45**|
|2|2|3|B|2.83||
|4|1|0|C|6.16||
|4|2|-1|C|6.63||
|4|3|1|C|4.58||
|2|4|5|A|||

k=4일 때, 점 (2, 4, 5) 는 여전히 A로 분류된다. 그 이유는 다음과 같다.

* class A에 대한 가중치의 합 : 1.00 + 0.58 + 0.45 = **2.03**
* class B에 대한 가중치의 합 : **0.45**
* class C에 대한 가중치의 합 : **0.00**

따라서 가중치의 합이 가장 큰 class인 class A로 분류된다.

참고로 점이 연속적이 아닌 이산적으로 분포되어 있어서 class 분류 대상 포인트와의 거리가 0인 (위치가 같은) 점이 존재할 수 있거나 그 외에도 성능 향상을 위해 필요한 경우라면, 거리의 역수가 무한대가 될 수 있으므로 ((거리) + a) 의 역수를 가중치로 이용할 수 있다. 이때 a는 일반적으로 0.001과 같은 매우 작은 값이다.