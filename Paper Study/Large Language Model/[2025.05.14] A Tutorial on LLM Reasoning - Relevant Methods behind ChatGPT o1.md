## 목차

* [1. 기존 Autoregressive LLM 의 도전 과제](#1-기존-autoregressive-llm-의-도전-과제)
  * [1-1. Next token prediction 의 기본 메커니즘](#1-1-next-token-prediction-의-기본-메커니즘)
  * [1-2. Next token prediction 에 의한 LLM 지능의 upper bound](#1-2-next-token-prediction-에-의한-llm-지능의-upper-bound) 
  * [1-3. LLM 의 O(n^2) 의 너무 큰 시간 복잡도](#1-3-llm-의-on2-의-너무-큰-시간-복잡도)
* [2. Markov Model 을 이용한 LLM 추론](#2-markov-model-을-이용한-llm-추론)
  * [2-1. LLM 추론 과정 상세](#2-1-llm-추론-과정-상세)
  * [2-2. Text Generation 과 추론의 관계](#2-2-text-generation-과-추론의-관계)
* [3. 실제 구현 방식](#3-실제-구현-방식)
  * [3-1. Reasoning step 의 자동 생성](#3-1-reasoning-step-의-자동-생성)
  * [3-2. Self-reinforced Training](#3-2-self-reinforced-training)
  * [3-3. Inference 시점에서의 연산](#3-3-inference-시점에서의-연산)
* [4. 실제 연구 사례](#4-실제-연구-사례)

## 논문 소개

* Jun Wang, "A Tutorial on LLM Reasoning: Relevant Methods behind ChatGPT o1", 2025
* [arXiv Link](https://arxiv.org/pdf/2502.10867)

## 1. 기존 Autoregressive LLM 의 도전 과제

* GPT 와 같은 LLM 에서는 기존 token 에 근거하여 next token 을 확률분포에서 샘플링하는 **Next token prediction** 을 사용한다.
* 그러나, 이는 다음과 같은 문제점이 있다.
  * LLM 의 지능을 일정 수준을 넘어서 올리기 어렵다. (upper bound)
  * 시간 복잡도가 $O(n^2)$ 으로 매우 크다.

![image](../images/LLM_Reasoning_1.PNG)

### 1-1. Next token prediction 의 기본 메커니즘

GPT 와 같은 현대의 LLM 에서 많이 사용하는 방법인 **Next token prediction** 의 기본 메커니즘은 **이전 token 들에 기반한 확률분포에서 next token 을 sampling** 하는 것이다.

* token 집합 $X$ 에 대한 joint probability
  * $\displaystyle P(X) = P(x_1, x_2, ..., x_T) = \Pi_{t=1}^T P(x_t|x_1, x_2, ..., x_{t-1})$ 
  * 각 token $x_t$ 는 이전 토큰 $x_1, x_2, ..., x_{t-1}$ 에 근거하여 예측

* inference 시의 text generation
  * $P(X) = P(x_1) \times P(x_2|x_1) \times P(x_3|x_1, x_2) \times ...$ 
  * 각 step $t$ 에서, next token $x_t$ 를 $(x_1, x_2, ..., x_{t-1})$ 에 근거하여 예측 

### 1-2. Next token prediction 에 의한 LLM 지능의 upper bound

위와 같은 메커니즘을 통해서 **LLM 의 지능을 향상시키는 데에는 한계 (upper bound)** 가 있다.

* 이는 마치 체스의 다음 수를 next token prediction 처럼 학습하면, **일반적인 체스 플레이어의 실수도 next token prediction 방식의 model 이 학습** 하게 됨
  * 즉, 이 경우 **체스 실력 향상에 한계** 가 있음
* 따라서, 이를 극복하려면 **무엇인가 혁신적인 전략** 필요

### 1-3. LLM 의 O(n^2) 의 너무 큰 시간 복잡도

GPT 와 같이 Next token prediction 메커니즘을 사용하는 LLM 은 기본적으로 **$O(n^2)$ 의 너무 큰 시간 복잡도** 로 연산한다. 그 해결 방법으로 [CoT (Chain of Thought)](../../AI%20Basics/LLM%20Basics/LLM_기초_Chain_of_Thought.md) 가 있다.

* CoT 는 **LLM 의 답변에 추론 과정을 포함시키는** 것이다.
* 그러나 결국 **쓰기만 되고 덮어쓰기, 삭제는 안 되는 제한된 메모리** 로서만 기능하므로, fully dynamic memory 시스템에는 적용하기 어렵다.
* 따라서 CoT 를 대체하는 해결 방법인 **Monte Carlo Tree Search (MCTS)** 와 같은 것이 필요하다.

## 2. Markov Model 을 이용한 LLM 추론

### 2-1. LLM 추론 과정 상세

### 2-2. Text Generation 과 추론의 관계

## 3. 실제 구현 방식

### 3-1. Reasoning step 의 자동 생성

### 3-2. Self-reinforced Training

### 3-3. Inference 시점에서의 연산

## 4. 실제 연구 사례