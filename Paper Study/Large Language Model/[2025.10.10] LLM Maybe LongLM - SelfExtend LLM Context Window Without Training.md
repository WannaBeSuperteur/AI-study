
## 목차

* [1. Position Encoding, RoPE 및 OOD](#1-position-encoding-rope-및-ood)
* [2. SelfExtend 의 핵심 아이디어](#2-selfextend-의-핵심-아이디어)
* [3. SelfExtend 상세](#3-selfextend-상세)
  * [3-1. 예비 분석](#3-1-예비-분석)
  * [3-2. SelfExtend LLM Context Window (without Tuning)](#3-2-selfextend-llm-context-window-without-tuning)
* [4. 실험 결과](#4-실험-결과)
  * [4-1. 언어 모델링 task 성능](#4-1-언어-모델링-task-성능)
  * [4-2. Synthetic Long Context Task 성능](#4-2-synthetic-long-context-task-성능)
  * [4-3. 실세계의 Long Context Task 성능](#4-3-실세계의-long-context-task-성능)
  * [4-4. Short Context Task 성능](#4-4-short-context-task-성능)
  * [4-5. Ablation Study](#4-5-ablation-study)
  * [4-6. 다양한 Context Window 길이에서의 성능](#4-6-다양한-context-window-길이에서의-성능)
* [5. 논의 사항](#5-논의-사항)

## 논문 소개

* Hongye Jin and Xiaotian Han et al., "LLM Maybe LongLM: SelfExtend LLM Context Window Without Tuning", 2024
* [arXiv Link](https://arxiv.org/pdf/2401.01325)

## 1. Position Encoding, RoPE 및 OOD

논문을 본격적으로 읽기 전에 알아야 할 3가지 개념으로 **Position Encoding, RoPE, OOD** 가 있다. 각각 다음과 같다.

| 개념                               | 설명                                                                                                                                                                                                | 참고 링크                                                                                                                              |
|----------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------|
| Position Encoding                | [Transformer](../../Natural%20Language%20Processing/Basics_트랜스포머%20모델.md) 에서 token 의 position 을 나타내기 위해 사용되는 Encoding<br>- **Absolute** Position Embedding 과 **Relative** Position Encoding 으로 구분 | [Positional Encoding](../../Natural%20Language%20Processing/Basics_트랜스포머%20모델.md#2-포지셔널-인코딩-positional-encoding)                   |
| RoPE (Rotary Position Embedding) | positional information 을 **query Q 및 key K vector와 결합** 하는 것                                                                                                                                      | [Rotary Embedding](%5B2025.03.12%5D%20LLaMA%20-%20Open%20and%20Efficient%20Foundation%20Language%20Models.md#2-3-rotary-embedding) |
| OOD (Out-of-Distribution)        | inference 시 입력되는 데이터를 **학습 과정에서 접하지 못한** 경우                                                                                                                                                       |                                                                                                                                    |

## 2. SelfExtend 의 핵심 아이디어

* 해결하려는 문제
  * RoPE 가 적용된 LLM은 아주 긴 텍스트를 처리할 수 있다.
  * 그러나, **OOD Position (학습 도중에는 나타나지 않은 token position)** 이 inference 시에 나타나는 경우 한계점이 있다. 

* SelfExtend 의 핵심 아이디어
  * **추가적인 [LLM Fine-Tuning](../../AI%20Basics/LLM%20Basics/LLM_기초_Fine_Tuning.md) 없이도 context window를 추가 확장** 할 수 있게 한다.
  * 이를 위해, **unseen (large) relative position 을 known position 으로 mapping** 시킨다.
    * 이를 통해 **LLM이 더 긴 context 에서도 일관성을 유지** 할 수 있게 한다.

## 3. SelfExtend 상세

| 내용 구성                                                                                               | 설명                                                         |
|-----------------------------------------------------------------------------------------------------|------------------------------------------------------------|
| [예비 분석](#3-1-예비-분석)                                                                                 | LLM 이 long context 를 처리하는 내부 메커니즘 (Grouped Attention 등) 분석 |
| [SelfExtend LLM Context Window (without Tuning)](#3-2-selfextend-llm-context-window-without-tuning) | SelfExtend 방법론 제안                                          |

### 3-1. 예비 분석

**1. LLM은 왜 pre-trained context window 보다 긴 문맥에서 inference가 실패하는가?**

* sequence length 가 pre-training context window 보다 긴 경우, **LLM의 출력은 예측 불가능** 하다.
  * 이는 이때 **Attention Distribution** 이 pre-training context window 에 있을 때와 **다르기** 때문이다.
* 즉, **신경망은 OOD input 에 robust 하지 않음** 을 의미한다.

**2. positional OOD problem 해결 방법**

* 한 가지 방법은 **unseen relative position** 을 **pre-training 에서 학습한 position 으로 바꾸는** 것이다.
* 이때 다음과 같이 ```FLOOR``` 연산 (= **Grouped Attention**) 을 사용할 수 있다.
  * 이 방법은 ```FLOOR``` 연산을 제외하면 원래 self-attention 과 같은 메커니즘이다.
  * ```P_g = P // G_s```

| notation  | 설명                                                                                         |
|-----------|--------------------------------------------------------------------------------------------|
| ```P_g``` | Grouped Attention 결과                                                                       |
| ```P```   | $P \in R^{B \times L}$, original position encoding<br>- B: batch size<br>- L: input length |
| ```G_s``` | grouped attention                                                                          |

![image](../images/SelfExtend_1.PNG)

[(출처)](https://arxiv.org/pdf/2401.01325) : Hongye Jin and Xiaotian Han et al., "LLM Maybe LongLM: SelfExtend LLM Context Window Without Tuning"

**3. LLM은 정확한 위치 정보 없이도 동작할 수 있는가?**

* 결론적으로 말하면 **일정 부분은 그렇다.**
* ```FLOOR``` 연산을 적용했을 때의 [Perplexity (PPL)](../../AI%20Basics/LLM%20Basics/LLM_기초_LLM의_성능_평가.md#3-4-기타-metric) 값을 비교한 결과는 다음과 같다.
  * **PG-19 데이터셋** 기준, 원본 Llama-2-7b-chat 모델의 Perplexity 는 **6k sequence length** 지점에서 폭증한다. 

![image](../images/SelfExtend_2.PNG)

[(출처)](https://arxiv.org/pdf/2401.01325) : Hongye Jin and Xiaotian Han et al., "LLM Maybe LongLM: SelfExtend LLM Context Window Without Tuning"

* 위 실험 결과로 다음을 알 수 있다.
  * **```FLOOR``` 연산을 적용한 LLM은 상대적으로 안정적인 Perplexity** 를 보인다.
  * 단, Grouped Attention 을 적용했을 때는 **Perplexity 가 원본 모델에 비해 약간 높다.**

**4. Grouped Attention 으로 저하된 LLM의 능력을 복구하는 방법**

* LLM의 Next token prediction 에서는 **target token 의 인접한 이웃 token** 이 가장 큰 역할을 한다.
* **target token 과 가까운 token 에 대해서는 원래의 표준 Attention 을 적용하는 것이 좋다** 는 여러 연구 결과가 있다.
* 따라서 해결 방법은 다음과 같다.
  * Grouped Attention을 적용하는 것은 생성된 문장의 전체적인 품질을 크게 향상시키지 않을 수 있다.
  * 따라서 **target token 부근에서는 표준 Attention** 을 적용해야 한다.

### 3-2. SelfExtend LLM Context Window (without Tuning)



## 4. 실험 결과

### 4-1. 언어 모델링 task 성능

### 4-2. Synthetic Long Context Task 성능

### 4-3. 실세계의 Long Context Task 성능

### 4-4. Short Context Task 성능

### 4-5. Ablation Study

### 4-6. 다양한 Context Window 길이에서의 성능

## 5. 논의 사항
