
## 목차

* [1. Position Encoding, RoPE 및 OOD](#1-position-encoding-rope-및-ood)
* [2. SelfExtend 의 핵심 아이디어](#2-selfextend-의-핵심-아이디어)
* [3. SelfExtend 상세](#3-selfextend-상세)
  * [3-1. 예비 분석](#3-1-예비-분석)
  * [3-2. SelfExtend LLM Context Window (without Tuning)](#3-2-selfextend-llm-context-window-without-tuning)
* [4. 실험 결과](#4-실험-결과)
  * [4-1. 언어 모델링 task 성능](#4-1-언어-모델링-task-성능)
  * [4-2. Synthetic Long Context Task 성능](#4-2-synthetic-long-context-task-성능)
  * [4-3. 실세계의 Long Context Task 성능](#4-3-실세계의-long-context-task-성능)
  * [4-4. Short Context Task 성능](#4-4-short-context-task-성능)
  * [4-5. Ablation Study](#4-5-ablation-study)
  * [4-6. 다양한 Context Window 길이에서의 성능](#4-6-다양한-context-window-길이에서의-성능)
* [5. 논의 사항](#5-논의-사항)

## 논문 소개

* Hongye Jin and Xiaotian Han et al., "LLM Maybe LongLM: SelfExtend LLM Context Window Without Tuning", 2024
* [arXiv Link](https://arxiv.org/pdf/2401.01325)

## 1. Position Encoding, RoPE 및 OOD

## 2. SelfExtend 의 핵심 아이디어

## 3. SelfExtend 상세

### 3-1. 예비 분석

### 3-2. SelfExtend LLM Context Window (without Tuning)

## 4. 실험 결과

### 4-1. 언어 모델링 task 성능

### 4-2. Synthetic Long Context Task 성능

### 4-3. 실세계의 Long Context Task 성능

### 4-4. Short Context Task 성능

### 4-5. Ablation Study

### 4-6. 다양한 Context Window 길이에서의 성능

## 5. 논의 사항
