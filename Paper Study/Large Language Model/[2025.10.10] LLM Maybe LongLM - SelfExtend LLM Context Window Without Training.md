
## 목차

* [1. Position Encoding, RoPE 및 OOD](#1-position-encoding-rope-및-ood)
* [2. SelfExtend 의 핵심 아이디어](#2-selfextend-의-핵심-아이디어)
* [3. SelfExtend 상세](#3-selfextend-상세)
  * [3-1. 예비 분석](#3-1-예비-분석)
  * [3-2. SelfExtend LLM Context Window (without Tuning)](#3-2-selfextend-llm-context-window-without-tuning)
* [4. 실험 결과](#4-실험-결과)
  * [4-1. 언어 모델링 task 성능](#4-1-언어-모델링-task-성능)
  * [4-2. Synthetic Long Context Task 성능](#4-2-synthetic-long-context-task-성능)
  * [4-3. 실세계의 Long Context Task 성능](#4-3-실세계의-long-context-task-성능)
  * [4-4. Short Context Task 성능](#4-4-short-context-task-성능)
  * [4-5. Ablation Study](#4-5-ablation-study)
  * [4-6. 다양한 Context Window 길이에서의 성능](#4-6-다양한-context-window-길이에서의-성능)
* [5. 논의 사항](#5-논의-사항)

## 논문 소개

* Hongye Jin and Xiaotian Han et al., "LLM Maybe LongLM: SelfExtend LLM Context Window Without Tuning", 2024
* [arXiv Link](https://arxiv.org/pdf/2401.01325)

## 1. Position Encoding, RoPE 및 OOD

논문을 본격적으로 읽기 전에 알아야 할 3가지 개념으로 **Position Encoding, RoPE, OOD** 가 있다. 각각 다음과 같다.

| 개념                               | 설명                                                                                                                                                                                                | 참고 링크                                                                                                                              |
|----------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------|
| Position Encoding                | [Transformer](../../Natural%20Language%20Processing/Basics_트랜스포머%20모델.md) 에서 token 의 position 을 나타내기 위해 사용되는 Encoding<br>- **Absolute** Position Embedding 과 **Relative** Position Encoding 으로 구분 | [Positional Encoding](../../Natural%20Language%20Processing/Basics_트랜스포머%20모델.md#2-포지셔널-인코딩-positional-encoding)                   |
| RoPE (Rotary Position Embedding) | positional information 을 **query Q 및 key K vector와 결합** 하는 것                                                                                                                                      | [Rotary Embedding](%5B2025.03.12%5D%20LLaMA%20-%20Open%20and%20Efficient%20Foundation%20Language%20Models.md#2-3-rotary-embedding) |
| OOD (Out-of-Distribution)        | inference 시 입력되는 데이터를 **학습 과정에서 접하지 못한** 경우                                                                                                                                                       |                                                                                                                                    |

## 2. SelfExtend 의 핵심 아이디어

* 해결하려는 문제
  * RoPE 가 적용된 LLM은 아주 긴 텍스트를 처리할 수 있다.
  * 그러나, **OOD Position (학습 도중에는 나타나지 않은 token position)** 이 inference 시에 나타나는 경우 한계점이 있다. 

* SelfExtend 의 핵심 아이디어
  * **추가적인 [LLM Fine-Tuning](../../AI%20Basics/LLM%20Basics/LLM_기초_Fine_Tuning.md) 없이도 context window를 추가 확장** 할 수 있게 한다.
  * 이를 위해, **unseen (large) relative position 을 known position 으로 mapping** 시킨다.
    * 이를 통해 **LLM이 더 긴 context 에서도 일관성을 유지** 할 수 있게 한다.

## 3. SelfExtend 상세

### 3-1. 예비 분석

### 3-2. SelfExtend LLM Context Window (without Tuning)

## 4. 실험 결과

### 4-1. 언어 모델링 task 성능

### 4-2. Synthetic Long Context Task 성능

### 4-3. 실세계의 Long Context Task 성능

### 4-4. Short Context Task 성능

### 4-5. Ablation Study

### 4-6. 다양한 Context Window 길이에서의 성능

## 5. 논의 사항
