## 목차

* [1. 핵심 아이디어](#1-핵심-아이디어)
* [2. LLaDA 의 접근 방법](#2-llada-의-접근-방법)
  * [2-1. Mask Prediction (확률 기반)](#2-1-mask-prediction-확률-기반)
  * [2-2. Pre-training](#2-2-pre-training)
  * [2-3. Supervised Fine-Tuning](#2-3-supervised-fine-tuning)
  * [2-4. Inference](#2-4-inference)
* [3. 실험 결과](#3-실험-결과)
  * [3-1. 기존 ARM baseline 과의 비교](#3-1-기존-arm-baseline-과의-비교)
  * [3-2. 벤치마크 테스트 결과](#3-2-벤치마크-테스트-결과)

## 논문 소개

* Shen Nie and Fengqi Zhu et al., "Large Language Diffusion Models", 2025
* [arXiv Link](https://arxiv.org/pdf/2502.09992)

## 1. 핵심 아이디어

이 논문에서는 **LLaDA (Large Language Diffusion with mAsking)** 를 소개한다.

* 이미지 생성을 위한 [Diffusion 기술](../../Generative%20AI/Basics_Diffusion%20Model.md) 의 기술을 LLM 에 적용한다.
  * 즉, Next token prediction 이 아닌, **전체 문장을 처음에 noise 로 prediction** 한 후 **그 noise 를 제거해 나가면서 문장을 완성** 한다.
* Diffusion 기술의 **Noise** 를 여기서는 **Mask** 로 한다.
  * 즉, 이미지 생성을 위한 Diffusion 에서의 **Noise Prediction** 대신 **Mask Prediction** 을 사용한다.

## 2. LLaDA 의 접근 방법

![image](../images/LLaDA_1.PNG)

[(출처)](https://arxiv.org/pdf/2502.09992) : Shen Nie and Fengqi Zhu et al., "Large Language Diffusion Models"

* LLaDA 의 접근 방법은 각 단계 (Pre-Training, Supervised Fine-Tuning, Sampling) 별로 다음과 같다.

| 구분                                                                                                                                                        | 그림  | 설명                                                                                                          |
|-----------------------------------------------------------------------------------------------------------------------------------------------------------|-----|-------------------------------------------------------------------------------------------------------------|
| Pre-Training                                                                                                                                              | (a) | **random mask** 가 독립적으로 적용된 텍스트 데이터로 Pre-train 됨                                                            |
| [Supervised Fine-Tuning (SFT)](https://github.com/WannaBeSuperteur/AI-study/blob/main/AI%20Basics/LLM%20Basics/LLM_%EA%B8%B0%EC%B4%88_Fine_Tuning_SFT.md) | (b) | Pre-Training 과 유사하되, **Response token 만을 masking** 처리                                                       |
| Sampling (Inference 의 기본 방법)                                                                                                                              | (c) | - $t=1$ (fully masked) → $t=0$ (unmasked) 로 진행<br>- 각 step 에서 **모든 mask 를 동시에** 예측하되, 이후에 **re-masking** 적용 |

### 2-1. Mask Prediction (확률 기반)

### 2-2. Pre-training

### 2-3. Supervised Fine-Tuning

### 2-4. Inference

## 3. 실험 결과

### 3-1. 기존 ARM baseline 과의 비교

### 3-2. 벤치마크 테스트 결과
