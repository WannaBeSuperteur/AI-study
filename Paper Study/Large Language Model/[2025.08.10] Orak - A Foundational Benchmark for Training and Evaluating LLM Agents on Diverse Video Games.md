## 목차

* [1. Orak 개요](#1-orak-개요)
* [2. 실험 설정](#2-실험-설정)
  * [2-1. 게임 플레이에 요구되는 LLM의 역량](#2-1-게임-플레이에-요구되는-llm의-역량)
  * [2-2. 실험 대상 게임](#2-2-실험-대상-게임)
  * [2-3. 모델 (LLM) 및 전략](#2-3-모델-llm-및-전략)
* [3. Fine-Tuning](#3-fine-tuning)
* [4. 실험 결과](#4-실험-결과)
  * [4-1. LLM Arena](#4-1-llm-arena)
  * [4-2. 시각적 입력의 효과](#4-2-시각적-입력의-효과)
  * [4-3. Fine-Tuning 의 효과](#4-3-fine-tuning-의-효과)

## 논문 소개

* Dongmin Park and Minkyu Kim et al., "Orak: A Foundational Benchmark for Training and Evaluating LLM Agents on Diverse Video Games", 2025
* [arXiv Link](https://arxiv.org/pdf/2506.03610)
* [AI Trend Link](../../AI%20Trend/AI_TREND_Jun_2025.md#20250616-월)

## 1. Orak 개요

**Orak (오락)** 은 **LLM이 여러 가지 비디오 게임을 플레이하는 성능** 을 평가하기 위한 벤치마크이다.

![image](../images/Orak_1.PNG)

[(출처)](https://arxiv.org/pdf/2506.03610) : Dongmin Park and Minkyu Kim et al., "Orak: A Foundational Benchmark for Training and Evaluating LLM Agents on Diverse Video Games"

* Orak 의 특징은 다음과 같다.

| 특징            | 설명                                                                                                     |
|---------------|--------------------------------------------------------------------------------------------------------|
| 2가지 커스터마이징 제공 | **LLM 커스터마이징** 및 **Agent 커스터마이징** (새로운 전략) 각각 가능                                                       |
| MCP 인터페이스 사용  | [MCP (Model Context Protocol)](../../AI%20Basics/LLM%20Basics/LLM_기초_MCP_Model_Context_Protocol.md) 참고 |
| 점수 산출 방법      | **게임, LLM (backbone), LLM 에이전트 전략** 에 대한 설정만으로 가능                                                      |

## 2. 실험 설정

### 2-1. 게임 플레이에 요구되는 LLM의 역량

* 게임 플레이에 요구되는 LLM의 역량은 다음과 같다.

| LLM의 역량                       | 설명                                               | 레벨 기준                                             |
|-------------------------------|--------------------------------------------------|---------------------------------------------------|
| Rule Following (RF)           | LLM은 **각 게임의 규칙** 을 충실히 따라야 한다.                  | 규칙의 개수                                            |
| Logical Reasoning (LR)        | in-game 행동 수행을 위한 LLM의 **추론 단계**                 | LLM 추론 단계 개수                                      |
| Spatial Reasoning (SR)        | **spatial understanding** (공간 인식) 이 필요한가?        | spatial understanding 의 필요 여부                     |
| Long-text Understanding (LTU) | LLM은 **긴 문맥을 이해** 할 수 있어야 한다.                    | 문맥의 길이                                            |
| Long-term Planning (LP)       | **전략적 계획 수립** 이 필요한가? (+ 앞으로 **몇 수를 내다봐야** 하는가?) | 계획의 길이 (= sequential action 개수)                   |
| Error Handling (EH)           | 게임 플레이 중 **오류 수정** (error correction) 의 필요 수준    | error correction 의 필요 수준 (one-step, multi-step 등) |
| Odds Handling (OH)            | 게임 플레이 중 **랜덤성** (randomness) 이해의 필요 수준          | randomness 가 게임 플레이에 미치는 영향 수준                    |

### 2-2. 실험 대상 게임

* 실험 대상 게임은 다음과 같다.

![image](../images/Orak_2.PNG)

[(출처)](https://arxiv.org/pdf/2506.03610) : Dongmin Park and Minkyu Kim et al., "Orak: A Foundational Benchmark for Training and Evaluating LLM Agents on Diverse Video Games"

* 각 게임에 요구되는 LLM의 역량 수준은 다음과 같다.

![image](../images/Orak_3.PNG)

[(출처)](https://arxiv.org/pdf/2506.03610) : Dongmin Park and Minkyu Kim et al., "Orak: A Foundational Benchmark for Training and Evaluating LLM Agents on Diverse Video Games"

### 2-3. 모델 (LLM) 및 전략

* Orak 에서는 다음과 같이 **오픈소스 LLM 및 proprietary LLM을 혼합** 하여 사용했다.

| LLM 종류              | LLM 유형      | LLM 규모 |
|---------------------|-------------|--------|
| LLaMA-3.2           | Open-source | 1B, 3B |
| Qwen-2.5            | Open-source | 3B, 7B |
| Minitron            | Open-source | 4B, 8B |
| GPT-4o-mini, GPT-4o | Proprietary |        |
| o3-mini             | Proprietary |        |
| Gemini-2.5-pro      | Proprietary |        |
| Claude-3.7-sonnet   | Proprietary |        |
| DeepSeek-R1         | Proprietary |        |

## 3. Fine-Tuning

## 4. 실험 결과

### 4-1. LLM Arena

### 4-2. 시각적 입력의 효과

### 4-3. Fine-Tuning 의 효과