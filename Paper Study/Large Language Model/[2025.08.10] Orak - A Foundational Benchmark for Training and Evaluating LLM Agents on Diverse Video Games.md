## 목차

* [1. Orak 개요](#1-orak-개요)
* [2. 실험 설정](#2-실험-설정)
  * [2-1. 게임 플레이에 요구되는 LLM의 역량](#2-1-게임-플레이에-요구되는-llm의-역량)
  * [2-2. 실험 대상 게임](#2-2-실험-대상-게임)
  * [2-3. 모델 (LLM) 및 전략](#2-3-모델-llm-및-전략)
* [3. Fine-Tuning](#3-fine-tuning)
* [4. 실험 결과](#4-실험-결과)
  * [4-1. LLM Arena](#4-1-llm-arena)
  * [4-2. 시각적 입력의 효과](#4-2-시각적-입력의-효과)
  * [4-3. Fine-Tuning 의 효과](#4-3-fine-tuning-의-효과)

## 논문 소개

* Dongmin Park and Minkyu Kim et al., "Orak: A Foundational Benchmark for Training and Evaluating LLM Agents on Diverse Video Games", 2025
* [arXiv Link](https://arxiv.org/pdf/2506.03610)
* [AI Trend Link](../../AI%20Trend/AI_TREND_Jun_2025.md#20250616-월)

## 1. Orak 개요

**Orak (오락)** 은 **LLM이 여러 가지 비디오 게임을 플레이하는 성능** 을 평가하기 위한 벤치마크이다.

![image](../images/Orak_1.PNG)

[(출처)](https://arxiv.org/pdf/2506.03610) : Dongmin Park and Minkyu Kim et al., "Orak: A Foundational Benchmark for Training and Evaluating LLM Agents on Diverse Video Games"

* Orak 의 특징은 다음과 같다.

| 특징            | 설명                                                                                                     |
|---------------|--------------------------------------------------------------------------------------------------------|
| 2가지 커스터마이징 제공 | **LLM 커스터마이징** 및 **Agent 커스터마이징** (새로운 전략) 각각 가능                                                       |
| MCP 인터페이스 사용  | [MCP (Model Context Protocol)](../../AI%20Basics/LLM%20Basics/LLM_기초_MCP_Model_Context_Protocol.md) 참고 |
| 점수 산출 방법      | **게임, LLM (backbone), LLM 에이전트 전략** 에 대한 설정만으로 가능                                                      |

## 2. 실험 설정

### 2-1. 게임 플레이에 요구되는 LLM의 역량

* 게임 플레이에 요구되는 LLM의 역량은 다음과 같다.

| LLM의 역량                       | 설명                                               | 레벨 기준                                             |
|-------------------------------|--------------------------------------------------|---------------------------------------------------|
| Rule Following (RF)           | LLM은 **각 게임의 규칙** 을 충실히 따라야 한다.                  | 규칙의 개수                                            |
| Logical Reasoning (LR)        | in-game 행동 수행을 위한 LLM의 **추론 단계**                 | LLM 추론 단계 개수                                      |
| Spatial Reasoning (SR)        | **spatial understanding** (공간 인식) 이 필요한가?        | spatial understanding 의 필요 여부                     |
| Long-text Understanding (LTU) | LLM은 **긴 문맥을 이해** 할 수 있어야 한다.                    | 문맥의 길이                                            |
| Long-term Planning (LP)       | **전략적 계획 수립** 이 필요한가? (+ 앞으로 **몇 수를 내다봐야** 하는가?) | 계획의 길이 (= sequential action 개수)                   |
| Error Handling (EH)           | 게임 플레이 중 **오류 수정** (error correction) 의 필요 수준    | error correction 의 필요 수준 (one-step, multi-step 등) |
| Odds Handling (OH)            | 게임 플레이 중 **랜덤성** (randomness) 이해의 필요 수준          | randomness 가 게임 플레이에 미치는 영향 수준                    |

### 2-2. 실험 대상 게임

* 실험 대상 게임은 다음과 같다.

![image](../images/Orak_2.PNG)

[(출처)](https://arxiv.org/pdf/2506.03610) : Dongmin Park and Minkyu Kim et al., "Orak: A Foundational Benchmark for Training and Evaluating LLM Agents on Diverse Video Games"

* 각 게임에 요구되는 LLM의 역량 수준은 다음과 같다.

![image](../images/Orak_3.PNG)

[(출처)](https://arxiv.org/pdf/2506.03610) : Dongmin Park and Minkyu Kim et al., "Orak: A Foundational Benchmark for Training and Evaluating LLM Agents on Diverse Video Games"

### 2-3. 모델 (LLM) 및 전략

* Orak 에서는 다음과 같이 **오픈소스 LLM 및 proprietary LLM을 혼합** 하여 사용했다.

| LLM 종류              | LLM 유형      | LLM 규모 |
|---------------------|-------------|--------|
| LLaMA-3.2           | Open-source | 1B, 3B |
| Qwen-2.5            | Open-source | 3B, 7B |
| Minitron            | Open-source | 4B, 8B |
| GPT-4o-mini, GPT-4o | Proprietary |        |
| o3-mini             | Proprietary |        |
| Gemini-2.5-pro      | Proprietary |        |
| Claude-3.7-sonnet   | Proprietary |        |
| DeepSeek-R1         | Proprietary |        |

## 3. Fine-Tuning

Orak 에서는 추가적으로 다음과 같이 **각 게임별로 LLM Fine-Tuning** 을 실시했다.

* LLM Fine-Tuning 기본 설정

| 설정 유형             | 설명                                                                                                                                              |
|-------------------|-------------------------------------------------------------------------------------------------------------------------------------------------|
| Data Format       | 게임 플레이를 위한 **trajectory** 및 **LLM inference sequence** 를 정의                                                                                     |
| Data Selection    | - N 개의 **gameplay trajectory** 를 먼저 수집<br>- 고품질 데이터 확보를 위해, **LLM inference 횟수가 900회를 초과** 할 때까지 **높은 점수의 trajectory 를 선택**                     |
| Data Augmentation | 언어적 다양성 확보를 위해, 각 데이터 샘플 $\tau$ 에 대해 **paraphrasing (새로운 표현 사용) 을 이용한 augmentation** 실시<br>- prompt $X^a$ 에 대한 새로운 표현을 도출하기 위해 **GPT-4o** 모델 사용 |

* Data Format 설정 상세 설명

| 구분                 | 설명                                                | 수식                                                                                                                                                                                                       |
|--------------------|---------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| trajectory         | LLM 에이전트의 전략에 의해 실행된 **LLM inference 의 sequence** | $T = {\tau_1, ..., \tau_T}$<br>- $T$ : total game step count<br>- $\tau_t$ : game step $t$ 에서의 LLM inference sequence                                                                                    |
| inference sequence | **LLM prompt + game state → LLM answer** 의 집합     | $\tau = {(X^{a_i}, S, Y^{a_i})}_{i=1}^n$<br>- $a_i \in {'reflection', 'planning', ..., 'action'}$ : $i$ 번째 agentic module<br>- $X^a$ : agentic module $a$ 에 입력되는 prompt<br>- $Y^a$ : $X^a$ 에 대응되는 LLM 출력 |

* Fine-Tuning 학습 데이터 예시
  * 아래 예시는 **Supermario** 게임의 **reflection** 에이전트에 대한 데이터임 

![image](../images/Orak_4.PNG)

[(출처)](https://arxiv.org/pdf/2506.03610) : Dongmin Park and Minkyu Kim et al., "Orak: A Foundational Benchmark for Training and Evaluating LLM Agents on Diverse Video Games"

## 4. 실험 결과

### 4-1. LLM Arena

### 4-2. 시각적 입력의 효과

### 4-3. Fine-Tuning 의 효과