## 목차

* [1. 핵심 아이디어](#1-핵심-아이디어)
  * [1-1. Offline PPO](#1-1-offline-ppo)
* [2. Loss Function](#2-loss-function)
  * [2-1. KL Estimate](#2-1-kl-estimate)
  * [2-2. Alignment Data 관련](#2-2-alignment-data-관련)
* [3. HALO 에 대한 연구](#3-halo-에-대한-연구)
  * [3-1. HALO 의 개념 및 설명](#3-1-halo-의-개념-및-설명)
  * [3-2. HALO 의 유용성](#3-2-halo-의-유용성)
* [4. 실험 설정 및 결과](#4-실험-설정-및-결과)
  * [4-1. 하이퍼파라미터 설정](#4-1-하이퍼파라미터-설정) 
  * [4-2. 실험 결과 (KTO > DPO)](#4-2-실험-결과-kto--dpo)
  * [4-3. 실험 결과에 대한 이유 분석](#4-3-실험-결과에-대한-이유-분석)

## 논문 소개

* Kawin Ethayarajh and Winnie Xu et al., "KTO: Model Alignment as Prospect Theoretic Optimization", 2024
* [arXiv Link](https://arxiv.org/pdf/2402.01306)

## 1. 핵심 아이디어

**Kahneman-Tversky Optimization (KTO)** 는 offline PPO 와 같이 **학습 데이터에서 LLM 의 답변에 대해 good / bad 의 binary signal** 을 주는 방법을 응용한 것이다.

* offline PPO 의 방법으로 [DPO (Direct Preference Optimization)](../../AI%20Basics/LLM%20Basics/LLM_기초_Fine_Tuning_DPO_ORPO.md#2-dpo-direct-preference-optimization) 수준의 성능을 달성했다.

### 1-1. Offline PPO

## 2. Loss Function

### 2-1. KL Estimate

### 2-2. Alignment Data 관련

## 3. HALO 에 대한 연구

### 3-1. HALO 의 개념 및 설명

### 3-2. HALO 의 유용성

## 4. 실험 설정 및 결과

### 4-1. 하이퍼파라미터 설정

### 4-2. 실험 결과 (KTO > DPO)

### 4-3. 실험 결과에 대한 이유 분석