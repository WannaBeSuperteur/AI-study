## 목차

* [1. 핵심 아이디어](#1-핵심-아이디어)
  * [1-1. Offline PPO](#1-1-offline-ppo)
* [2. Loss Function](#2-loss-function)
  * [2-1. KL Estimate](#2-1-kl-estimate)
  * [2-2. Alignment Data 관련](#2-2-alignment-data-관련)
* [3. HALO 에 대한 연구](#3-halo-에-대한-연구)
  * [3-1. HALO 의 개념 및 설명](#3-1-halo-의-개념-및-설명)
  * [3-2. HALO 의 유용성](#3-2-halo-의-유용성)
* [4. 실험 설정 및 결과](#4-실험-설정-및-결과)
  * [4-1. 하이퍼파라미터 설정](#4-1-하이퍼파라미터-설정) 
  * [4-2. 실험 결과 (KTO > DPO)](#4-2-실험-결과-kto--dpo)
  * [4-3. 실험 결과에 대한 이유 분석](#4-3-실험-결과에-대한-이유-분석)

## 논문 소개

* Kawin Ethayarajh and Winnie Xu et al., "KTO: Model Alignment as Prospect Theoretic Optimization", 2024
* [arXiv Link](https://arxiv.org/pdf/2402.01306)

## 1. 핵심 아이디어

**Kahneman-Tversky Optimization (KTO)** 는 offline PPO 와 같이 **학습 데이터에서 LLM 의 답변에 대해 good / bad 의 binary signal** 을 주는 방법을 응용한 것이다.

* Offline PPO 의 방법으로 [DPO (Direct Preference Optimization)](../../AI%20Basics/LLM%20Basics/LLM_기초_Fine_Tuning_DPO_ORPO.md#2-dpo-direct-preference-optimization) 수준의 성능을 달성했다.

### 1-1. Offline PPO

**PPO (Proximal Policy Optimization)** 은 **강화학습 (Reinforcement Learning)** 방법론의 일종으로, **강화 학습 시 주어진 데이터를 이용하여 정책을 최대한 효율적으로 빠르게 개선** 하기 위한 방법 중 하나이다.

* 새로운 정책과 기존 정책에 따른 **action 실행의 확률 비율** 과, 해당 action 실행 시의 **추가적인 advantage** 를 이용한 Loss Function 을 이용하여 학습한다.

Offline PPO 의 [Loss Function](../../AI%20Basics/Deep%20Learning%20Basics/딥러닝_기초_Loss_function.md) 은 다음과 같다.

* 수식
  * $L_{PPO (offline)} = -E_{x,y,t \sim D}[min(q_\theta A(x:y_{<t},y_t)), clip(q_\theta, 1-\epsilon, 1+\epsilon) A(x:y_{<t},y_t)]$
* 수식 설명 (LLM 기준)

| notation                                                            | 설명                                                                                                                                                                    |
|---------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| $q_\theta$                                                          | 새로운 정책과 기존 정책에 따른 action 실행 (특정 next token prediction) 의 확률 비율<br>- $\displaystyle q_\theta = \frac{\pi_\theta (y_t \vert x:y_{<t})}{\pi_{old} (y_t \vert x:y_{<t})}$ |
| $\pi_\theta (y_t \vert x:y_{<t})$, $\pi_{old} (y_t \vert x:y_{<t})$ | 각각 새로운 정책과 기존 정책에 따른 next token $y_t$ 에 대한 prediction 의 확률                                                                                                            |
| $x$                                                                 | LLM 학습 데이터의 입력 데이터                                                                                                                                                    |
| $y_{<t}$                                                            | LLM 학습 데이터의 출력 데이터의 직전 token 까지의 내용                                                                                                                                   |
| $A(x:y_{<t},y_t)$                                                   | token $y_t$ 를 next token 으로 예측함에 따른 Advantage                                                                                                                         |
| $\epsilon$                                                          | 새로운 정책의 이전 정책에 비한 발전 정도를 나타내는 small const.                                                                                                                            |

![image](../images/LLM_KTO_1.PNG)

* Reward (LLM 기준)
  * +1 과 -1 의 **Binary Reward** 를 이용한다.

| 좋은 답변 | 나쁜 답변 |
|-------|-------|
| +1    | -1    |

## 2. Loss Function

### 2-1. KL Estimate

### 2-2. Alignment Data 관련

## 3. HALO 에 대한 연구

### 3-1. HALO 의 개념 및 설명

### 3-2. HALO 의 유용성

## 4. 실험 설정 및 결과

### 4-1. 하이퍼파라미터 설정

### 4-2. 실험 결과 (KTO > DPO)

### 4-3. 실험 결과에 대한 이유 분석