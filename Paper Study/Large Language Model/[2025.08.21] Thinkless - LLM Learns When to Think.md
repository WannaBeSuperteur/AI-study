
## 목차

* [1. Thinkless 개요 및 등장 배경](#1-thinkless-개요-및-등장-배경)
* [2. Thinkless의 핵심 아이디어](#2-thinkless의-핵심-아이디어)
  * [2-1. Distillation for Warm-up](#2-1-distillation-for-warm-up)
  * [2-2. Reinforcement Learning with DeGRPO](#2-2-reinforcement-learning-with-degrpo)
* [3. 실험 및 그 결과](#3-실험-및-그-결과)
  * [3-1. 실험 설정](#3-1-실험-설정)
  * [3-2. Hybrid Reasoning 결과](#3-2-hybrid-reasoning-결과)
  * [3-3. Reinforcement Learning 결과](#3-3-reinforcement-learning-결과)
  * [3-4. Warm-up Distillation 상세](#3-4-warm-up-distillation-상세)
* [4. 논의 사항](#4-논의-사항)

## 논문 소개

* Gongfan Fang and Xinyin Ma et al., "Thinkless: LLM Learns When to Think", 2025
* [arXiv Link](https://arxiv.org/pdf/2505.13379)

## 1. Thinkless 개요 및 등장 배경

**Thinkless** 는 **[추론 기능을 포함한](../../AI%20Basics/LLM%20Basics/LLM_기초_추론형_모델.md) LLM이 주어진 프롬프트에 대해 '추론'을 해야 할지 말아야 할지 결정하는 '판단력'을 부여하는 컨셉** 의 방법론이다.

* Thinkless 의 등장 배경
  * 최근 등장하고 있는 추론형 모델의 성공 요인은 [Chain of Thought (COT)](../../AI%20Basics/LLM%20Basics/LLM_기초_Chain_of_Thought.md) 방법론임
  * 그러나, **CoT 방법을 모든 사용자 프롬프트에 똑같이 적용하면 자원 낭비가 발생** 할 수 있음
  * 이를 위해 다음과 같은 방법들이 등장했으나, **모델이 추론을 실시할지 말지를 결정하는 부분** 은 여전히 해결되지 않은 과제임
    * 추론형 모델의 inference 효율성 향상
    * **hybrid reasoning** 등 방법 존재

## 2. Thinkless의 핵심 아이디어

![image](../images/LLM_Thinkless_1.PNG)

[(출처)](https://arxiv.org/pdf/2505.13379) : "Thinkless: LLM Learns When to Think", 2025

**Thinkless** 의 핵심 아이디어는 다음과 같다.

| 핵심 아이디어                                    | 설명                                                                                                        |
|--------------------------------------------|-----------------------------------------------------------------------------------------------------------|
| Distillation for Warm-up                   | **warm-up** 단계에서, token 을 이용하여 response style 결정<br>- token 은 ```<think>``` ```<short>``` 의 2종류           |
| Reinforcement Learning with Decoupled GRPO | **reinforcement learning** 단계에서, 모델은 **performance feedback 에 기반** 하여 **적절한 inference mode** 를 선택하도록 최적화됨 |

### 2-1. Distillation for Warm-up

**Distillation for Warm-up** 은 Pre-train 된 추론형 LLM을 **2개의 response style** 로 Fine-Tuning 하는 것이다.

| 구분                                                                                                            | 설명                                                                                                                                                                                 |
|---------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| 핵심 목표                                                                                                         | **short-term** response 와 **long-term** response 를 둘 다 생성 가능한 LLM 학습                                                                                                               |
| [Knowledge Distillation (지식 증류)](../../AI%20Basics/Deep%20Learning%20Basics/딥러닝_기초_Knowledge_Distillation.md) | 다음 2개의 Pre-trained 모델을 Teacher Model 로 하여 **지식 증류**<br>- $\pi_{think}$ : step-by-step 추론을 통해 **상세한 CoT chain 생성**<br>- $\pi_{short}$ : instruction following model (비교적 간단한 답변 생성) |

* 다음 수식과 같이 **synthetic paired dataset** 을 생성
  * $\displaystyle D_{distill} = \lbrace (x_i, a_i^{think}, a_i^{short}) \rbrace_{i=1}^N$
  * 각 response 는 ```<short>``` 또는 ```<think>``` 의 prefix 를 가짐

| notation      | 설명                                                                                   |
|---------------|--------------------------------------------------------------------------------------|
| $x_i$         | prompt corpus $X = \lbrace x_i \rbrace_{i=1}^N$ 에 있는 각 prompt                        |
| $a_i^{think}$ | Pre-trained LLM $\pi_{long}$ 에 $x_i$ 를 입력시킨 결과<br>$a_i^{think} = \pi_{long} (x_i)$   |
| $a_i^{short}$ | Pre-trained LLM $\pi_{short}$ 에 $x_i$ 를 입력시킨 결과<br>$a_i^{short} = \pi_{short} (x_i)$ |

### 2-2. Reinforcement Learning with DeGRPO

**Reinforcement Learning with DeGRPO** 는 모델이 적절한 response style 을 선택하도록 **performance feedback 을 이용하여 강화학습** 시키는 것을 말한다.

## 3. 실험 및 그 결과

### 3-1. 실험 설정

### 3-2. Hybrid Reasoning 결과

### 3-3. Reinforcement Learning 결과

### 3-4. Warm-up Distillation 상세

## 4. 논의 사항
