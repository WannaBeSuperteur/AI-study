## 목차

* [1. LLM Architecture Overview](#1-llm-architecture-overview)
* [2. Encoder-Decoder](#2-encoder-decoder)
* [3. Causal Decoder](#3-causal-decoder)
* [4. Prefix Decoder](#4-prefix-decoder)
* [5. Transformer Architecture](#5-transformer-architecture)
  * [5-1. Normalization](#5-1-normalization) 
  * [5-2. Position Embeddings](#5-2-position-embeddings)
  * [5-3. Attention 메커니즘](#5-3-attention-메커니즘)
* [6. Emerging architectures](#6-emerging-architectures)

## 논문 소개

* Andrea Matarazzo and Riccardo Torlone, "A Survey on Large Language Models with some Insights on their Capabilities and Limitations", 2025
* [arXiv Link](https://arxiv.org/pdf/2501.04040)
* 이 문서에서 다룰 파트
  * "3. Foundations of Large Language Models" 중 **3-5. Architecture**

## 1. LLM Architecture Overview

## 2. Encoder-Decoder

## 3. Causal Decoder

## 4. Prefix Decoder

## 5. Transformer Architecture

### 5-1. Normalization

### 5-2. Position Embeddings

### 5-3. Attention 메커니즘

## 6. Emerging architectures

