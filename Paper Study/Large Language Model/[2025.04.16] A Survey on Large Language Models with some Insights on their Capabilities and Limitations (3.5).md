## 목차

* [1. LLM Architecture Overview](#1-llm-architecture-overview)
* [2. Encoder-Decoder](#2-encoder-decoder)
* [3. Causal Decoder](#3-causal-decoder)
* [4. Prefix Decoder](#4-prefix-decoder)
* [5. Transformer Architecture](#5-transformer-architecture)
  * [5-1. Normalization](#5-1-normalization) 
  * [5-2. Position Embeddings](#5-2-position-embeddings)
  * [5-3. Attention 메커니즘](#5-3-attention-메커니즘)
* [6. Emerging architectures](#6-emerging-architectures)

## 논문 소개

* Andrea Matarazzo and Riccardo Torlone, "A Survey on Large Language Models with some Insights on their Capabilities and Limitations", 2025
* [arXiv Link](https://arxiv.org/pdf/2501.04040)
* 이 문서에서 다룰 파트
  * "3. Foundations of Large Language Models" 중 **3-5. Architecture**
* 참고하면 좋은 문서
  * [Transformer 모델 설명](../../Natural%20Language%20Processing/Basics_트랜스포머%20모델.md) 

## 1. LLM Architecture Overview

거대 언어 모델 (LLM) 의 구조에는 다음과 같은 5가지가 있다.

| 구조              | 설명                                                                                                                                                |
|-----------------|---------------------------------------------------------------------------------------------------------------------------------------------------|
| Encoder-Decoder | - Encoder 는 문장을 LLM 이 해석할 수 있는, 그 문장의 **특징을 나타내는 representation (tensor) 으로 변환** 한다.<br>- Decoder 는 **이 representation 으로부터 문장을 생성** 한다.          |
| Causal Decoder  | - 이전 token 들에 기반하여 next token 을 예측한다.                                                                                                             |
| Prefix Decoder  | - Causal Decoder 구조의 **masking 메커니즘** 에 착안하여, **생성된 문장에 대해 'partial conditioning'** 을 한다.                                                         |
| Transformer 구조  | - 현재 LLM 에서 가장 많이 쓰이는 구조이다. (참고: [Transformer 모델 설명](../../Natural%20Language%20Processing/Basics_트랜스포머%20모델.md))<br>- **Attention 메커니즘** 을 사용한다. |
| 기타 새로운 구조       | - 기본적인 Transformer 구조의 연산량이 token 개수의 제곱에 비례하는 **비효율성 해결** 등의 목적으로 새로운 구조들이 제안되고 있다.                                                              |

![image](../images/LLM_Survey_250416_1.PNG)

[(출처)](https://arxiv.org/pdf/2501.04040) : Andrea Matarazzo and Riccardo Torlone, "A Survey on Large Language Models with some Insights on their Capabilities and Limitations"


## 2. Encoder-Decoder

## 3. Causal Decoder

## 4. Prefix Decoder

## 5. Transformer Architecture

### 5-1. Normalization

### 5-2. Position Embeddings

### 5-3. Attention 메커니즘

## 6. Emerging architectures

