
## 목차

* [1. 환각 현상에 대한 몇 가지 사항](#1-환각-현상에-대한-몇-가지-사항)
  * [1-1. Pre-training 에 의한 오류](#1-1-pre-training-에-의한-오류)
  * [1-2. 환각 현상은 왜 Post-training 이후에도 있는가?](#1-2-환각-현상은-왜-post-training-이후에도-있는가)
  * [1-3. IIV mis-classification & IIV reduction](#1-3-iiv-mis-classification--iiv-reduction)
* [2. Pre-training Error](#2-pre-training-error)
  * [2-1. Prompt 없는 Reduction](#2-1-prompt-없는-reduction)
  * [2-2. Prompt 있는 Reduction](#2-2-prompt-있는-reduction)
  * [2-3. base model 의 Error Factor](#2-3-base-model-의-error-factor)
  * [2-4. 추가적인 Factor](#2-4-추가적인-factor)
* [3. Post-training 에서의 환각 현상](#3-post-training-에서의-환각-현상)
  * [3-1. 모델 평가가 환각 현상을 강화시킴](#3-1-모델-평가가-환각-현상을-강화시킴)
  * [3-2. 명백한 Confidence Target](#3-2-명백한-confidence-target)
* [4. 논의 사항](#4-논의-사항)
* [5. 본 연구의 한계점](#5-본-연구의-한계점)

## 논문 소개

* Adam Tauman Kalai and Ofir Nachum et al., "Why Language Models Hallucinate", 2025
* [OpenAI Paper Link](https://cdn.openai.com/pdf/d04913be-3f6f-4d2b-b283-ff432ef4aaa5/why-language-models-hallucinate.pdf)

## 1. 환각 현상에 대한 몇 가지 사항

### 1-1. Pre-training 에 의한 오류

### 1-2. 환각 현상은 왜 Post-training 이후에도 있는가?

### 1-3. IIV mis-classification & IIV reduction

## 2. Pre-training Error

### 2-1. Prompt 없는 Reduction

### 2-2. Prompt 있는 Reduction

### 2-3. base model 의 Error Factor

### 2-4. 추가적인 Factor

## 3. Post-training 에서의 환각 현상

### 3-1. 모델 평가가 환각 현상을 강화시킴

### 3-2. 명백한 Confidence Target

## 4. 논의 사항

## 5. 본 연구의 한계점

