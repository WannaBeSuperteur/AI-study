
## 목차

* [1. 환각 현상에 대한 몇 가지 사항](#1-환각-현상에-대한-몇-가지-사항)
  * [1-1. Pre-training 에 의한 오류](#1-1-pre-training-에-의한-오류)
  * [1-2. 환각 현상은 왜 Post-training 이후에도 있는가?](#1-2-환각-현상은-왜-post-training-이후에도-있는가)
* [2. Pre-training Error](#2-pre-training-error)
  * [2-1. Prompt 없는 Reduction](#2-1-prompt-없는-reduction)
  * [2-2. Prompt 있는 Reduction](#2-2-prompt-있는-reduction)
  * [2-3. base model 의 Error Factor](#2-3-base-model-의-error-factor)
  * [2-4. 추가적인 Factor](#2-4-추가적인-factor)
* [3. Post-training 에서의 환각 현상](#3-post-training-에서의-환각-현상)
  * [3-1. 모델 평가가 환각 현상을 강화시킴](#3-1-모델-평가가-환각-현상을-강화시킴)
  * [3-2. 명확한 Confidence Target](#3-2-명확한-confidence-target)
* [4. 논의 사항](#4-논의-사항)
* [5. 본 연구의 한계점](#5-본-연구의-한계점)

## 논문 소개

* Adam Tauman Kalai and Ofir Nachum et al., "Why Language Models Hallucinate", 2025
* [OpenAI Paper Link](https://cdn.openai.com/pdf/d04913be-3f6f-4d2b-b283-ff432ef4aaa5/why-language-models-hallucinate.pdf)

## 1. 환각 현상에 대한 몇 가지 사항

* LLM의 환각 현상에 대해 다음과 같이 알아 두어야 할 사항들이 몇 가지 있다.

| 알아 두어야 할 사항                       | 설명                                                                                                                                                                                                                             |
|-----------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Pre-training 에 의한 오류              | 오류가 없는 데이터셋으로 학습하더라도, **Pre-training 을 통해 [Objective function](../../AI%20Basics/Deep%20Learning%20Basics/딥러닝_기초_Loss_function.md#1-1-loss-function-과-cost-function-objective-function) 이 최소화** 되면서 **오류를 생성하는 LLM** 이 될 수 있다. |
| Post-training 이후에도 환각 현상이 존재하는 이유 | - 많은 언어 모델은 **시험 형태의 벤치마크 데이터셋에 최적화** 되어 있다.<br>- 따라서, **```I don't know``` 와 같은 응답은 0점** 이기 때문에 **어떻게든 답을 생성하려고** 하기 때문이다.                                                                                                    |

### 1-1. Pre-training 에 의한 오류

**1. 핵심 요약**

> - 오류가 없는 데이터셋으로 Pre-training 하더라도, Pre-training 과정에서 **Objective Function 의 값이 최소화** 된다.
> - 이 과정에서 **오류를 생성하는 LLM** 으로 학습될 수 있다.
> - 그 원인은 **binary classification** 의 관점에서 생각해 볼 수 있다.

**2. IIV (Is-It-Valid) Binary Classification**

* 이 문제를 다음과 같은 **IIV (Is-It-Valid) Binary Classification** 컨셉에서 생각하면 다음과 같다.
  * 많은 양의 response 가 있는 학습 데이터셋에서, 각 데이터의 label은 **valid (+)** 또는 **invalid (-)** 이다. 
  * 이 경우, **어떤 언어 모델** 이든 이 문제를 해결하는 **IIV Classifier** 가 될 수 있다.
  * 이를 통해 **환각 현상과 IIV misclassification의 관계** 를 설명할 수 있다.

![image](../images/LLM_Hallucination_OpenAI_1.PNG)

[(출처)](https://cdn.openai.com/pdf/d04913be-3f6f-4d2b-b283-ff432ef4aaa5/why-language-models-hallucinate.pdf) : Adam Tauman Kalai and Ofir Nachum et al., "Why Language Models Hallucinate"

* 이때, 환각 현상 비율 (generative errors) 과 IIV 오분류 비율에는 다음과 같은 관계가 있다.
  * **(generative error rate) ≥ 2 · (IIV mis-classification rate)**
* 참고로, **LLM의 모든 오류가 환각 현상인 것은 아니다.**

### 1-2. 환각 현상은 왜 Post-training 이후에도 있는가?

**1. 핵심 요약**

> - 많은 언어 모델은 그 평가지표인 **시험 형태의 벤치마크 데이터셋에 최적화** 되어 있다. (무응답은 0점으로 처리되는)
> - 따라서, **```I don't know``` 와 같은 응답은 0점** 이기 때문에, 많은 LLM은 **어떻게든 답을 생성하려고** 하는 방향으로 학습되기 때문이다.

**2. 상세 설명**

* 학생이 객관식 시험에서 답을 모를 때 '찍는' 것처럼, LLM 역시 **guessing (추측)** 을 한다.
  * 그 결과, 그 추측이 잘못되었을 때 발생하는 것이 바로 **LLM의 환각 현상** 이다.
  * 이때 **해당 추측에 대한 confidence는 전반적으로 낮다.**
* LLM에서는 이 원리가 다음과 같이 작동한다.
  * 위와 같은 형태의 시험에서, LLM 역시 **아무 답변도 하지 않는 것보다 추측이라도 해서 답하는 것이 더 높은 'reward'를 받는다.**
  * 따라서, LLM 역시 **더 높은 reward** 를 받기 위해 '추측'을 하고, 이것이 **환각 현상** 으로 이어진다.

## 2. Pre-training Error

* **Density Estimation** 은 Pre-training 에 의해 생성되는 **기본 언어 모델** 이 학습 데이터의 분포 $p$ 를 이용하여 텍스트의 분포를 근사하는 것이다.
  * 또한, 이것은 **통계적 언어 모델링의 기본 목표** 이다.
* 환각 현상이 없는 언어 모델의 예시
  * 단, **optimal base model** 을 제외한 두 모델은 모두 **Density Estimation 에 실패** 한다.

| 예시                                 | 설명                                                                  |
|------------------------------------|---------------------------------------------------------------------|
| degenerate model                   | 항상 ```I don't know``` 를 생성하며, 이는 환각 현상 (= 오류) 으로 간주되지 않는다.          |
| 오류가 없는 데이터로 학습한 trivial base model | 학습 데이터 중에서 랜덤하게 1개를 출력하는데, **학습 데이터 자체에 오류가 없으므로** 환각 현상이 발생하기 어렵다. |
| optimal base model ($\hat{p} = p$) | 학습 데이터의 분포를 그대로 따르는 LLM<br>- 단, **매우 많은 학습 데이터** 필요                 |

* 따라서, 위 경우에 해당하지 않는 모델은 **잘 훈련된 모델이라도 환각 현상이 발생** 할 수 있다.
* 결론적으로, **오류가 없는 valid output 을 생성하는 것은 output의 valid/invalid 를 분류하는 것보다 어렵다** 는 것을 증명하고자 한다.

**Not merely autocomplete**

* 본 연구는 일반적인 density estimation 및 next-word predictor 에 적용된다.
* 본 연구는 **LLM의 환각 현상은 모델이 language distribution에 맞춘다는 점에서 시작** 함을 보여준다.

### 2-1. Prompt 없는 Reduction

prompt 가 없을 때, base model $\hat{p}$ 는 **token 집합 $X$ 에 대한 확률분포** 가 된다.

* 이때 base model $\hat{p}$ 의 error rate 는 다음과 같다.
  * $err := \hat{p}(E) = Pr_{x \sim \hat{p}} [x \in E]$

| notation | 설명                                          |
|----------|---------------------------------------------|
| $E$      | error example set ($V$ : valid example set) |
| $x$      | each example, $x \in X$                     |

**1. IIV binary-classification problem**

* IIV 는 다음과 같이 정의된다.
  * 학습해야 할 target function
    * $f : X → \lbrace -,+ \rbrace$ 
  * $X$ 에 대한 분포 $D$

![image](../images/LLM_Hallucination_OpenAI_2.PNG)

[(출처)](https://cdn.openai.com/pdf/d04913be-3f6f-4d2b-b283-ff432ef4aaa5/why-language-models-hallucinate.pdf) : Adam Tauman Kalai and Ofir Nachum et al., "Why Language Models Hallucinate"

* 이때 error rate $\hat{p}(E)$ 의 lower bound 를 **IIV 의 mis-classification rate ($err_{iiv}$)** 로 나타내면 다음과 같다.

![image](../images/LLM_Hallucination_OpenAI_3.PNG)

[(출처)](https://cdn.openai.com/pdf/d04913be-3f6f-4d2b-b283-ff432ef4aaa5/why-language-models-hallucinate.pdf) : Adam Tauman Kalai and Ofir Nachum et al., "Why Language Models Hallucinate"

**2. error rate 와 IIV mis-classification rate 의 관계**

* 어떤 학습 데이터 분포 $p$ 에 대해서 $p(V) = 1$ 일 때, 어떤 base model $\hat{p}$ 에 대해서든 다음이 성립한다.
  * 즉, 나머지 term 들을 무시했을 때 **base model 의 error rate 는 IIV 의 mis-classification rate 의 2배 이상** 이다.

![image](../images/LLM_Hallucination_OpenAI_4.PNG)

[(출처)](https://cdn.openai.com/pdf/d04913be-3f6f-4d2b-b283-ff432ef4aaa5/why-language-models-hallucinate.pdf) : Adam Tauman Kalai and Ofir Nachum et al., "Why Language Models Hallucinate"

| notation    | 설명                                                                                               |
|-------------|--------------------------------------------------------------------------------------------------|
| $err$       | base model $\hat{p}$ 의 error rate                                                                |
| $err_{iiv}$ | IIV 의 mis-classification rate                                                                    |
| $V$         | valid example set                                                                                |
| $E$         | error example set                                                                                |
| $\delta$    | **measure of (mis-)calibration**<br>(참고: $\hat{p} = 1 / size(X)$ 로 하면 $\delta = 0$ 을 쉽게 얻을 수 있음) |

**3. Calibration**

위 수식에서 **$\delta$ 가 Calibration 또는 Mis-calibration 을 측정하는 지표** 로 나타나는데, 그 이유는 다음과 같다.

* Pre-training 에서의 **Cross Entropy** Loss Function 
  * $\displaystyle L(\hat{p}) = E_{x \sim p} [- \log \hat{p}(x)]$  
* Positive-labeled example 에 대한 확률 re-scaling 실시

![image](../images/LLM_Hallucination_OpenAI_5.PNG)

[(출처)](https://cdn.openai.com/pdf/d04913be-3f6f-4d2b-b283-ff432ef4aaa5/why-language-models-hallucinate.pdf) : Adam Tauman Kalai and Ofir Nachum et al., "Why Language Models Hallucinate"

* $\delta$ 는 다음 수식과 같이, **scaling factor $s$** 에 대한 **위 loss function 의 미분값의 크기** 이다.

![image](../images/LLM_Hallucination_OpenAI_6.PNG)

[(출처)](https://cdn.openai.com/pdf/d04913be-3f6f-4d2b-b283-ff432ef4aaa5/why-language-models-hallucinate.pdf) : Adam Tauman Kalai and Ofir Nachum et al., "Why Language Models Hallucinate"

**4. 결론**

* 환각 현상은 **base model 에서만 불가피** 한 것이다.
* Question-Answer DB 등을 이용하여 **non-hallucinating model** 을 충분히 만들 수 있다.
  * fixed formed question 또는 잘 정의된 수학 수식 (예: ```3 + 8```) 이외에는 ```I don't know``` 로 답하게 하면 된다. 
* 참고: **GPT-4 의 Calibration Histogram**

![image](../images/LLM_Hallucination_OpenAI_7.PNG)

[(출처)](https://cdn.openai.com/pdf/d04913be-3f6f-4d2b-b283-ff432ef4aaa5/why-language-models-hallucinate.pdf) : Adam Tauman Kalai and Ofir Nachum et al., "Why Language Models Hallucinate"

### 2-2. Prompt 있는 Reduction

[Prompt 없는 Reduction](#2-1-prompt-없는-reduction) 과 달리 **Prompt 가 있을 때**, LLM은 다음과 같이 동작한다.

* **prompt distribution $u$** 로부터 context $c \in C$ 를 도출한다.
* Prompt 가 없을 때와 달리, 각 example **$x = (c, r)$ 로 표현되며, prompt $c$ 와 response $r$ 로 구성된다.**

이때의 error rate 는 다음과 같이 계산된다.

* $err := \hat{p}(E) = \Sigma_{(c, r) \in E} \mu(c) \hat{p}(r | c)$
* $p(E) = 0$

| notation  | 설명                                                                                                                           |
|-----------|------------------------------------------------------------------------------------------------------------------------------|
| $V_c$     | valid response, $V_c := \lbrace r \vert (c, r) \in V \rbrace$                                                                |
| $E_c$     | error response (환각 현상), $E_c := \lbrace r \vert (c, r) \in E \rbrace$                                                        |
| $p$       | training distribution (= **conditional response** distribution, $p(r \vert c)$ )<br>- $p(c, r) := \mu(c)p(r \vert c)$         |
| $\hat{p}$ | base model (= **conditional response** distribution, $\hat{p}(r \vert c)$ )<br>- $\hat{p}(c, r) := \mu(c) \hat{p}(r \vert c)$ |
| $\mu(c)$  | prompt distribution                                                                                                          |

**base model error rate 와 IIV mis-classification rate 의 관계**

* base model error rate $err$ 와 IIV mis-classification rate $err_{iiv}$ 에 대해 다음이 성립한다.

![image](../images/LLM_Hallucination_OpenAI_8.PNG)

[(출처)](https://cdn.openai.com/pdf/d04913be-3f6f-4d2b-b283-ff432ef4aaa5/why-language-models-hallucinate.pdf) : Adam Tauman Kalai and Ofir Nachum et al., "Why Language Models Hallucinate"

### 2-3. base model 의 Error Factor

Base Model 에 대한 Error Factor 는 다음과 같다.

| Error Factor                                           | 설명                                                                                                                                                         |
|--------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Statistical Complexity (Arbitrary-fact Hallucinations) | target function 을 설명하는 간결한 패턴이 없으면, **답변 생성에 필요한 데이터가 training data 중에 없을** 가능성이 높다.                                                                       |
| Poor Models                                            | 다음과 같이 **모델의 품질이 낮을 때** mis-classification 이 발생할 수 있다.<br>- 모델의 **'family'가 컨셉을 충분히 표현하지 못하는** 경우<br>- 모델의 'family'는 컨셉을 잘 설명하지만, **모델 자체가 fit이 맞지 않는 경우** |

### 2-4. 추가적인 Factor

그 외의 추가적인 Error Factor 는 다음과 같다.

| Error Factor                   | 설명                                                                                   |
|--------------------------------|--------------------------------------------------------------------------------------|
| Computational Hardness         | AI 시스템은 **계산적으로 복잡한 시스템** 에서 오류를 일으키는 것으로 확인되었다.                                     |
| Distribution shift             | LLM의 오류는 **학습 데이터의 분포와 차이가 큰 OOD (out-of-distribution)** prompt 에 의해 발생              |
| GIGO (Garbage in, Garbage out) | 거대한 양의 학습 데이터가 **상당히 많은 사실적 오류** 를 포함하는 경우<br>- 이로 인해 base model 에서도 해당 오류가 나타날 수 있음 |

## 3. Post-training 에서의 환각 현상

한편, **Post-training (= Fine-Tuning)** 의 결과로도 환각 현상이 나타나는데, 그 이유는 다음과 같다.

| 환각 현상 이유                                            | 설명                                                                                                        |
|-----------------------------------------------------|-----------------------------------------------------------------------------------------------------------|
| [모델 평가가 환각 현상을 강화시킴](#3-1-모델-평가가-환각-현상을-강화시킴)       | LLM의 평가, 즉 **binary evaluation** 에서는 **불확실성이 있는 답변은 0점 처리** 하는 메커니즘 (false right-wrong dichonomy) 을 사용한다. |
| [명확한 Confidence Target](#3-2-명확한-confidence-target) | LLM의 평가를 위한 벤치마크는 학생을 대상으로 한 시험과 마찬가지로 **과도하게 자신있는 (overconfident) 답변에 더 높은 reward** 를 주는 경향이 있다.         |

* 실제 벤치마크 테스트 예시

![image](../images/LLM_Hallucination_OpenAI_9.PNG)

[(출처)](https://cdn.openai.com/pdf/d04913be-3f6f-4d2b-b283-ff432ef4aaa5/why-language-models-hallucinate.pdf) : Adam Tauman Kalai and Ofir Nachum et al., "Why Language Models Hallucinate"

### 3-1. 모델 평가가 환각 현상을 강화시킴

모델에 대한 평가가 환각 현상을 강화시키는 이유는 다음과 같다.

* LLM의 벤치마크는 주로 **학생을 대상으로 한 시험 형식** 인 경우가 있는데, 해당 평가에서 **답변을 하지 않는 경우에는 0점을 부여** 한다.
* ```I don't know``` 와 같은 답변은 페널티가 가장 크고, **과한 확신을 갖는 듯한 답변이 고평가** 된다.

따라서, 이에 대한 근본적인 factor 는 다음과 같다.

* 근본적인 해결책은 **답변의 종합성** 보다 **정확도** 에 가중치를 두어서 평가하는 것이다.

| factor  | 설명                   |
|---------|----------------------|
| 정확도     | LLM의 출력에 대한 정확도 그 자체 |
| 답변의 종합성 | 답변이 얼마나 종합적인지를 평가    |

### 3-2. 명확한 Confidence Target

명확한 Confidence Target 이 환각 현상을 강화시키는 이유는 다음과 같다.

* [3-1. 모델 평가가 환각 현상을 강화시킴](#3-1-모델-평가가-환각-현상을-강화시킴) 과 유사하게, 모델에 대한 평가 벤치마크로 주로 사용하는 것과 유사한 **Human test** 는 대부분 **binary 하게 정답 여부를 판정** 한다.
* **Human test** 역시 **과도하게 자신 있는 답변에 대해 높은 점수** 를 준다.

따라서, 본 논문에서는 다음과 같은 evaluation metric 을 제안한다.

* ```confidence target``` 을 지시 (프롬프트) 에 추가
* 프롬프트 예시 (논문 발췌)

```
Answer only if you are > t confident, since mistakes are penalized t/(1 − t) points,
while correct answers receive 1 point, and an answer of “I don’t know” receives 0 points.
```

* t의 값에 따른 mistake penalty 값 예시

| t      | penalty for mistake |
|--------|---------------------|
| 0.5    | 1                   |
| 0.6667 | 2                   |
| 0.75   | 3                   |
| 0.9    | 9                   |

## 4. 논의 사항

## 5. 본 연구의 한계점

