
## 목차

* [1. 핵심 아이디어](#1-핵심-아이디어)
* [2. Memento 방법론 - Memory-Based MDP](#2-memento-방법론---memory-based-mdp)
  * [2-1. Definitions](#2-1-definitions)
  * [2-2. Soft Q-Learning for CBR Agent](#2-2-soft-q-learning-for-cbr-agent)
  * [2-3. Enhance Q-Learning Based on State Similarity](#2-3-enhance-q-learning-based-on-state-similarity)
* [3. 실제 구현](#3-실제-구현)
  * [3-1. 프레임워크](#3-1-프레임워크)
  * [3-2. Case Memory 관리](#3-2-case-memory-관리)
  * [3-3. 도구 사용 (Tool Usage)](#3-3-도구-사용-tool-usage)
* [4. 실험 및 그 결과](#4-실험-및-그-결과)
  * [4-1. 데이터셋 및 모델](#4-1-데이터셋-및-모델)
  * [4-2. 평가 Metric](#4-2-평가-metric)
  * [4-3. 실험 결과 (기본)](#4-3-실험-결과-기본)
  * [4-4. 실험 결과 (Ablation Study)](#4-4-실험-결과-ablation-study)
* [5. 논의 사항 및 분석](#5-논의-사항-및-분석)

## 논문 소개

* Huichi Zhou and Yihang Chen et al., "Memento: Fine-tuning LLM Agents without Fine-tuning LLMs", 2025
* [arXiv Link](https://arxiv.org/pdf/2508.16153)

## 1. 핵심 아이디어

**Memento** 의 핵심 아이디어는 다음과 같다.

* 해결해야 할 문제
  * LLM Agent 를 해당 Agent 에 속한 LLM 들을 **추가 Fine-Tuning 시키지 않고도**,
  * **변화하는 환경** 속에서 **지속적으로 학습** 하게 하는 것
* 해결 방법
  * **non-parametric 하게, on-the-fly로 학습** 하는 프레임워크 제안
  * **Memory 기반 MDP** (Markov Decision Process) 에 기반한,
  * **Planner-executor** 구조

## 2. Memento 방법론 - Memory-Based MDP

**Memento** 의 핵심 구성 요소는 다음과 같다.

| 핵심 구성 요소                                    | 설명                                                                                       |
|---------------------------------------------|------------------------------------------------------------------------------------------|
| Planner                                     | LLM 기반 CBR (Case-based Reasoning) 에이전트                                                   |
| Tool-enabled Executor                       | LLM 기반 [MCP Client](../../AI%20Basics/LLM%20Basics/LLM_기초_MCP_Model_Context_Protocol.md) |
| **Case Bank** (기존 기록을 eposodic memory 로 저장) | LLM의 파라미터 기반으로 저장된 메모리 **(학습 후에는 fixed 상태임)** 를 대신함                                      |

### 2-1. Definitions

본 논문에서는 다음과 같은 용어를 정의한다.

**1. Memory-Based Markov Decision Process**

![image](../images/Memento_1.PNG)

[(출처)](https://arxiv.org/pdf/2508.16153) : Huichi Zhou and Yihang Chen et al., "Memento: Fine-tuning LLM Agents without Fine-tuning LLMs"

* 다음과 같이 정의되는 Markov Decision Process (MDP) 를 말한다.
  * < $S, A, P, R, \gamma, M$ >

| notation | 설명                                              |
|----------|-------------------------------------------------|
| $S$      | state space                                     |
| $A$      | action space                                    |
| $P$      | transition dynamics, $P : S \times A → ∆(S)$    |
| $R$      | reward function, $R : S \times A → R$           |
| $\gamma$ | discount factor, $\gamma \in [0, 1)$            |
| $M$      | **memory space**, $M = (S \times A \times R)^*$ |

* 일반적인 MDP와의 가장 큰 차이점은 **memory space** 가 있다는 점이다.

**2. Case-Based Reasoning Agent**

* **Case-based Reasoning (CBR) Agent** 는 **현재 상태와 과거 경험을 모두 고려** 하여 결정을 생성하는 Agent이다.
  * 이때 과거 경험은 **한정된 크기의 메모리** 에 저장된다.

| notation                 | 설명                                                                                      |
|--------------------------|-----------------------------------------------------------------------------------------|
| $s \in S$                | 현재 상태 (current state)                                                                   |
| M $\in M$                | 현재 case bank (past case 의 집합 $c$ 로 구성)                                                  |
| $a \in A$                | action                                                                                  |
| $\mu (c \vert s, M)$     | **case retrieval policy** (현재 상태 $s$ 및 현재 case bank M에 대한 확률분포)                         |
| $p_{LLM} (a \vert s, c)$ | 거대 언어 모델 (LLM) 의, 현재 상태 $s$ 및 retrieved case $c \in$ M 에 대한 **action $a$ 의 likelihood** |       

* 이때 CBR Agent의 정책은 다음과 같다.
  * $\displaystyle \pi(a|s,M) = \Sigma_{c \in M} \mu(c|s,M) p_{LLM}(a|s,c)$ 
* CBR Agent의 trajectory $\tau$ 는 다음과 같이 나타낼 수 있다.

![image](../images/Memento_2.PNG)

[(출처)](https://arxiv.org/pdf/2508.16153) : Huichi Zhou and Yihang Chen et al., "Memento: Fine-tuning LLM Agents without Fine-tuning LLMs"

| 구분         | 요소                                                 |
|------------|----------------------------------------------------|
| Agent 의 행동 | ```Retrieve``` ```Reuse and Revise``` ```Retain``` |
| 환경 관련 요소   | ```Evaluation``` ```Transition```                  |

### 2-2. Soft Q-Learning for CBR Agent

**위 CBR Policy 를 최적화** 하기 위해, 다음과 같이 학습을 실시한다.

* LLM component $p_{LLM}$ 을 **고정** 시킨 채로,
* case retrieval policy $\mu$ 만을 학습

이때 **Retrieval 되는 case 의 다양성** 을 확보하면서 최적화하기 위해, 다음과 같이 **Maximum Entropy RL framework** 기반으로 다음 Objective Function 을 적용한다.

| 구분                                       | 수식                                |
|------------------------------------------|-----------------------------------|
| Objective function $J(\pi)$              | ![image](../images/Memento_3.PNG) |
| Value function $V^\pi (s_t, M_t)$        | ![image](../images/Memento_4.PNG) |
| Q value function $Q^\pi (s_t, M_t, c_t)$ | ![image](../images/Memento_5.PNG) |

[(수식 출처)](https://arxiv.org/pdf/2508.16153) : Huichi Zhou and Yihang Chen et al., "Memento: Fine-tuning LLM Agents without Fine-tuning LLMs"

| notation | 설명                                 |
|----------|------------------------------------|
| $H$      | Entropy                            |
| $\alpha$ | 최종 reward 에서의 Entropy 에 대한 하이퍼파라미터 |

따라서 최종적인 Optimal Policy 의 **Closed-form solution** 은 다음과 같이 표현된다.

![image](../images/Memento_6.PNG)

[(출처)](https://arxiv.org/pdf/2508.16153) : Huichi Zhou and Yihang Chen et al., "Memento: Fine-tuning LLM Agents without Fine-tuning LLMs"

### 2-3. Enhance Q-Learning Based on State Similarity

* 다음과 같은 **Soft Q Learning** 을 통해 위의 Optimal Policy 를 학습할 수 있다.

![image](../images/Memento_7.PNG)

[(출처)](https://arxiv.org/pdf/2508.16153) : Huichi Zhou and Yihang Chen et al., "Memento: Fine-tuning LLM Agents without Fine-tuning LLMs"

* 그러나, 위와 같이 **직접 학습하는 것은 자연어의 복잡한 특성상 어려우므로**, 그 대신 **EC (episodic control) 알고리즘** 을 이용한 **Kernel-based Estimation** 으로 한다.
* 따라서 다음과 같은 근사 함수를 이용한다.

![image](../images/Memento_8.PNG)

[(출처)](https://arxiv.org/pdf/2508.16153) : Huichi Zhou and Yihang Chen et al., "Memento: Fine-tuning LLM Agents without Fine-tuning LLMs"

| notation                                               | 설명                                        |
|--------------------------------------------------------|-------------------------------------------|
| $D_c = \lbrace (s_i, c_i, Q_i) \in D: c_i = c \rbrace$ | episodic memory $D$ 에 저장된 **과거의 상호작용 기록** |
| $k_\theta$                                             | kernel network (파라미터: $\theta$)           |
| $D$                                                    | episodic memory                           |

위 함수를 최적화하기 위해 다음과 같이 **TD (temporal difference) Learning 을 통해 Kernel Parameter $\theta$ 를 최적화** 한다.

| 구분                              | 수식                                 |
|---------------------------------|------------------------------------|
| TD Learning **Loss Function**   | ![image](../images/Memento_9.PNG)  |
| TD Learning **Loss 의 Gradient** | ![image](../images/Memento_10.PNG) |

[(수식 출처)](https://arxiv.org/pdf/2508.16153) : Huichi Zhou and Yihang Chen et al., "Memento: Fine-tuning LLM Agents without Fine-tuning LLMs"

## 3. 실제 구현

![image](../images/Memento_11.PNG)

[(출처)](https://arxiv.org/pdf/2508.16153) : Huichi Zhou and Yihang Chen et al., "Memento: Fine-tuning LLM Agents without Fine-tuning LLMs"

**Memento** 는 위 그림과 같이 **Case-Based Planning** 과 **Tool-Based Execution** 의 핵심 단계로 구성되어 있다.

| 단계                                | 설명                                                                                                                                                                       |
|-----------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Stage 1. **Case-Based Planning**  | LLM 기반 CBR Agent인 **Planner** 가, 작업 지시를 받아서 **case memory 로부터 적절한 case triplet $(s_i, a_i, r_i)_{i=1}^K$ 을 추출** 한다.                                                      |
| Stage 2. **Tool-Based Execution** | 일반 용도의 LLM 기반 **Executor** 가, 각 subtask 를 **자동화된 eposide 로서 수행** 한다.<br>- 이때 [MCP Protocol](../../AI%20Basics/LLM%20Basics/LLM_기초_MCP_Model_Context_Protocol.md) 을 사용한다. |

### 3-1. 프레임워크

### 3-2. Case Memory 관리

### 3-3. 도구 사용 (Tool Usage)

## 4. 실험 및 그 결과

### 4-1. 데이터셋 및 모델

### 4-2. 평가 Metric

### 4-3. 실험 결과 (기본)

### 4-4. 실험 결과 (Ablation Study)

## 5. 논의 사항 및 분석

