
## 목차

* [1. 핵심 아이디어](#1-핵심-아이디어)
* [2. Memento 방법론 - Memory-Based MDP](#2-memento-방법론---memory-based-mdp)
  * [2-1. Definitions](#2-1-definitions)
  * [2-2. Soft Q-Learning for CBR Agent](#2-2-soft-q-learning-for-cbr-agent)
  * [2-3. Enhance Q-Learning Based on State Similarity](#2-3-enhance-q-learning-based-on-state-similarity)
* [3. 실제 구현](#3-실제-구현)
  * [3-1. 프레임워크](#3-1-프레임워크)
  * [3-2. Case Memory 관리](#3-2-case-memory-관리)
  * [3-3. 도구 사용 (Tool Usage)](#3-3-도구-사용-tool-usage)
* [4. 실험 및 그 결과](#4-실험-및-그-결과)
  * [4-1. 데이터셋 및 모델](#4-1-데이터셋-및-모델)
  * [4-2. 평가 Metric](#4-2-평가-metric)
  * [4-3. 실험 결과 (기본)](#4-3-실험-결과-기본)
  * [4-4. 실험 결과 (Ablation Study)](#4-4-실험-결과-ablation-study)
* [5. 논의 사항 및 분석](#5-논의-사항-및-분석)

## 논문 소개

* Huichi Zhou and Yihang Chen et al., "Memento: Fine-tuning LLM Agents without Fine-tuning LLMs", 2025
* [arXiv Link](https://arxiv.org/pdf/2508.16153)

## 1. 핵심 아이디어

**Memento** 의 핵심 아이디어는 다음과 같다.

* 해결해야 할 문제
  * LLM Agent 를 해당 Agent 에 속한 LLM 들을 **추가 Fine-Tuning 시키지 않고도**,
  * **변화하는 환경** 속에서 **지속적으로 학습** 하게 하는 것
* 해결 방법
  * **non-parametric 하게, on-the-fly로 학습** 하는 프레임워크 제안
  * **Memory 기반 MDP** (Markov Decision Process) 에 기반한,
  * **Planner-executor** 구조

## 2. Memento 방법론 - Memory-Based MDP

**Memento** 의 핵심 구성 요소는 다음과 같다.

| 핵심 구성 요소                                    | 설명                                                                                                                                                        |
|---------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------|
| Planner                                     | LLM 기반 CBR (Case-based Reasoning) 에이전트                                                                                                                    |
| Tool-enabled Executor                       | LLM 기반 [MCP Client](https://github.com/WannaBeSuperteur/AI-study/blob/main/AI%20Basics/LLM%20Basics/LLM_%EA%B8%B0%EC%B4%88_MCP_Model_Context_Protocol.md) |
| **Case Bank** (기존 기록을 eposodic memory 로 저장) | LLM의 파라미터 기반으로 저장된 메모리 **(학습 후에는 fixed 상태임)** 를 대신함                                                                                                       |

### 2-1. Definitions

본 논문에서는 다음과 같은 용어를 정의한다.

**1. Memory-Based Markov Decision Process**

![image](../images/Memento_1.PNG)

[(출처)](https://arxiv.org/pdf/2508.16153) : Huichi Zhou and Yihang Chen et al., "Memento: Fine-tuning LLM Agents without Fine-tuning LLMs"

* 다음과 같이 정의되는 Markov Decision Process (MDP) 를 말한다.
  * < $S, A, P, R, \gamma, M$ >

| notation | 설명                                              |
|----------|-------------------------------------------------|
| $S$      | state space                                     |
| $A$      | action space                                    |
| $P$      | transition dynamics, $P : S \times A → ∆(S)$    |
| $R$      | reward function, $R : S \times A → R$           |
| $\gamma$ | discount factor, $\gamma \in [0, 1)$            |
| $M$      | **memory space**, $M = (S \times A \times R)^*$ |

* 일반적인 MDP와의 가장 큰 차이점은 **memory space** 가 있다는 점이다.

**2. Case-Based Reasoning Agent**

* **Case-based Reasoning (CBR) Agent** 는 **현재 상태와 과거 경험을 모두 고려** 하여 결정을 생성하는 Agent이다.
  * 이때 과거 경험은 **한정된 크기의 메모리** 에 저장된다.

| notation                 | 설명                                                                                      |
|--------------------------|-----------------------------------------------------------------------------------------|
| $s \in S$                | 현재 상태 (current state)                                                                   |
| M $\in M$                | 현재 case bank (past case 의 집합 $c$ 로 구성)                                                  |
| $a \in A$                | action                                                                                  |
| $\mu (c \vert s, M)$     | **case retrieval policy** (현재 상태 $s$ 및 현재 case bank M에 대한 확률분포)                         |
| $p_{LLM} (a \vert s, c)$ | 거대 언어 모델 (LLM) 의, 현재 상태 $s$ 및 retrieved case $c \in$ M 에 대한 **action $a$ 의 likelihood** |       

* 이때 CBR Agent의 정책은 다음과 같다.
  * $\displaystyle \pi(a|s,M) = \Sigma_{c \in M} \mu(c|s,M) p_{LLM}(a|s,c)$ 
* CBR Agent의 trajectory $\tau$ 는 다음과 같이 나타낼 수 있다.

![image](../images/Memento_2.PNG)

[(출처)](https://arxiv.org/pdf/2508.16153) : Huichi Zhou and Yihang Chen et al., "Memento: Fine-tuning LLM Agents without Fine-tuning LLMs"

| 구분         | 요소                                                 |
|------------|----------------------------------------------------|
| Agent 의 행동 | ```Retrieve``` ```Reuse and Revise``` ```Retain``` |
| 환경 관련 요소   | ```Evaluation``` ```Transition```                  |

### 2-2. Soft Q-Learning for CBR Agent

### 2-3. Enhance Q-Learning Based on State Similarity

## 3. 실제 구현

### 3-1. 프레임워크

### 3-2. Case Memory 관리

### 3-3. 도구 사용 (Tool Usage)

## 4. 실험 및 그 결과

### 4-1. 데이터셋 및 모델

### 4-2. 평가 Metric

### 4-3. 실험 결과 (기본)

### 4-4. 실험 결과 (Ablation Study)

## 5. 논의 사항 및 분석

