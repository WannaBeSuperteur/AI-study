## Large Language Model

* **Total 28 Papers | 30 Documents** (2025.09.24)
  * Including 28 Generative AI Papers
  * üíª (Single LLM Product / Model), ü§ñ (LLM Agent), üß™ (LLM Methodology), ‚öñ (LLM Ethics), üß† [(LLM Reasoning)](../../AI%20Basics/LLM%20Basics/LLM_Í∏∞Ï¥à_Ï∂îÎ°†Ìòï_Î™®Îç∏.md), üé≠ (multi-modal), üí¨ (others)

| Study Date<br>(Study Doc. Link)                                                                                                                                                      | Paper                                                                                                                                                                 | Published | ÎπÑÍ≥†                                                                                                                                      |
|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------|-----------------------------------------------------------------------------------------------------------------------------------------|
| 2025.03.12 - 03.13 [(Study Doc)](%5B2025.03.12%5D%20LLaMA%20-%20Open%20and%20Efficient%20Foundation%20Language%20Models.md)                                                          | üíª [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/pdf/2302.13971)                                                                           | 2023.02   | [DeepSeek ÌôúÏö© ÌîÑÎ°úÏ†ùÌä∏ (2025.03.12 - 03.27)](https://github.com/WannaBeSuperteur/AI_Projects/tree/main/2025_03_12_DeepSeek_LLM) Í¥ÄÎ†® ÎÖºÎ¨∏Ïùò Ï∞∏Í≥† ÎÖºÎ¨∏ |
| 2025.03.13 [(Study Doc)](%5B2025.03.13%5D%20DeepSeek%20LLM%20Scaling%20Open-Source%20Language%20Models%20with%20Longtermism.md)                                                      | üíª [DeepSeek LLM Scaling Open-Source Language Models with Longtermism](https://arxiv.org/pdf/2401.02954)                                                              | 2024.01   | DeepSeek ÌôúÏö© ÌîÑÎ°úÏ†ùÌä∏ (2025.03.12 - 03.27) Í¥ÄÎ†®                                                                                                |
| 2025.03.13 [(Study Doc)](%5B2025.03.13%5D%20DeepSeek-R1%20-%20Incentivizing%20Reasoning%20Capability%20in%20LLM%20via%20Reinforcement%20Learning.md)                                 | üíª [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](https://arxiv.org/pdf/2501.12948)                                             | 2025.01   | DeepSeek ÌôúÏö© ÌîÑÎ°úÏ†ùÌä∏ (2025.03.12 - 03.27) Í¥ÄÎ†®                                                                                                |
| 2025.03.22 + 2025.03.27 [(Study Doc)](%5B2025.03.22%5D%20Large%20Language%20Models%20A%20Survey%20(IV,%20VII).md)                                                                    | üí¨ [Large Language Models: A Survey](https://arxiv.org/pdf/2402.06196) - Part III (G,H,I), IV, VII                                                                    | 2024.02   |                                                                                                                                         |
| 2025.04.04 [(Study Doc)](%5B2025.04.04%5D%20A%20Survey%20on%20Large%20Language%20Models%20with%20some%20Insights%20on%20their%20Capabilities%20and%20Limitations%20(3.1,%203.3).md)  | üí¨ [A Survey on Large Language Models with some Insights on their Capabilities and Limitations](https://arxiv.org/pdf/2501.04040) - Part 3.1, 3.3                     | 2025.01   |                                                                                                                                         |
| 2025.04.09 [(Study Doc)](%5B2025.04.09%5D%20KTO%20-%20Model%20Alignment%20as%20Prospect%20Theoretic%20Optimization.md)                                                               | üß™ [KTO: Model Alignment as Prospect Theoretic Optimization](https://arxiv.org/pdf/2402.01306)                                                                        | 2024.02   | [Í∞ÄÏÉÅ Ïù∏Í∞Ñ ÏÉùÏÑ± ÌîÑÎ°úÏ†ùÌä∏ (Oh-LoRA v1) (2025.04.08 - 04.25)](https://github.com/WannaBeSuperteur/AI_Projects/tree/main/2025_04_08_OhLoRA) Í¥ÄÎ†® ÎÖºÎ¨∏    |
| 2025.04.16 - 04.19 [(Study Doc)](%5B2025.04.16%5D%20A%20Survey%20on%20Large%20Language%20Models%20with%20some%20Insights%20on%20their%20Capabilities%20and%20Limitations%20(3.5).md) | üí¨ [A Survey on Large Language Models with some Insights on their Capabilities and Limitations](https://arxiv.org/pdf/2501.04040) - Part 3.5                          | 2025.01   |                                                                                                                                         |
| 2025.04.22 - 04.26 [(Study Doc)](%5B2025.04.22%5D%20A%20Survey%20on%20Large%20Language%20Models%20with%20some%20Insights%20on%20their%20Capabilities%20and%20Limitations%20(3.6).md) | üí¨ [A Survey on Large Language Models with some Insights on their Capabilities and Limitations](https://arxiv.org/pdf/2501.04040) - Part 3.6                          | 2025.01   |                                                                                                                                         |
| 2025.04.27 + 2025.05.02 [(Study Doc)](%5B2025.04.27%5D%20Train%20Small,%20Infer%20Large%20-%20Memory-Efficient%20LoRA%20Training%20for%20Large%20Language%20Models.md)               | üß™ [Train Small, Infer Large - Memory-Efficient LoRA Training for Large Language Models](https://arxiv.org/pdf/2502.13533)                                            | 2025.02   |                                                                                                                                         |
| 2025.05.05 - 05.08 [(Study Doc)](%5B2025.05.05%5D%20A%20Survey%20on%20Large%20Language%20Model%20Hallucination%20via%20a%20Creativity%20Perspective.md)                              | üí¨ [A Survey on Large Language Model Hallucination via a Creativity Perspective](https://arxiv.org/pdf/2402.06647)                                                    | 2024.02   |                                                                                                                                         |
| 2025.05.14 - 05.16 [(Study Doc)](%5B2025.05.14%5D%20A%20Tutorial%20on%20LLM%20Reasoning%20-%20Relevant%20Methods%20behind%20ChatGPT%20o1.md)                                         | üß† [A Tutorial on LLM Reasoning: Relevant Methods behind ChatGPT o1](https://arxiv.org/pdf/2502.10867)                                                                | 2025.02   |                                                                                                                                         |
| 2025.05.18 - 05.22 [(Study Doc)](%5B2025.05.18%5D%20Can%201B%20LLM%20Surpass%20405B%20LLM%20-%20Rethinking%20Compute-Optimal%20Test-Time%20Scaling.md)                               | üß† [Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling](https://arxiv.org/pdf/2502.06703)                                                      | 2025.02   |                                                                                                                                         |
| 2025.05.26 + 2025.05.29 [(Study Doc)](%5B2025.05.26%5D%20CodeCoR%20-%20An%20LLM-Based%20Self-Reflective%20Multi-Agent%20Framework%20for%20Code%20Generation.md)                      | ü§ñ [CodeCoR: An LLM-Based Self-Reflective Multi-Agent Framework for Code Generation](https://arxiv.org/pdf/2501.07811)                                                | 2025.01   |                                                                                                                                         |
| 2025.06.07 [(Study Doc)](%5B2025.06.07%5D%20Can%20LLM%20feedback%20enhance%20review%20quality%20-%20A%20randomized%20study%20of%2020K%20reviews%20at%20ICLR%202025.md)               | ü§ñ [Can LLM feedback enhance review quality? A randomized study of 20K reviews at ICLR 2025](https://arxiv.org/pdf/2504.09737)                                        | 2025.04   |                                                                                                                                         |
| 2025.06.09 - 06.10 [(Study Doc)](%5B2025.06.09%5D%20LLM%20Agents%20for%20Education%20-%20Advances%20and%20Applications.md)                                                           | ü§ñ [LLM Agents for Education: Advances and Applications](https://arxiv.org/pdf/2503.11733)                                                                            | 2025.03   |                                                                                                                                         |
| 2025.06.19 - 06.20 [(Study Doc)](%5B2025.06.19%5D%20A%20Survey%20on%20Trustworthy%20LLM%20Agents%20-%20Threats%20and%20Countermeasures.md)                                           | ‚öñ [A Survey on Trustworthy LLM Agents: Threats and Countermeasures](https://arxiv.org/pdf/2503.09648)                                                                 | 2025.03   | [Oh-LoRA v4 (2025.06.24 - 06.30)](https://github.com/WannaBeSuperteur/AI_Projects/tree/main/2025_06_24_OhLoRA_v4) Í∞úÎ∞ú Ïãú Ï∞∏Í≥† ÎÖºÎ¨∏            |
| 2025.06.23 [(Study Doc)](%5B2025.06.23%5D%20LLM%20Agents%20Making%20Agent%20Tools.md)                                                                                                | ü§ñ [LLM Agents Making Agent Tools](https://arxiv.org/pdf/2502.11705?)                                                                                                 | 2025.02   |                                                                                                                                         |
| 2025.07.01 - 07.02 [(Study Doc)](%5B2025.07.01%5D%20Large%20Language%20Diffusion%20Models.md)                                                                                        | üß™ [Large Language Diffusion Models](https://arxiv.org/pdf/2502.09992)                                                                                                | 2025.02   |                                                                                                                                         |
| 2025.07.11 [(Study Doc)](%5B2025.07.11%5D%20LLM-Based%20Multi-Agent%20Systems%20for%20Software%20Engineering%20-%20Literature%20Review,%20Vision%20and%20the%20Road%20Ahead.md)      | ü§ñ [LLM-Based Multi-Agent Systems for Software Engineering: Literature Review, Vision and the Road Ahead](https://arxiv.org/pdf/2404.04834)                           | 2024.04   |                                                                                                                                         |
| 2025.07.19 - 07.20 [(Study Doc)](%5B2025.07.19%5D%20Why%20Do%20Multi-Agent%20LLM%20Systems%20Fail.md)                                                                                | ü§ñ [Why Do Multi-Agent LLM Systems Fail?](https://arxiv.org/pdf/2503.13657)                                                                                           | 2025.03   |                                                                                                                                         |
| 2025.07.26 [(Study Doc)](%5B2025.07.26%5D%20Revisiting%20Multi-Modal%20LLM%20Evaluation.md)                                                                                          | üé≠ [Revisiting Multi-Modal LLM Evaluation](https://openaccess.thecvf.com/content/CVPR2025W/BEAM/papers/Lu_Revisiting_Multi-Modal_LLM_Evaluation_CVPRW_2025_paper.pdf) | 2025      |                                                                                                                                         |
| 2025.08.04 [(Study Doc)](%5B2025.08.04%5D%20Agent%20Laboratory%20-%20Using%20LLM%20Agents%20as%20Research%20Assistants.md)                                                           | ü§ñ [Agent Laboratory: Using LLM Agents as Research Assistants](https://arxiv.org/pdf/2501.04227)                                                                      | 2025.01   |                                                                                                                                         |
| 2025.08.10 [(Study Doc)](%5B2025.08.10%5D%20Orak%20-%20A%20Foundational%20Benchmark%20for%20Training%20and%20Evaluating%20LLM%20Agents%20on%20Diverse%20Video%20Games.md)            | üí¨ [Orak: A Foundational Benchmark for Training and Evaluating LLM Agents on Diverse Video Games](https://arxiv.org/pdf/2506.03610)                                   | 2025.06   | [AI Trend (2025.06.16)](../../AI%20Trend/AI_TREND_Jun_2025.md#20250616-Ïõî)                                                               |
| 2025.08.17 - 08.18 [(Study Doc)](%5B2025.08.17%5D%20Context-Independent%20OCR%20with%20Multimodal%20LLMs%20-%20Effects%20of%20Image%20Resolution%20and%20Visual%20Complexity.md)     | üé≠ [Context-Independent OCR with Multimodal LLMs: Effects of Image Resolution and Visual Complexity](https://arxiv.org/pdf/2503.23667)                                | 2025.03   |                                                                                                                                         |
| 2025.08.21 - 08.22 [(Study Doc)](%5B2025.08.21%5D%20Thinkless%20-%20LLM%20Learns%20When%20to%20Think.md)                                                                             | üß™ [Thinkless: LLM Learns When to Think](https://arxiv.org/pdf/2505.13379)                                                                                            | 2025.05   |                                                                                                                                         |
| 2025.08.30 + 2025.09.01 [(Study Doc)](%5B2025.08.30%5D%20A%20Minimalist%20Approach%20to%20LLM%20Reasoning%20-%20from%20Rejection%20Sampling%20to%20Reinforce.md)                     | üß† [A Minimalist Approach to LLM Reasoning: from Rejection Sampling to Reinforce](https://arxiv.org/pdf/2504.11343)                                                   | 2025.04   |                                                                                                                                         |
| 2025.09.04 - 09.06 [(Study Doc)](%5B2025.09.04%5D%20ChainBuddy%20-%20An%20AI%20Agent%20System%20for%20Generating%20LLM%20Pipelines.md)                                               | ü§ñ [ChainBuddy: An AI Agent System for Generating LLM Pipelines](https://arxiv.org/pdf/2409.13588)                                                                    | 2024.09   |                                                                                                                                         |
| 2025.09.09 - 09.12 [(Study Doc)](%5B2025.09.09%5D%20Why%20Language%20Models%20Hallucinate.md)                                                                                        | ‚öñ [Why Language Models Hallucinate](https://cdn.openai.com/pdf/d04913be-3f6f-4d2b-b283-ff432ef4aaa5/why-language-models-hallucinate.pdf)                              | 2025.09   |                                                                                                                                         |
| 2025.09.15 [(Study Doc)](%5B2025.09.15%5D%20PaSa%20-%20An%20LLM%20Agent%20for%20Comprehensive%20Academic%20Paper%20Search.md)                                                        | ü§ñ [PaSa: An LLM Agent for Comprehensive Academic Paper Search](https://arxiv.org/pdf/2501.10120)                                                                     | 2025.01   |                                                                                                                                         |
| 2025.09.24 - 09.25 [(Study Doc)](%5B2025.09.24%5D%20HalluLens%20-%20LLM%20Hallucination%20Benchmark.md)                                                                              | ‚öñ [HalluLens: LLM Hallucination Benchmark](https://arxiv.org/pdf/2504.17550)                                                                                          | 2025.04   |                                                                                                                                         | 
