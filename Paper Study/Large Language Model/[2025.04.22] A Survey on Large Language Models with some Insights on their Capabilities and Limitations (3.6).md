## 목차

* [1. Parameter-efficient Model Adaption](#1-parameter-efficient-model-adaption)
  * [1-1. Adapter Tuning](#1-1-adapter-tuning)
  * [1-2. Prefix Tuning](#1-2-prefix-tuning)
  * [1-3. Prompt Tuning](#1-3-prompt-tuning)
  * [1-4. LoRA (Low-Rank Adaption)](#1-4-lora-low-rank-adaption)
* [2. Memory-efficient Model Adaption](#2-memory-efficient-model-adaption)
  * [2-1. Quantization](#2-1-quantization)

## 논문 소개

* Andrea Matarazzo and Riccardo Torlone, "A Survey on Large Language Models with some Insights on their Capabilities and Limitations", 2025
* [arXiv Link](https://arxiv.org/pdf/2501.04040)
* 이 문서에서 다룰 파트
  * "3. Foundations of Large Language Models" 중 **3-6. Tuning and Optimization**
* 참고하면 좋은 문서
  * [LLM Fine-Tuning 방법 설명](../../AI%20Basics/LLM%20Basics/LLM_기초_Fine_Tuning.md)
  * [LLM Quantization 방법 설명](../../AI%20Basics/LLM%20Basics/LLM_기초_Quantization.md)

## 1. Parameter-efficient Model Adaption

Parameter-efficient Model Adaption 은 **LLM 을 Fine-Tuning 할 때, 그 성능 향상도는 최대한 유지하면서 학습하는 파라미터의 개수를 줄이는** 것이다.

* 이러한 Fine-Tuning 을 [PEFT (Parameter-Efficient Fine-Tuning)](../../AI%20Basics/LLM%20Basics/LLM_기초_Fine_Tuning_PEFT.md) 라고 한다.

그 방법으로는 대표적으로 **Adapter Tuning, Prefix Tuning, Prompt Tuning, LoRA (Low-Rank Adaption)** 의 4가지가 있다.

| 방법                       | 설명                                                                                                                              |
|--------------------------|---------------------------------------------------------------------------------------------------------------------------------|
| Adapter Tuning           | 'Adapter'라고 하는, 학습 가능한 작은 모듈을 layer 사이에 추가<br>- Fine-Tuning 대상인 새로운 task 가 **전체 모델을 재 학습 시켜야 하는 수준** 일 때의 Fine-Tuning 의 비효율성 해결 |
| Prefix Tuning            | 'prefix'라고 하는, task-specific vector 를 LLM 입력에 추가                                                                                |
| Prompt Tuning            | 'prompt token'이라는, 학습 가능한 vector 를 LLM 에 포함                                                                                     |
| LoRA (Low-Rank Adaption) | 원래 가중치 행렬 $W$ 를 **2개의 작은 행렬 $A$, $B$ 로 분해** 하여, **메모리 사용량 및 연산량을 드라마틱하게 감소**                                                    |

![image](../images/LLM_Survey_250422_1.PNG)

[(출처)](https://arxiv.org/pdf/2501.04040) : Andrea Matarazzo and Riccardo Torlone, "A Survey on Large Language Models with some Insights on their Capabilities and Limitations"

### 1-1. Adapter Tuning

**Adapter Tuning** 은 'Adapter'라고 하는, 학습 가능한 작은 모듈을 layer 사이에 추가하여 **해당 모듈을 추가 학습** 시키는 것이다.

* 기본 학습 방법
  * 학습 가능한 Adapter Layer 를 기존 Feed-Forward Layer 와 [Layer Normalization](../../AI%20Basics/Deep%20Learning%20Basics/딥러닝_기초_Regularization.md#4-2-layer-normalization) 사이에 추가한다.
  * 원래 모델에 있던 나머지 파라미터들은 모두 **Frozen 상태로 둔다.**
* 특히 유용한 경우
  * Fine-Tuning 대상인 새로운 task 를 학습하려면 **전체 모델을 재 학습 시켜야 하는 수준** 일 때, 이러한 비효율성을 해결하기 위한 것이다.
  * 즉, **기존 Pre-train 된 task와 다른 새로운 task 를 여러 개 학습** 할 때 유용한 방법이라고 할 수 있다.
* [참고 문서 (Adapter Layer 추가)](../../AI%20Basics/LLM%20Basics/LLM_기초_Fine_Tuning_PEFT.md#2-5-adapter-layer-추가)

**1. Adapter Tuning 의 레이어 구조**

![image](../images/LLM_Survey_250422_2.PNG)

[(출처)](https://arxiv.org/pdf/2501.04040) : Andrea Matarazzo and Riccardo Torlone, "A Survey on Large Language Models with some Insights on their Capabilities and Limitations"

**2. Feed-forward Down-project & Feed-forward Up-project 레이어**

Adapter Layer 는 다음과 같이 **Feed-forward Down-project** 와 **Feed-forward Up-project** 레이어로 구성된다.

| 레이어                       | 설명                                                                            | 수식                         |
|---------------------------|-------------------------------------------------------------------------------|----------------------------|
| Feed-forward Down-project | 입력 차원 $d$ 을 더 작은 중간 차원 $m$ 으로 mapping<br>- ($x (dim: d)$ → $y (dim: m)$)      | $y = \sigma (W_d x + b_d)$ |
| Feed-forward Up-project   | 중간 차원 $m$ 을 입력 차원과 동일한 출력 차원 $d$ 로 mapping<br>- ($y (dim: m)$ → $z (dim: d)$) | $z = \sigma (W_u y + b_u)$ |

* 이때 출력 벡터 $z$ 는 **입력 벡터 $x$ 에 대한 reconstruction** 에 해당한다.

**3. Parallel Adapter**

Transformer 에서는 **Parallel Adapter** 라는 기법을 사용하는데, 그 컨셉은 다음과 같다.

* Adapter 가 Attention Layer 와 Feed-Forward Layer 각각에 대해 **병렬적으로 추가됨**
* LLM Fine-Tuning 에서의 학습 가능한 파라미터 개수를 효과적으로 줄일 수 있음

### 1-2. Prefix Tuning

### 1-3. Prompt Tuning

### 1-4. LoRA (Low-Rank Adaption)

## 2. Memory-efficient Model Adaption

### 2-1. Quantization