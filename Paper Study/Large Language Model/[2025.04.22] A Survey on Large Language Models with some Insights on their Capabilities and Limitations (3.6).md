## 목차

* [1. Parameter-efficient Model Adaption](#1-parameter-efficient-model-adaption)
  * [1-1. Adapter Tuning](#1-1-adapter-tuning)
  * [1-2. Prefix Tuning](#1-2-prefix-tuning)
  * [1-3. Prompt Tuning](#1-3-prompt-tuning)
  * [1-4. LoRA (Low-Rank Adaption)](#1-4-lora-low-rank-adaption)
* [2. Memory-efficient Model Adaption](#2-memory-efficient-model-adaption)
  * [2-1. Quantization](#2-1-quantization)

## 논문 소개

* Andrea Matarazzo and Riccardo Torlone, "A Survey on Large Language Models with some Insights on their Capabilities and Limitations", 2025
* [arXiv Link](https://arxiv.org/pdf/2501.04040)
* 이 문서에서 다룰 파트
  * "3. Foundations of Large Language Models" 중 **3-6. Tuning and Optimization**
* 참고하면 좋은 문서
  * [LLM Fine-Tuning 방법 설명](../../AI%20Basics/LLM%20Basics/LLM_기초_Fine_Tuning.md)
  * [LLM Quantization 방법 설명](../../AI%20Basics/LLM%20Basics/LLM_기초_Quantization.md)

## 1. Parameter-efficient Model Adaption

### 1-1. Adapter Tuning

### 1-2. Prefix Tuning

### 1-3. Prompt Tuning

### 1-4. LoRA (Low-Rank Adaption)

## 2. Memory-efficient Model Adaption

### 2-1. Quantization