## 목차

* [1. Parameter-efficient Model Adaption](#1-parameter-efficient-model-adaption)
  * [1-1. Adapter Tuning](#1-1-adapter-tuning)
  * [1-2. Prefix Tuning](#1-2-prefix-tuning)
  * [1-3. Prompt Tuning](#1-3-prompt-tuning)
  * [1-4. LoRA (Low-Rank Adaption)](#1-4-lora-low-rank-adaption)
* [2. Memory-efficient Model Adaption](#2-memory-efficient-model-adaption)
  * [2-1. Quantization](#2-1-quantization)

## 논문 소개

* Andrea Matarazzo and Riccardo Torlone, "A Survey on Large Language Models with some Insights on their Capabilities and Limitations", 2025
* [arXiv Link](https://arxiv.org/pdf/2501.04040)
* 이 문서에서 다룰 파트
  * "3. Foundations of Large Language Models" 중 **3-6. Tuning and Optimization**
* 참고하면 좋은 문서
  * [LLM Fine-Tuning 방법 설명](../../AI%20Basics/LLM%20Basics/LLM_기초_Fine_Tuning.md)
  * [LLM Quantization 방법 설명](../../AI%20Basics/LLM%20Basics/LLM_기초_Quantization.md)

## 1. Parameter-efficient Model Adaption

Parameter-efficient Model Adaption 은 **LLM 을 Fine-Tuning 할 때, 그 성능 향상도는 최대한 유지하면서 학습하는 파라미터의 개수를 줄이는** 것이다.

* 이러한 Fine-Tuning 을 [PEFT (Parameter-Efficient Fine-Tuning)](../../AI%20Basics/LLM%20Basics/LLM_기초_Fine_Tuning_PEFT.md) 라고 한다.

그 방법으로는 대표적으로 **Adapter Tuning, Prefix Tuning, Prompt Tuning, LoRA (Low-Rank Adaption)** 의 4가지가 있다.

| 방법                       | 설명                                                                                                                              |
|--------------------------|---------------------------------------------------------------------------------------------------------------------------------|
| Adapter Tuning           | 'Adapter'라고 하는, 학습 가능한 작은 모듈을 layer 사이에 추가<br>- Fine-Tuning 대상인 새로운 task 가 **전체 모델을 재 학습 시켜야 하는 수준** 일 때의 Fine-Tuning 의 비효율성 해결 |
| Prefix Tuning            | 'prefix'라고 하는, task-specific vector 를 LLM 입력에 추가                                                                                |
| Prompt Tuning            | 'prompt token'이라는, 학습 가능한 vector 를 LLM 에 포함                                                                                     |
| LoRA (Low-Rank Adaption) | 원래 가중치 행렬 $W$ 를 **2개의 작은 행렬 $A$, $B$ 로 분해** 하여, **메모리 사용량 및 연산량을 드라마틱하게 감소**                                                    |

![image](../images/LLM_Survey_250422_1.PNG)

[(출처)](https://arxiv.org/pdf/2501.04040) : Andrea Matarazzo and Riccardo Torlone, "A Survey on Large Language Models with some Insights on their Capabilities and Limitations"

### 1-1. Adapter Tuning

### 1-2. Prefix Tuning

### 1-3. Prompt Tuning

### 1-4. LoRA (Low-Rank Adaption)

## 2. Memory-efficient Model Adaption

### 2-1. Quantization