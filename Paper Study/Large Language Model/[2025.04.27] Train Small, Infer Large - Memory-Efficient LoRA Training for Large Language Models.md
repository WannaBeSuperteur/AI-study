## 목차

* [1. LoRAM 의 핵심 아이디어](#1-loram-의-핵심-아이디어)
* [2. 학습 메커니즘](#2-학습-메커니즘)
  * [2-1. Pruned Full-Rank Weight Generation](#2-1-pruned-full-rank-weight-generation)
  * [2-2. Pruned Low-Rank Matrix Training](#2-2-pruned-low-rank-matrix-training)
  * [2-3. Recovered Low-Rank Matrix Generation](#2-3-recovered-low-rank-matrix-generation)
  * [2-4. Pruned Full-Rank Weight Alignment](#2-4-pruned-full-rank-weight-alignment)
* [3. Inference 메커니즘 (Recovered Low-Rank Matrix Inference)](#3-inference-메커니즘-recovered-low-rank-matrix-inference)
* [4. 양자화 메커니즘 (Pruned Full-Rank Weight Quantization)](#4-양자화-메커니즘-pruned-full-rank-weight-quantization)
* [5. 실험 결과](#5-실험-결과)
  * [5-1. 실험 설정](#5-1-실험-설정)
  * [5-2. Fine-Tuning 실험 결과](#5-2-fine-tuning-실험-결과)
  * [5-3. Downstream Task 실험 결과](#5-3-downstream-task-실험-결과)
  * [5-4. LLaMA-3.1 실험 결과](#5-4-llama-31-실험-결과)
* [6. 논의 및 탐구](#6-논의-및-탐구)
  * [6-1. Recovery 및 Alignment 의 필요성](#6-1-recovery-및-alignment-의-필요성)
  * [6-2. LoRAM 의 Parameter Reduction 에 대한 스케일링 법칙](#6-2-loram-의-parameter-reduction-에-대한-스케일링-법칙)

## 논문 소개

* Jun Zhang, Jue Wang et al., "Train Small, Infer Large: Memory-Efficient LoRA Training for Large Language Models", 2025
* Memory-Efficient 한 [LoRA](../../AI%20Basics/LLM%20Basics/LLM_기초_Fine_Tuning_LoRA_QLoRA.md) 학습 방법인 **LoRAM** 에 대한 소개 논문
* [arXiv Link](https://arxiv.org/pdf/2502.13533)

## 1. LoRAM 의 핵심 아이디어

## 2. 학습 메커니즘

### 2-1. Pruned Full-Rank Weight Generation

### 2-2. Pruned Low-Rank Matrix Training

### 2-3. Recovered Low-Rank Matrix Generation

### 2-4. Pruned Full-Rank Weight Alignment

## 3. Inference 메커니즘 (Recovered Low-Rank Matrix Inference)

## 4. 양자화 메커니즘 (Pruned Full-Rank Weight Quantization)

## 5. 실험 결과

### 5-1. 실험 설정

### 5-2. Fine-Tuning 실험 결과

### 5-3. Downstream Task 실험 결과

### 5-4. LLaMA-3.1 실험 결과

## 6. 논의 및 탐구

### 6-1. Recovery 및 Alignment 의 필요성

### 6-2. LoRAM 의 Parameter Reduction 에 대한 스케일링 법칙