## 목차

* [1. LoRAM 의 핵심 아이디어](#1-loram-의-핵심-아이디어)
* [2. 학습 메커니즘](#2-학습-메커니즘)
  * [2-1. Pruned Full-Rank Weight Generation](#2-1-pruned-full-rank-weight-generation)
  * [2-2. Pruned Low-Rank Matrix Training](#2-2-pruned-low-rank-matrix-training)
  * [2-3. Recovered Low-Rank Matrix Generation](#2-3-recovered-low-rank-matrix-generation)
  * [2-4. Pruned Full-Rank Weight Alignment](#2-4-pruned-full-rank-weight-alignment)
* [3. Inference 메커니즘 (Recovered Low-Rank Matrix Inference)](#3-inference-메커니즘-recovered-low-rank-matrix-inference)
* [4. 양자화 메커니즘 (Pruned Full-Rank Weight Quantization)](#4-양자화-메커니즘-pruned-full-rank-weight-quantization)
* [5. 실험 결과](#5-실험-결과)
  * [5-1. 실험 설정](#5-1-실험-설정)
  * [5-2. Fine-Tuning 실험 결과](#5-2-fine-tuning-실험-결과)
  * [5-3. Downstream Task 실험 결과](#5-3-downstream-task-실험-결과)
  * [5-4. LLaMA-3.1 실험 결과](#5-4-llama-31-실험-결과)
* [6. 논의 및 탐구](#6-논의-및-탐구)
  * [6-1. Recovery 및 Alignment 의 필요성](#6-1-recovery-및-alignment-의-필요성)
  * [6-2. LoRAM 의 Parameter Reduction 에 대한 스케일링 법칙](#6-2-loram-의-parameter-reduction-에-대한-스케일링-법칙)

## 논문 소개

* Jun Zhang, Jue Wang et al., "Train Small, Infer Large: Memory-Efficient LoRA Training for Large Language Models", 2025
* Memory-Efficient 한 [LoRA](../../AI%20Basics/LLM%20Basics/LLM_기초_Fine_Tuning_LoRA_QLoRA.md) 학습 방법인 **LoRAM** 에 대한 소개 논문
* [arXiv Link](https://arxiv.org/pdf/2502.13533)

## 1. LoRAM 의 핵심 아이디어

LoRAM 을 통해 해결해야 할 문제

* LoRAM 은 [LoRA (Low-Rank Optimization)](../../AI%20Basics/LLM%20Basics/LLM_기초_Fine_Tuning_LoRA_QLoRA.md) 를 통한 LLM Fine-Tuning 시, Pre-trained Weight Matrix **$W_0$ (LoRA 를 통해 2개의 행렬로 분해할 원래 행렬)** 로 인한 memory overhead 를 줄이면서도 그 성능을 유지하기 위한 방법이다.
* 즉, LoRA 를 **Memory-Efficient 하게 학습** 하기 위한 방법이다.

LoRAM 의 핵심 아이디어

* Low-Rank 행렬에 대한 **Pruning 및 복원** 메커니즘
  * LoRA 의 **Low-Rank 행렬들을 Pruning** 한 **Pruned Model** 을 학습한다.
  * Inference 시에는 **복원된 Low-Rank 행렬들을 원래 모델과 결합** 한다.
* 원래 모델과 Pruned Model 간의 **Knowledge Inconsistency 해결**
  * Pruned Model 을 적은 양의 일반적인 corpus 를 이용하여 학습하는 방법으로 Fine-Tuning
  * **One-shot offline process** 를 이용

## 2. 학습 메커니즘

여기서 LoRA 의 수식을 다음과 같다고 먼저 가정한다.

* 수식
  * $h = xW_0 + xW_{LoRA} = xW_0 + xBA$
* 수식 설명
  * $W_0$ 은 LoRA 를 적용하지 않은 원래의 High-Rank 행렬 
  * $A$, $B$ 는 LoRA 적용을 위한 Low-Rank 행렬 

LoRAM 의 기본적인 학습 메커니즘은 다음과 같다.

| 학습 메커니즘                              | 설명                                                                                                                                                                                                               |
|--------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Pruned Full-Rank Weight Generation   | - Pruning 알고리즘을 이용하여 원래 행렬 $W_0$ 으로부터 Pruning 된 행렬 $W_0^P$ 를 얻음                                                                                                                                                  |
| Pruned Low-Rank Matrix Training      | - $W_0^P$ 를 frozen 시킨 채로 유지<br>- Low-Rank 행렬 $A$, $B$ 를 각각 Pruning 시킨 행렬을 각각 $A^P$, $B^P$ 이라 할 때,<br>- $A^P$, $B^P$ 를 이용하여, **Pruning 된 Low-Rank decomposition 행렬 $W^P_{LoRA} = W_{LoRA} ◦ M^P = B^P A^P$ 를 학습** |
| Recovered Low-Rank Matrix Generation | - 학습된 Low-Rank 행렬을 $W_{LoRA}^{P*}$ 라 할 때,<br>- $W_{LoRA}^{P*}$ 와 Pruning Mask $M^P$ 를 이용하여 **복원된 Low-Rank 행렬** $W_{LoRA}^{R*}$ 계산                                                                                |
| Pruned Full-Rank Weight Alignment    | - **Pruned weight** $W_0^P$ 와 **원래의 High-Rank 행렬** $W_0$ 간의 **Knowledge Mismatch 를 해결** 해야 함<br>- 이를 위해, Pruned weight $W_0^P$ 를 일반적인 corpus 인 $D_A$ 를 이용하여 추가 학습                                                |

![image](../images/LoRAM_1.PNG)

[(출처)](https://arxiv.org/pdf/2502.13533) : Jun Zhang, Jue Wang et al., "Train Small, Infer Large: Memory-Efficient LoRA Training for Large Language Models"

### 2-1. Pruned Full-Rank Weight Generation

먼저, **Pruning 알고리즘 $P$** 를 통해, $W_0$ 에 대해 **Pruned weight matrix $W_0^P$** 를 계산한다.

* 수식
  * $W_0^P = P(W_0) = W_0 ◦ M^P$
* 수식 설명
  * $M^P \in {0,1}^{m \times n}$ : Pruning Mask
    * retained/pruned parameter 를 나타내는 binary mask 행렬
    * **1 성분은 대응되는 원본 성분을 유지, 0 성분은 원본 성분을 Pruning** 함을 의미
  * ◦ : 행렬 간의 Hadamard Product

### 2-2. Pruned Low-Rank Matrix Training

**1. 기본 설명**

* 원래의 LoRA 방식을 수정하여 **Pruned Low-Rank Decomposition Matrix $W_{LoRA}^P$** 를 학습
* 이때, Pruned weight matrix $W_0^P$ 를 이용

**2. 수식**

* Pruned Low-Rank Decomposition Matrix 계산
  * $W_{LoRA}^P = W_{LoRA} ◦ M^P = B^P A^P$
* Output Activation 계산
  * $h = xW_0^P + xW_{LoRA}^P = xW_0^P + x(B^P A^P)$ 

### 2-3. Recovered Low-Rank Matrix Generation

**1. 기본 설명**

* 먼저, Pruned Low-Rank Matrix $W_{LoRA}^{P*}$ 를 계산
  * 이때, Objective Function (Loss Function) 은 [LLM 의 Supervied Fune-Tuning](../../AI%20Basics/LLM%20Basics/LLM_기초_Fine_Tuning_SFT.md) 에서 사용하는 Loss 인 $L_{SFT}$ 를 사용 
* $W_{LoRA}^{P*}$ 를 이용하여, **Recovered Low-Rank Matrix** $W_{LoRA}^{R*}$ 를 계산
  * 이때, 원래의 weight 을 최대한 활용하여 **추론 성능을 최대한 향상** 시키기 위해, Pruning Mask $M^P$ 를 이용한 Recovery Function $R$ 을 이용

**2. 수식 및 설명**

* 수식
  * $W_{LoRA}^{R*}$ 계산
    * $W_{LoRA}^{R*} = B^{R*} A^{R*} = R(W_{LoRA}^{P*}) = W_{LoRA}^{P*} ◦ (1 - M^P)$ 
  * $W_0 + W_{LoRA}^{R*}$ 계산
    * $M^P [i, j] = 1$ (retain) 이면 → $(W_0 + W_{LoRA}^{R*})[i, j] = W_0[i, j]$
    * $M^P [i, j] = 0$ (pruned) 이면 → $(W_0 + W_{LoRA}^{R*})[i, j] = W_0[i, j] + W_{LoRA}^{R*}[i, j]$
* $W_0 + W_{LoRA}^{R*}$ 수식 설명
  * mask $M^P$ 의 값이 1인 위치 [i, j] → 원래 행렬 $W_0$ 의 값 유지
  * mask $M^P$ 의 값이 0인 위치 [i, j] → 원래 행렬 $W_0$ 의 값 + Recovered Low-Rank Matrix $W_{LoRA}^{R*}$ 의 해당 위치의 값

### 2-4. Pruned Full-Rank Weight Alignment

**1. 기본 설명 및 수식**

* 원래 행렬 $W_0$ 과 Pruning 된 행렬 $W_0^P$ 사이에 **Knowledge Mismatch** 발생
  * 그 원인은 Pruning function $P$ 에 의해 **원래 행렬 $W_0$ 의 weight 이 나타내는 지식이 일부 손상** 되었기 때문 
* 이러한 Knowledge Mismatch 해결을 위해, **Continual Pre-training** 이라는 방법 사용
  * Pruned weight $W_0^P$ 를 작은 규모의 general corpus $D_A$ 를 이용하여 추가 학습 
  * 이때, [Next Token Prediction Loss 컨셉](../../AI%20Basics/LLM%20Basics/LLM_기초_Fine_Tuning.md#5-llm-fine-tuning-의-loss-function) + [Log Likelihood](../../AI%20Basics/Data%20Science%20Basics/데이터_사이언스_기초_Probability_vs_Likelihood.md#3-1-log-likelihood-로그-가능도-로그-우도) 를 이용하는, 다음과 같은 **Alignment Loss $L_A$** 를 사용

![image](../images/LoRAM_2.PNG)

[(출처)](https://arxiv.org/pdf/2502.13533) : Jun Zhang, Jue Wang et al., "Train Small, Infer Large: Memory-Efficient LoRA Training for Large Language Models"

**2. 학습된 Optimal Low-Rank Matrix 대응 비교**

| Weight matrix                       | Optimal Trained Low-Rank Matrix                     |
|-------------------------------------|-----------------------------------------------------|
| $W_0$, original pre-trained weights | $W_{LoRA}^*$, **Optimal Low-rank** matrix           |
| $W_0^P$, **Pruned** weights         | $W_{LoRA}^{P*}$, Pruned **optimal Low-Rank** matrix |

**3. Knowledge Mismatch 의 원인 상세**

* $W_0$ 과 $W_0^P$ 의 encoding 된 knowledge 가 **서로 가깝게 align 되어 있지 않음**
* 이로 인해, $W_{LoRA}^{P*}$ 에 align 된 knowledge 와 $W_{LoRA}^*$ 에 align 된 knowledge 가 **서로 다름**
* 결국, Recovered Matrix $W_{LoRA}^{R*}$ 와 $W_0$ 의 knowledge alignment 가 서로 맞지 않음

## 3. Inference 메커니즘 (Recovered Low-Rank Matrix Inference)

## 4. 양자화 메커니즘 (Pruned Full-Rank Weight Quantization)

## 5. 실험 결과

### 5-1. 실험 설정

### 5-2. Fine-Tuning 실험 결과

### 5-3. Downstream Task 실험 결과

### 5-4. LLaMA-3.1 실험 결과

## 6. 논의 및 탐구

### 6-1. Recovery 및 Alignment 의 필요성

### 6-2. LoRAM 의 Parameter Reduction 에 대한 스케일링 법칙