
## 목차

* [1. 논문 개요](#1-논문-개요)
* [2. XAI를 이용하여 LLM 향상](#2-xai를-이용하여-llm-향상)
  * [2-1. Attribution Methods](#2-1-attribution-methods)
  * [2-2. LLM Component 해석](#2-2-llm-component-해석)
  * [2-3. Sample-based Explanation](#2-3-sample-based-explanation)
  * [2-4. 신뢰할 수 있는 LLM을 위한 설명 가능성 & Human Alignment](#2-4-신뢰할-수-있는-llm을-위한-설명-가능성--human-alignment)
  * [2-5. Explainable Prompt 를 이용한 LLM Inference](#2-5-explainable-prompt-를-이용한-llm-inference)
  * [2-6. Knowledge-enhanced Prompt 를 이용한 LLM Inference](#2-6-knowledge-enhanced-prompt-를-이용한-llm-inference)
  * [2-7. Data Augmentation 학습](#2-7-data-augmentation-학습)
* [3. LLM을 이용하여 XAI 향상](#3-llm을-이용하여-xai-향상)
  * [3-1. 사용자 친화적 설명 생성](#3-1-사용자-친화적-설명-생성)
  * [3-2. 설명을 이용한, 해석 가능한 AI 시스템 설계](#3-2-설명을-이용한-해석-가능한-ai-시스템-설계)
  * [3-3. Emulating Humans for XAI](#3-3-emulating-humans-for-xai)
* [4. 결론](#4-결론)

## 논문 소개

* Xuansheng Wu and Haiwan Zhao et al., "Usable XAI: 10 Strategies Towards Exploiting Explainability in the LLM Era", 2024
* [arXiv Link](https://arxiv.org/pdf/2403.08946)

## 1. 논문 개요

이 논문의 핵심 주제는 다음과 같다.

* LLM의 관점에서 **Usable XAI (eXplainable AI, 설명 가능한 AI)** 를 정의
* 다음을 위한 총 10가지 전략 소개 및 분석
  * **XAI를 이용** 하여 **LLM** 기반 AI 시스템을 설명하는 방법들 **(총 7가지)**
  * **LLM을 이용** 하여 **XAI** 기술을 개선하는 방법들 **(총 3가지)**

![image](../images/Usable_XAI_1.PNG)

[(출처)](https://arxiv.org/pdf/2403.08946) : Xuansheng Wu and Haiwan Zhao et al., "Usable XAI: 10 Strategies Towards Exploiting Explainability in the LLM Era"

## 2. XAI를 이용하여 LLM 향상

### 2-1. Attribution Methods

**Attribution Method** 는 **각 입력 feature 의 중요도 (= 모델의 예측에 기여한 정도) 를 측정** 하는 XAI 방법론이다.

* 생성형 AI (LLM 등) 의 경우에는 다음과 같은 원리이다.
  * **입력 $x$ 에 대한 전체적인 confidence $p(\hat{y} | x)$ 를 계산** 한다. ($x$ : input, $\hat{y}$ : 생성된 LLM 응답) 
  * input token $a_{n,m}$ 에 대한 중요도는 다음과 같이 **해당 토큰을 제외했을 때의 생성 확률과의 차이** 로 계산한다.

![image](../images/Usable_XAI_2.PNG)

[(출처)](https://arxiv.org/pdf/2403.08946) : Xuansheng Wu and Haiwan Zhao et al., "Usable XAI: 10 Strategies Towards Exploiting Explainability in the LLM Era"

| notation    | 설명                                                        |
|-------------|-----------------------------------------------------------|
| $a_{n,m}$   | 입력 토큰 $x_n$ 의 출력 토큰 $\hat{y}_m$ 에 대한 **중요도 (importance)** |
| $x_{/n}$    | 입력 $x$ 에서 $n$번째 token을 제외한 것                              |
| $\hat{y}_m$ | 각각의 output word                                           |
| $E[x_n]$    | token $x_n$ 에 대한 **Embedding**                            |

* 각 Attribution Method 별 생성 task에서의 시간 복잡도

![image](../images/Usable_XAI_3.PNG)

[(출처)](https://arxiv.org/pdf/2403.08946) : Xuansheng Wu and Haiwan Zhao et al., "Usable XAI: 10 Strategies Towards Exploiting Explainability in the LLM Era"

### 2-2. LLM Component 해석

**LLM 의 Component 를 해석 (LLM Components Interpretation)** 하는 것은 **LLM의 각 Component를 해석** 하기 위한 XAI 방법론을 말한다.

* 즉, LLM의 **internal (내부 구조) 에 대한 설명 방법** 이라고 할 수 있다.
* 구체적인 방법들은 다음과 같다.

![image](../images/Usable_XAI_4.PNG)

[(출처)](https://arxiv.org/pdf/2403.08946) : Xuansheng Wu and Haiwan Zhao et al., "Usable XAI: 10 Strategies Towards Exploiting Explainability in the LLM Era"

**1. LLM Component 해석 방법 기본 분류**

| 해석 방법 분류                 | 설명                                                                                                                                   |
|--------------------------|--------------------------------------------------------------------------------------------------------------------------------------|
| Model Component 해석       | LLM의 Self-Attention (ATT), Feed-forward Network (FFN) 과 같은 **개별 Component 를 분석** 하기 위한 방법                                            |
| Latent Representation 해석 | LLM의 행동을 해석하기 위해 **LLM의 Latent (Hidden) Representation** 을 해석하려고 함<br>- Latent Representation 안에 있는 **각 값을 해석하는 방법으로는 확실히 해석하기 어려움** |

**2. Model Component 해석 방법 상세**

| 해석 방법                        | 설명                                                                                |
|------------------------------|-----------------------------------------------------------------------------------|
| 각 단일 Component 에 대한 중요도 분석   | LLM의 **각 layer 의 contextual impact** 를 측정<br>- 각 Component 간 선형 관계 분석을 위한 수식에서 착안 |
| 여러 Component 에 대한 Circuit 분석 | 복잡한 생성 문제 해결을 위한 LLM의 **서로 다른 레이어 간 협업 방식** 탐구                                    |
| 사용성 분석                       | **Knowledge Editing & Model Pruning** 에 직접 적용 가능                                  |

**3. Latent Representation 해석 방법 상세**

| 해석 방법                                   | 설명                                                                                                                                                                                      |
|-----------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **Non-training** 기반 방법                  | 다음과 같은 방법 존재<br>- latent representation $x^l$ 을 **Output Embedding 으로 mapping**<br>- $x^l$ 을 **Input Embedding** 으로 취급하여 처리                                                             |
| **지도학습 (Supervised Training)** 기반 방법    | 다음에 초점을 두어 연구<br>- LLM의 latent space 가 특정 concept 을 인코딩하는지<br>- 해당 concept 이 **LLM의 예측에 기여하고, 얼마나 중요한지**                                                                                |
| **비지도학습 (Unsupervised Training)** 기반 방법 | 학습된 concept 를 벡터 $C$ 로 나타낼 때,<br>- **Orthogonal** Assumption ($CC^T = I$, 즉 Singular Vector Decomposition에 중점)<br>- **Sparse** Assumption (Top-k ($X · C^T$), 즉 Sparse Auto-Encoder 컨셉) |
| 사용성 분석                                  | 모델 개발 프로세스의 3단계, 즉 **Data Pre-processing & Training & Inference** 에 초점                                                                                                                  |

### 2-3. Sample-based Explanation

**Sample-based Explanation** 은 **특정한 학습 데이터 샘플을 이용하여 LLM의 생성 답변을 trace-back** 하는 것이다.

* 이 방법을 사용하는 목적은 다음과 같다.

| 목적               | 설명                                                                                                                                           |
|------------------|----------------------------------------------------------------------------------------------------------------------------------------------|
| 생성 결과에 대한 증거 제공  | LLM의 예측을 trace-back 하는 것은 **생성된 결과에 대한 증거를 제공** 한다.<br>- 이를 통해 **오류 케이스에서 모델을 더 잘 디버깅** 할 수 있고, 사용자로 하여금 모델의 신뢰성을 향상시킬 수 있다.                 |
| LLM의 일반화 메커니즘 이해 | 연구자들이 **LLM이 학습 데이터로부터 일반화를 하는 메커니즘** 을 이해하는 것을 돕는다.<br>- LLM의 출력이 학습 데이터의 token 순서 그대로 trace-back 되면, **LLM은 단순히 데이터를 기억하는 것** 으로 이해할 수 있다. |

* Sample-based Explanation 의 방법은 다음과 같다.

| 방법                               | 설명                                                                                                                                        |
|----------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------|
| Influence Function-based Methods | 학습 데이터 샘플 $z_i$ 의 학습 데이터셋에서의 영향력을 측정하기 위해 **Influence Function** 을 이용한다.<br>- 이를 이용하여 **특정 test sample 에 대한 prediction loss의 변화를 측정** 한다. |
| Embedding-based Methods          | Transformer 구조 안에 있는 **latent (hidden) representation** 을 이용한다.<br>- 이를 통해 **학습 데이터셋 샘플과 테스트 데이터셋 샘플** 간의 **semantic similarity** 를 계산한다. |

### 2-4. 신뢰할 수 있는 LLM을 위한 설명 가능성 & Human Alignment

이 방법은 **LLM의 신뢰성 (trustworthiness)** 에 초점을 둔다.

![image](../images/Usable_XAI_5.PNG)

[(출처)](https://arxiv.org/pdf/2403.08946) : Xuansheng Wu and Haiwan Zhao et al., "Usable XAI: 10 Strategies Towards Exploiting Explainability in the LLM Era"

* 구체적으로 다음과 같은 분야에 초점을 둔다.

| 분야                 | 설명                                                                                                                   |
|--------------------|----------------------------------------------------------------------------------------------------------------------|
| Security (보안)      | 프롬프트 등을 이용한 공격에 의해 **해로운 정보, misleading 하는 정보를 생성** 하지는 않는가?                                                         |
| Privacy (프라이버시)    | - 모델이 **민감한 정보를 기억하지는** 않는가?<br>- 모델이 민감한 정보를 출력하지 않도록 **방지 조치는 잘 되어 있는가?**                                          |
| Fairness (공정성)     | 인종, 성별, 나이 등에 대한 차별을 하지는 않는가?<br>- 예를 들어, ```[?] is a doctor``` 에서 ```[?]``` 에는 ```He``` 가 들어갈 가능성이 ```She``` 보다 높다. |
| Toxicity (해로운지 여부) | LLM이 **해로운 정보 (폭탄 제조 방법, 해킹 방법 등)** 를 생성하지는 않는가?                                                                     |
| Trustfulness (신뢰성) | - LLM이 학습한 정보에 비추어 **거짓인 정보** 를 생성하지는 않는가? **(Dishonesty)**<br>- **환각 현상** 은 없는가? **(Hallucination)**                |

### 2-5. Explainable Prompt 를 이용한 LLM Inference

이 방법은 **[프롬프트 엔지니어링](../../AI%20Basics/LLM%20Basics/LLM_기초_Prompt_Engineering.md) 을 통해 LLM이 설명을 하도록 프롬프트를 작성** 하게 하는 것을 말한다.

* 구체적 방법들

| 방법                                                                                  | 설명                                                                                                                                                                                                                                                 |
|-------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| [CoT (Chain of Thought)](../../AI%20Basics/LLM%20Basics/LLM_기초_Chain_of_Thought.md) | 생성 과정에서의 **구체적인 추론 과정을 포함** 하여 답변을 생성하도록 함<br><br>그 장점은 다음과 같다.<br>- 추론 과정에서의 오류 (환각 현상 등) 감소<br>- 조정 가능한 intermediate step 제공<br>- [Knowledge Distillation](../../AI%20Basics/Deep%20Learning%20Basics/딥러닝_기초_Knowledge_Distillation.md) 을 용이하게 함 |
| Tree of Thoughts (ToT)                                                              | CoT 를 발전시킨 방법으로, **모델이 여러 개의 reasoning path 를 통해 추론** 하도록 함                                                                                                                                                                                        |
| Graph of Thoughts (GoT)                                                             | CoT & ToT 를 발전시킨 방법으로, 모델의 출력을 **그래프 자료구조** 형태로 발전시킴                                                                                                                                                                                               |

### 2-6. Knowledge-enhanced Prompt 를 이용한 LLM Inference

이 방법은 **LLM이 기존 프롬프트 내용 외에 제공되는 '외부 지식'에 기반하여 답변** 하도록 하는 것이다.

* LLM은 Pre-training 등을 통해 지식을 학습했지만, **이 지식이 inference 시에 어떻게 활용되는지는 직접 설명하기 어렵다.**
* 이를 보완하기 위해, [RAG (Retrieval-Augmented Generation)](../../AI%20Basics/LLM%20Basics/LLM_기초_RAG.md) 을 사용한다.
  * 이를 통해 **어떤 외부 지식이 사용되었는지를 명시적으로 나타낼** 수 있다.

### 2-7. Data Augmentation 학습

이 방법은 **Data Augmentation (데이터 증강)** 을 위해 **어떤 설명으로부터 LLM을 이용하여 synthetic data 를 생성하는 것** 이다.

* 데이터 증강은 각종 머신러닝 분야의 과제에서 널리 쓰인다.
* 굳이 '설명'이 필요한 이유는 다음과 같다.

| 이유                  | 상세 설명                                                                                                                                        |
|---------------------|----------------------------------------------------------------------------------------------------------------------------------------------|
| Explanation Tool 관점 | Explanation Tool 들은 **기존 결함을 identify** 하기 위해 사용되며, 이는 **데이터 증강을 위한 훌륭한 지침** 이 된다.<br>- 증강된 데이터에는 Sample과 함께 **label annotation** 이 포함되어 있다. |
| LLM의 설명 생성 관점       | LLM이 **설명 내용을 직접 생성** 하며, 이는 데이터셋에 대한 보충적 정보로 작용한다.                                                                                          |

## 3. LLM을 이용하여 XAI 향상

### 3-1. 사용자 친화적 설명 생성

### 3-2. 설명을 이용한, 해석 가능한 AI 시스템 설계

### 3-3. Emulating Humans for XAI

## 4. 결론
