
## 목차

* [1. Methods](#1-methods)
  * [1-1. RAFT (Rejection Sampling Fine-Tuning)](#1-1-raft-rejection-sampling-fine-tuning)
  * [1-2. Policy Gradient and Reinforce](#1-2-policy-gradient-and-reinforce)
  * [1-3. GRPO](#1-3-grpo)
  * [1-4. (Iterative) GPO](#1-4-iterative-gpo)
  * [1-5. RAFT++](#1-5-raft)
* [2. 실험 및 그 결과](#2-실험-및-그-결과)
  * [2-1. 실험 설정](#2-1-실험-설정)
  * [2-2. 실험 결과](#2-2-실험-결과)
  * [2-3. Ablation Study 결과](#2-3-ablation-study-결과)
* [3. 결론](#3-결론)

## 논문 소개

* Wei Xiong and Jiarui Yao et al., "A Minimalist Approach to LLM Reasoning: from Rejection Sampling to Reinforce", 2025
* [arXiv Link](https://arxiv.org/pdf/2505.13379)

## 1. Methods

* 본 논문에서 소개하는 Method 에는 다음과 같은 것들이 있다.

| Method                                | 설명                                                                                                                                                                                                  |
|---------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| RAFT (Rejection Sampling Fine-Tuning) | - **Data Collection** : prompt batch 에서, **각 prompt 당 N개의 response** 를 추출<br>- **Data Ranking** : 해당 response 를 reward 기반으로 랭킹<br>- **Model Fine-Tuning** : 주어진 데이터셋에 대한 Log Likelihood 가 최대화되도록 학습 |
| Policy Gradient and Reinforce         | Policy Ascent                                                                                                                                                                                       |
| GRPO                                  | reward function 을 **advantage function** 으로 대체                                                                                                                                                      |
| (Iterative) DPO                       | [DPO (Direct Perference Optimization)](../../AI%20Basics/LLM%20Basics/LLM_기초_Fine_Tuning_DPO_ORPO.md#2-dpo-direct-preference-optimization)                                                          |
| RAFT++                                | 기존 RAFT 알고리즘에 **importance sampling & clipping** 적용                                                                                                                                                 |

### 1-1. RAFT (Rejection Sampling Fine-Tuning)

**RAFT (Rejection Sampling Fine-Tuning)** 은 다음과 같은 3개의 step으로 구성된 Fine-Tuning 알고리즘이다.

| Step                              | 설명                                                                                                                                                                                        |
|-----------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Data Collection                   | - prompt batch ${x_1, ..., x_M}$ 에서, 각 prompt 당 **n 개의 reponse 추출**<br>- 이를 통해, 각 $x_i$ 에 대해 후보 답변 리스트 $\lbrance a_{i,1}, ..., a_{i,n} \rbrace$ 를 얻음                                      |
| Data Ranking (Rejection Sampling) | - 각 prompt $x_i$ 에 대해,<br>- **binary** reward function $r(x, a)$ 을 이용하여 각 response 에 대한 **reward value** $\lbrace r_{i,1}, ..., r_{i,n} \rbrace$ 계산<br>- **response 값이 가장 큰 후보** 만 남기고 제거 |
| Model Fine-Tuning                 | - 주어진 dataset 에 대해 **Log-likelihood 를 최대화** 하도록 최적화                                                                                                                                       |

![image](../images/LLM_RAFT_1.PNG)

* Model Fine-Tuning 수식
  * $\displaystyle L^{RAFT}(\theta) = \Sigma_{(x, a) \in D} \log \pi_\theta (a|x)$ 

### 1-2. Policy Gradient and Reinforce

### 1-3. GRPO

### 1-4. (Iterative) GPO

### 1-5. RAFT++

## 2. 실험 및 그 결과

### 2-1. 실험 설정

### 2-2. 실험 결과

### 2-3. Ablation Study 결과

## 3. 결론

