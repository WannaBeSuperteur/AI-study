## 목차

* [1. 1개의 LLM 의 misunderstanding 이 미치는 영향](#1-1개의-llm-의-misunderstanding-이-미치는-영향)
* [2. CodeCoR Overview](#2-codecor-overview)
* [3. CodeCoR 의 Agents](#3-codecor-의-agents)
  * [3-1. Prompt Agent](#3-1-prompt-agent)
  * [3-2. Test Agent](#3-2-test-agent)
  * [3-3. Coding Agent](#3-3-coding-agent)
  * [3-4. Repair Agent](#3-4-repair-agent)
* [4. CodeCoR 의 알고리즘](#4-codecor-의-알고리즘)
  * [4-1. Pruning](#4-1-pruning)
  * [4-2. Result Checking](#4-2-result-checking)
  * [4-3. Code Repairing](#4-3-code-repairing)
* [5. 실험 결과](#5-실험-결과)
* [6. 논의 사항](#6-논의-사항)
  * [6-1. CodeCoR 은 다른 LLM 과 협력하여 동작할 수 있는가?](#6-1-codecor-은-다른-llm-과-협력하여-동작할-수-있는가)
  * [6-2. CodeCoR 은 왜 효과적으로 동작하는가?](#6-2-codecor-은-왜-효과적으로-동작하는가)
  * [6-3. Repair Round 의 횟수는 성능에 어떻게 영향을 미치는가?](#6-3-repair-round-의-횟수는-성능에-어떻게-영향을-미치는가)

## 논문 소개

* Ruwei Pan and Hongyu Zhang et al., "CodeCoR: An LLM-Based Self-Reflective Multi-Agent Framework for Code Generation", 2025
* [arXiv Link](https://arxiv.org/pdf/2501.07811)

## 1. 1개의 LLM 의 misunderstanding 이 미치는 영향

![image](../images/LLM_CodeCoR_1.PNG)

[(출처)](https://arxiv.org/pdf/2501.07811) : Ruwei Pan and Hongyu Zhang et al., "CodeCoR: An LLM-Based Self-Reflective Multi-Agent Framework for Code Generation"

* 위와 같이 **1개의 LLM 이 주어진 task 를 mis-understand** 하면, **이어지는 모든 프로세스에서 오류가 발생** 한다.
* 위 그림과 같은 경우,
  * (❌) **Prompt Agent** 가 ```a + b``` 가 아닌 ```a - b``` 를 계산하는 코드를 생성하도록 지시
  * (❌) **Coding Agent** 가 ```a - b``` 를 계산하는 코드 생성
  * (❌) **Test Agent** 가 잘못된 테스트 케이스 작성
  * Compiler 실행 결과 Runtime Error 없이 정상 동작
  * (❌) 최종 결과물은 ```a - b``` 를 계산하는 코드

## 2. CodeCoR Overview

![image](../images/LLM_CodeCoR_2.PNG)

[(출처)](https://arxiv.org/pdf/2501.07811) : Ruwei Pan and Hongyu Zhang et al., "CodeCoR: An LLM-Based Self-Reflective Multi-Agent Framework for Code Generation"

* CodeCoR = **Code Co**llaboration and **R**epair
* 핵심 아이디어
  * CodeCoR 은 **코딩 프롬프트 생성 → 테스트 케이스 생성 → 코드 생성 → 결과 검토 → 코드 수정** 의 **프로그래밍의 종합적인 과정** 을 LLM 을 이용하여 해결하는 시스템이다.
  * [LG CNS 의 **DevOn AI-Driven Development** 시스템](../../AI%20Trend/AI_Trend_May_2025.md#20250528-수) 과 컨셉이 유사하다.
* 프로세스 요약
  * Ranked Code Set 의 code snippet 들은 **repair round 횟수 & passed test case 개수를 종합적으로 고려** 하여 순위가 매겨짐

| 단계                      | 에이전트                              | 입력                              | 출력                                                                                                                                                  |
|-------------------------|-----------------------------------|---------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------|
| 1. Prompt Generation    | [Prompt Agent](#3-1-prompt-agent) | task description                | [CoT (Chain-of-thought)](../../AI%20Basics/LLM%20Basics/LLM_기초_Chain_of_Thought.md) Prompts<br>(CoT Pool 에 저장)                                      |
| 2. Test Case Generation | [Test Agent](#3-2-test-agent)     | **selected** CoT Prompts        | **high-quality** test cases                                                                                                                         |
| 3. Code Generation      | [Coding Agent](#3-3-coding-agent) | **selected** CoT Prompts        | **selected** code snippets                                                                                                                          |
| 4. Result Checking      |                                   | code snippets + test cases      | - 모든 테스트 케이스를 통과한 code snippet + repair 가 완료된 code snippet 을 **Ranked Code Set** 에 저장<br>- 테스트 케이스를 통과하지 못한 code snippet 에 대해 **Code Repairing** 실시 |
| 5. Code Repairing       | [Repair Agent](#3-4-repair-agent) | **failed** code snippets        | **high-quality** repair advices                                                                                                                     |
| 5. Code Repairing       | [Coding Agent](#3-3-coding-agent) | **high-quality** repair advices | **selected "repaired"** code snippets                                                                                                               |

## 3. CodeCoR 의 Agents

### 3-1. Prompt Agent

### 3-2. Test Agent

### 3-3. Coding Agent

### 3-4. Repair Agent

## 4. CodeCoR 의 알고리즘

### 4-1. Pruning

### 4-2. Result Checking

### 4-3. Code Repairing

## 5. 실험 결과

## 6. 논의 사항

### 6-1. CodeCoR 은 다른 LLM 과 협력하여 동작할 수 있는가?

### 6-2. CodeCoR 은 왜 효과적으로 동작하는가?

### 6-3. Repair Round 의 횟수는 성능에 어떻게 영향을 미치는가?
