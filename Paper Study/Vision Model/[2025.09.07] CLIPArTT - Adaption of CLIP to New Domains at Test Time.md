
## 목차

* [1. CLIPArTT의 핵심 아이디어](#1-clipartt의-핵심-아이디어)
  * [1-1. 기존 CLIP 방법 및 그 문제점](#1-1-기존-clip-방법-및-그-문제점)
  * [1-2. CLIP 방법의 문제점에 대한 Key Insight](#1-2-clip-방법의-문제점에-대한-key-insight)
* [2. CLIPArTT의 구조](#2-clipartt의-구조)
  * [2-1. Image-Text Similarity 를 이용한 예측](#2-1-image-text-similarity-를-이용한-예측)
  * [2-2. top-K 클래스 예측을 이용한 텍스트 프롬프트 생성](#2-2-top-k-클래스-예측을-이용한-텍스트-프롬프트-생성)
  * [2-3. 최종 Loss (= TTA) 계산](#2-3-최종-loss--tta-계산)
* [3. 기존 기술과의 관련성](#3-기존-기술과의-관련성)
  * [3-1. 관련 기술](#3-1-관련-기술)
  * [3-2. TTA Loss 를 Laplacian Regularization 으로 표현](#3-2-tta-loss-를-laplacian-regularization-으로-표현)
* [4. 실험 설정 및 결과](#4-실험-설정-및-결과)
  * [4-1. 실험 설정](#4-1-실험-설정)
  * [4-2. 기본 실험 결과](#4-2-기본-실험-결과)
  * [4-3. 서로 다른 데이터셋에서의 실험 결과](#4-3-서로-다른-데이터셋에서의-실험-결과)
  * [4-4. CLIPArTT의 한계점 분석](#4-4-clipartt의-한계점-분석)

## 논문 소개

* Gustavo A. Vargas Hakim and David Osowiechi et al., "CLIPArTT: Adaption of CLIP to New Domains at Test Time", 2024
* [Arxiv Link](https://arxiv.org/pdf/2405.00754)

## 1. CLIPArTT의 핵심 아이디어

* **CLIPArTT** 는 **CLIP (Contrastive Language-Image Pre-Training)** [(GitHub)](https://github.com/openai/CLIP) [(Paper)](https://arxiv.org/pdf/2103.00020) 에 **Test-Time Adaption** 을 추가 적용한 방법이다.
  * **Normalization Layer 의 파라미터** 를 업데이트하여 VLM을 최적화시킨다.
* **다중 클래스 → 단일 text prompt 로의 변환** 을 통해 구현한다.
  * 해당 text prompt 는 **pseudo-label** 로 사용된다.

### 1-1. 기존 CLIP 방법 및 그 문제점

* **1. 기존 CLIP 방법**

![image](../images/CLIPArTT_1.PNG)

[(출처)](https://arxiv.org/pdf/2103.00020) : Alec Radford and Jong Wook Kim et al., "Learning Transferable Visual Models From Natural Language Supervision" 

----

> 1. Contrasive Pre-training

* N장의 이미지를 **image encoder** 에 입력시켜, 그 **출력값 (feature vector)** 을 얻는다.
* 마찬가지로 N개의 텍스트를 **text encoder** 에 입력시켜, 그 출력값 (feature vector) 을 얻는다.
* 이 출력 feature vector 들에 대해 **Contrasive Learning** 실시
  * 위 이미지의 $N \times N$ 행렬의 blue cell (correct) 에 해당하는 cosine similarity 는 최대화한다.
  * white cell (incorrect) 에 해당하는 consine simiilarity 는 최소화한다.

> 2. Test (Prediction) Time

* 각 텍스트를 Text Encoder 에 입력하여 { $T_1$, $T_2$, ..., $T_N$} 을 얻는다.
* 이미지 (with index = k) 를 Image Encoder 에 입력하여 $I_k$ 를 얻는다.
* $I_k · T_1$, $I_k · T_2$, ..., $I_k · T_N$ 중 최댓값으로 해당 이미지의 text class 를 예측한다.

----

* **2. 기존 CLIP 방법의 문제점**

기존 CLIP 방법의 문제점은 다음과 같다.

| 문제점               | 설명                                                                                                                                        |
|-------------------|-------------------------------------------------------------------------------------------------------------------------------------------|
| Domain Shift 문제   | Domain Shift 로 인해 **모델의 예측 신뢰성이 떨어진다.**<br>- 예를 들어, 잘못된 class에 대한 예측 확률이 높다.<br>- Entropy Minimization 과 같은 기술을 통해서도 이러한 예측 오류를 해결하기 어렵다. |
| Test batch sample | test batch 에 있는 sample 들은 **서로 독립적** 이다.<br>- 또한 이들 간의 semantic relationship 을 이용하지 못한다.                                                  |

### 1-2. CLIP 방법의 문제점에 대한 Key Insight

* CLIP 방법이 가지고 있는 위 문제점에 대한 핵심적인 insight 는 다음과 같다.

| 인사이트                                                             | 설명                                                                                                                                                       |
|------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------|
| correct class 는 **가장 가능성이 높은 class 중 하나** 인 경우가 많다.              | - conformal learning (95% 등 1에 가까운 확률의 prediction region 생성) 관련 인사이트임<br>- **여러 개의 class에 대한 정보** 를 포함하는 방법은 **Test Time 에 model adaption 을 적용** 하는 것이다. |
| batch sample 간 유사성은 **visual, text embedding 을 이용하여 측정** 될 수 있다. | - 이러한 유사성은 **Stochastic Neighbor Embedding (SNE)** 등의 전략으로도 확인할 수 있다.                                                                                    |

## 2. CLIPArTT의 구조

* CLIPArTT의 전체 구조는 다음과 같다.

![image](../images/CLIPArTT_2.PNG)

[(출처)](https://arxiv.org/pdf/2405.00754) : Gustavo A. Vargas Hakim and David Osowiechi et al., "CLIPArTT: Adaption of CLIP to New Domains at Test Time", 2024

* CLIPArTT 구조 요약

| 세부 구조                          | 설명                                                                                     |
|--------------------------------|----------------------------------------------------------------------------------------|
| Image-Text Similarity 를 이용한 예측 | 기존 CLIP 알고리즘 기반                                                                        |
| top-K 클래스 예측을 이용한 텍스트 프롬프트 생성  | top-k 예측으로부터 **Instance-specific 한 prompt 를 생성**                                       |
| 최종 Loss 계산                     | **Test-Time Adaption (TTA) Loss** 를 이용하여 **batch sample 간 semantic relationship 을 처리** |

### 2-1. Image-Text Similarity 를 이용한 예측

CLIPArTT 에서는 기본적으로 CLIP 처럼 **Image-Text Similarity** 를 이용하여 class 를 예측한다.

* 어떤 텍스트 $t_k$ 와 새로운 이미지 $x_i$ 에 대해, 그 이미지가 class $k$ 에 속할 확률 $p_{ik}$ 는 다음과 같다.

| 구분    | 설명                                                                                                                                                                                                                                                                                                             |
|-------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| 핵심 컨셉 | - 이미지의 embedding vector 와 텍스트의 embedding vector 간의,<br>- [Cosine Similarity](../../AI%20Basics/Data%20Science%20Basics/데이터_사이언스_기초_Cosine_similarity.md) 값에 대한,<br>- [Temperature 를 이용한 Softmax (= soft label)](../../AI%20Basics/Deep%20Learning%20Basics/딥러닝_기초_Knowledge_Distillation.md#4-1-soft-label) 컨셉 |
| 수식    | $\displaystyle p_{ik} = \frac{\exp(\cos(z_i^v, z_k^t) / \tau)}{\Sigma_j \exp(\cos(z_i^v, z_j^t) / \tau)}$                                                                                                                                                                                                      |

* notation 설명

| notation | 설명                                           |
|----------|----------------------------------------------|
| $z^v$    | $z^v \in R^D$ 로, 이미지 $x$ 에 대한 visual feature |
| $z^t$    | $z^t \in R^D$ 로, 텍스트 프롬프트 $t$ 에 대한 feature   |
| $\tau$   | softmax temperature                          |

### 2-2. top-K 클래스 예측을 이용한 텍스트 프롬프트 생성

CLIPArTT 에서는 다음과 같이 **top-k 예측 결과를 이용한 instance-specific prompt** 를 생성한다.

* 먼저, CLIP 기반으로 각 class의 확률을 계산한다.
* 이 계산한 확률을 바탕으로 **top k 개의 class와 관련된 프롬프트** 를 생성한다.
* 예시
  * ```a photo of a {class 1} or ... or {class k}```

### 2-3. 최종 Loss (= TTA) 계산

**Transductive TTA (= Test-Time Adaption) Loss** 는 **각 batch sample 간의 semantic relationship 을 처리** 하기 위한 Loss Function 이다.

![image](../images/CLIPArTT_3.PNG)

[(출처)](https://arxiv.org/pdf/2405.00754) : Gustavo A. Vargas Hakim and David Osowiechi et al., "CLIPArTT: Adaption of CLIP to New Domains at Test Time", 2024

* TTA Loss 의 기본 컨셉 및 수식

| 구분    | 설명                                                                                                     |
|-------|--------------------------------------------------------------------------------------------------------|
| 기본 컨셉 | **text, image embedding** 에 대한 Prediction 과 Pseudo-Label 간의 **Cross-Entropy**                          |
| 수식    | $\displaystyle L_{TTA}(\theta) = - \frac{1}{B} \Sigma_{i=1}^B \Sigma_{j=1}^B q_{ij} \log \hat{p}_{ij}$ |

* TTA Loss 의 계산 순서

| 계산 순서                                          | 설명                                                                                                                                                       |
|------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------|
| 정규화된 visual, text embedding                    | - $Z^v \in R^{B \times D}$ : **visual** embedding<br>- $\hat{Z}^t \in R^{B \times D}$ : **text** embedding                                               |
| **image-to-image** similarity matrix           | $S^v = Z^v(Z^v)^T \in [-1, 1]^{B \times B}$                                                                                                              |
| **text-to-text** similarity matrix             | $S^t = \hat{Z}^t (\hat{Z}^t)^T \in [-1, 1]^{B \times B}$                                                                                                 |
| Pseudo-Label 생성을 위한 pairwise similarity matrix | $Q = softmax((S^v + S^t)/2 \tau) \in [0,1]^{B \times B}$<br>- 여기서 softmax 연산은 **column-wise 하게 적용** 하며, temperature $\tau$ 의 값은 이 실험에서는 **0.01** 로 고정    |
| Zero-shot prediction matrix                    | $\hat{P} = softmax(Z^v(\hat{Z}^t)^T / \tau)$<br>- instance-specific 한 multi-class 텍스트 프롬프트 사용<br>- $\hat{P}$ 의 각 성분 $p_{ij}$ 의 계산식은 **원래 CLIP 알고리즘과 동일** |
| **TTA (Test-Time Adaption) Loss**              | $L_{TTA}(\theta)$ 로, 위 수식과 같이 **Prediction 과 Pseudo-Label 간의 Cross-Entropy** 컨셉                                                                          |

## 3. 기존 기술과의 관련성

| 관련 기술                               | 설명                                                       | CLIPArTT와의 관련성                                                                                     |
|-------------------------------------|----------------------------------------------------------|----------------------------------------------------------------------------------------------------|
| Stochastic Neighbor Embedding (SNE) | local probability $q_{ij}$ 가 $p_{ij}$ 와 비슷해지는 저차원 임베딩 찾기 |                                                                                                    |
| Graph-Laplacian Regularization      | Context Semi-supervised Learning 에서 사용                   | [TTA Loss 를 **Laplacian Regularization 으로 표현 가능**](#3-2-tta-loss-를-laplacian-regularization-으로-표현) |

### 3-1. 관련 기술

**1. Stochastic Neighbor Embedding (SNE)**

* 차원 축소에 많이 사용되는 기술 중 하나
* 주어진 점 $x_i$ 에 대해, $x_j$ 의 local probability 를 계산한다.
  * 이때, Euclidean Distance $d_{ij} = ||x_i - x_j||_2$ 를 사용한다.
* 목표는 **각 $x_i$ 에 대해, local probability $q_{ij}$ 가 $p_{ij}$ 와 비슷해지는 low-dimensional embedding $y_i$ 를 찾는** 것이다.

![image](../images/CLIPArTT_4.PNG)

**2. Graph-Laplacian Regularization**

* Context Semi-supervised Learning 에 많이 사용되는 기술 중 하나
* 본 논문의 케이스에서는 다음과 같이 계산된다.

![image](../images/CLIPArTT_5.PNG)

[(출처)](https://arxiv.org/pdf/2405.00754) : Gustavo A. Vargas Hakim and David Osowiechi et al., "CLIPArTT: Adaption of CLIP to New Domains at Test Time", 2024

| notation | 설명                                                                        |
|----------|---------------------------------------------------------------------------|
| $B$      | node 개수                                                                   |
| $Z$      | model prediction, $Z \in R^{B \times D}$                                  |
| $L_W$    | edge weight matrix $W$에 의해 정의된 Laplacian matrix, $L_W \in R^{B \times B}$ |

### 3-2. TTA Loss 를 Laplacian Regularization 으로 표현

TTA Loss 는 다음과 같이 **Laplacian Regularization** 으로 표현 가능하다.

* 단, image embedding 들을 node 집합으로, text embedding 들을 node 집합으로 각각 가정할 때의 **bipartite graph (이분 그래프)** 를 가정한다.

![image](../images/CLIPArTT_6.PNG)

[(출처)](https://arxiv.org/pdf/2405.00754) : Gustavo A. Vargas Hakim and David Osowiechi et al., "CLIPArTT: Adaption of CLIP to New Domains at Test Time", 2024

## 4. 실험 설정 및 결과

* 실험 결과 요약

| 실험                                 | 결과 요약                                                                                         |
|------------------------------------|-----------------------------------------------------------------------------------------------|
| Pseudo Label 생성을 위한 K의 값           | **K > 1** (K=3, 5) 이 더 좋음                                                                     |
| test time 에서의 모델 업데이트 반복 횟수        | **5 ~ 10회 정도 반복** 하는 것이 최적                                                                    |
| Pseudo Label 선택을 위한 modality 선택    | text 또는 image 각각보다는 **image-to-image, text-to-text 유사도를 같이 활용하여 Pseudo Label을 선택** 하는 것이 더 좋음 |
| 다양한 corruption option 에서의 방법론 간 비교 | **CLIPArTT** 가 다른 방법론 (CLIP, TENT, TDA) 보다 **성능 측면에서 우위**                                     |

### 4-1. 실험 설정

CLIPArTT 의 성능을 **다양한 TTA (Test-Time Adaption) 데이터셋** 을 이용하여 평가한다.

* Test-Time Adaption (TTA) 상세 설정

| 항목                                                                                  | 설정값                                                                                                                                            |
|-------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------|
| 모델 업데이트 적용 범위                                                                       | Visual Encoder 내부의 모든 [Layer Normalization](../../AI%20Basics/Deep%20Learning%20Basics/딥러닝_기초_Regularization.md#4-2-layer-normalization) layer |
| [Optimizer](../../AI%20Basics/Deep%20Learning%20Basics/딥러닝_기초_Optimizer.md)         | [Adam Optimizer](../../AI%20Basics/Deep%20Learning%20Basics/딥러닝_기초_Optimizer.md#2-2-adam)                                                      |
| [Learning Rate](../../AI%20Basics/Deep%20Learning%20Basics/딥러닝_기초_Learning_Rate.md) | **1e-3 (= 0.001)** (fixed)                                                                                                                     |
| batch size                                                                          | 128 (consistent)                                                                                                                               |

### 4-2. 기본 실험 결과

**1. CIFAR-10 / 10.1 / 10-C** 데이터셋에서의 **Level 5 Corruption** 시의 성능

* 각각 **Pseudo Label 생성을 위한 K 값 & 모델 업데이트 반복 횟수** 에 따른 실험 결과이다.

| 실험                                  | 결과 요약                                         |
|-------------------------------------|-----------------------------------------------|
| Pseudo Label 생성을 위한 **K 값** 에 따른 분석 | K = 1 보다는 **K > 1 일 때** 성능이 좋음                |
| **모델 업데이트 반복 횟수** 에 따른 분석           | 반복 횟수 **5~10회 정도** 에서 성능이 가장 좋고, 이후에는 오히려 떨어짐 |

![image](../images/CLIPArTT_8.PNG)

[(출처)](https://arxiv.org/pdf/2405.00754) : Gustavo A. Vargas Hakim and David Osowiechi et al., "CLIPArTT: Adaption of CLIP to New Domains at Test Time", 2024

**2. CIFAR-10 / 10.1 / 10-C** 데이터셋에서의 **Pseudo Label 선택을 위한 target (Modality)** 및 **배치 크기** 차이에 따른 비교 실험

| 실험                                                | 결과 요약                                                                                           |
|---------------------------------------------------|-------------------------------------------------------------------------------------------------|
| **Pseudo-Label 선택을 위한 target (modality)** 에 따른 비교 | 각 modality 를 독립적으로 적용하는 것보다는 **image-to-image & text-to-text similarity 정보를 같이** 사용하는 것이 성능이 좋음 |
| **배치 크기** 에 따른 비교                                 | 배치 크기가 증가할수록 성능 향상                                                                              |

![image](../images/CLIPArTT_9.PNG)

[(출처)](https://arxiv.org/pdf/2405.00754) : Gustavo A. Vargas Hakim and David Osowiechi et al., "CLIPArTT: Adaption of CLIP to New Domains at Test Time", 2024

### 4-3. 서로 다른 데이터셋에서의 실험 결과

* **CIFAR-100-C & ImageNet-C** 데이터셋에서의, **다양한 corruption 옵션** 에 따른 **ViT-B/32** Visual Encoder 사용 시의 정확도
  * 기본적으로 CLIPArTT 가 CLIP, TENT, TDA 등 다른 방법론에 비해 **소폭 ~ 압도적인 우위** 를 보임
  * ```Impulse Noise``` 옵션의 경우, TENT 는 baseline 보다 정확도가 3%p 떨어짐

![image](../images/CLIPArTT_7.PNG)

[(출처)](https://arxiv.org/pdf/2405.00754) : Gustavo A. Vargas Hakim and David Osowiechi et al., "CLIPArTT: Adaption of CLIP to New Domains at Test Time", 2024

### 4-4. CLIPArTT의 한계점 분석

본 논문에서 언급하는 CLIPArTT의 한계점은 다음과 같다.

* **자연 환경 이미지** 에서는 TENT보다 성능이 다소 떨어질 수 있다.
  * 이는 자연 환경 이미지들은 CLIP 의 pre-training 에서 이미 사용되어서, **Adaption 적용 시 [Overfitting](../../AI%20Basics/Deep%20Learning%20Basics/딥러닝_기초_Overfitting_Dropout.md#2-딥러닝에서의-오버피팅-overfitting) 가능성이 있기** 때문일 것으로 추정된다. 
  * **CLIP의 pre-training 데이터셋에 없는 이미지는 잠재적인 domain shift 로 간주** 할 수 있다.
* CLIPArTT 의 이러한 문제점을 깊이 탐구하기 위해 추가한 시나리오는 다음과 같다.
  * $C$ 개의 랜덤하게 선택된 class만 batch 에 존재하는, **일종의 [데이터 불균형](../../AI%20Basics/Data%20Science%20Basics/데이터_사이언스_기초_데이터_불균형.md) 이 심한 케이스**
  * Open-set Classification (out-of-distribution 이미지들이 batch 에 추가됨)
