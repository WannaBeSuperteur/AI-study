## 목차

* [1. 핵심 아이디어](#1-핵심-아이디어)
* [2. Fast Pre-training Distillation](#2-fast-pre-training-distillation)
  * [2-1. 해결하려는 문제 및 그 해결 방법](#2-1-해결하려는-문제-및-그-해결-방법) 
  * [2-2. 수학적 원리](#2-2-수학적-원리)
  * [2-3. Soft Labels](#2-3-soft-labels) 
  * [2-4. Data Augmentation Encoding](#2-4-data-augmentation-encoding)
* [3. 모델 상세 구조](#3-모델-상세-구조)
  * [3-1. 계층적인 Vision Transformer](#3-1-계층적인-vision-transformer) 
  * [3-2. Contraction Factors](#3-2-contraction-factors)
  * [3-3. 모델 설정값 (하이퍼파라미터 등)](#3-3-모델-설정값-하이퍼파라미터-등)
* [4. 모델에 대한 질문거리](#4-모델에-대한-질문거리)
  * [4-1. 작은 모델이 큰 데이터를 학습하기 어려운 원인은?](#4-1-작은-모델이-큰-데이터를-학습하기-어려운-원인은)
  * [4-2. Distillation 은 이것을 어떻게 극복하는가?](#4-2-distillation-은-이것을-어떻게-극복하는가)
* [5. 실험 결과](#5-실험-결과)
  * [5-1. Ablation Study](#5-1-ablation-study)
  * [5-2. ImageNet 분류 실험 결과](#5-2-imagenet-분류-실험-결과)
  * [5-3. Object Detection 실험 결과](#5-3-object-detection-실험-결과)

## 논문 소개

* Kan Wu and Jinnian Zhang et al., TinyViT: Fast Pretraining Distillation for Small Vision Transformers
* [Arxiv Link](https://arxiv.org/pdf/2207.10666v1)

## 1. 핵심 아이디어

TinyViT 의 핵심 아이디어는 다음과 같다.

* 큰 데이터에 대한 **Fast Pre-training Distillation** 을 통한, [작은 모델의 Pre-training](../../AI%20Basics/Deep%20Learning%20Basics/딥러닝_기초_Transfer_Learning.md#3-1-사전-학습-pre-training)
  * Distillation 과정에서 **Data augmentation 및 Teacher 모델의 예측 정보를 미리 저장** 한 후 빠르게 사용
  * 이를 통해 Teacher Model 의 연산량 및 메모리 사용량 감소
* Student Model 이 될 수 있는 **작은 크기의 후보 모델** 을 여러 개 생성 후, 이들 중 **파라미터 개수 및 throughput 조건을 만족시키는** 것을 선택
  * 이것은 **constrained local search** 의 일종임 

![image](../images/Vision_TinyViT_1.PNG)

[(출처)](https://arxiv.org/pdf/2207.10666v1) : Kan Wu and Jinnian Zhang et al., "TinyViT: Fast Pretraining Distillation for Small Vision Transformers"

## 2. Fast Pre-training Distillation

다음과 같은 문제를 해결하기 위해 Fast Pre-training Distillation 을 이용한다.

| 구분    | 설명                                                                                                                                                                                                          |
|-------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| 문제점   | - 작은 모델을 거대한 데이터로 직접 Pre-training 하는 것은 **큰 효과가 없음**<br>- [Knowledge Distillation](../../AI%20Basics/Deep%20Learning%20Basics/딥러닝_기초_Knowledge_Distillation.md) 을 이용하는 Pre-training 은 **효율적이지 않고 자원 소비가 큼** |
| 해결 방법 | - Fast Training Distillation<br>- Distillation 과정에서 **Data Augmentation 및 Teacher 모델의 예측 등 정보를 저장하고 이를 재사용**                                                                                                |

### 2-1. 해결하려는 문제 및 그 해결 방법

**1. 작은 모델을 거대한 데이터로 직접 Pre-training 하는 것은 큰 효과가 없음**

* 다음 그래프에서 TinyViT w/ pretrain 은 TinyViT w/o pretrain 에 비해 **성능의 큰 향상이 없다.**

![image](../images/Vision_TinyViT_2.PNG)

[(출처)](https://arxiv.org/pdf/2207.10666v1) : Kan Wu and Jinnian Zhang et al., "TinyViT: Fast Pretraining Distillation for Small Vision Transformers"

**2. [Knowledge Distillation](../../AI%20Basics/Deep%20Learning%20Basics/딥러닝_기초_Knowledge_Distillation.md) 을 이용하는 Pre-training 은 효율적이지 않고 자원 소비가 큼**

* 이는 Distillation 을 위해서는 Teacher Model 로 학습 데이터를 입력하여 그 출력까지 확인해야 하는데, 이때의 **자원 소비가 Student Model 만을 학습시킬 때에 비해 훨씬 크기** 때문이다.
* 또한, 이때 **GPU 메모리 역시 많이 소비** 되기 때문이다.

**3. 해결 방법: Fast Training Distillation**

* 다음과 같은 정보를 저장한 후, **학습 중 필요할 때마다 재사용** 한다. 이를 통해 Distillation 과정에서 **연산량 및 메모리 사용량을 감소** 시킬 수 있다.
  * Data Augmentation 에 대한 정보
  * Teacher model 의 예측
* 이 방법론의 핵심 요소는 다음과 같다.
  * [Soft Labels](#2-3-soft-labels)
  * [Data Augmentation Encoding](#2-4-data-augmentation-encoding)

### 2-2. 수학적 원리

* 입력 이미지 $x$ 와 Data Augmentation 방법 $A$ 에 대해서, 다음을 저장할 수 있다.
  * Data Augmentation 방법 $A$
  * Teacher 모델의 예측 $\hat{y} = T(A(x))$
  * 이때, **Data Augmentation 의 Randomness** 가 있기 때문에, $(A, \hat{y})$ 가 **각 이미지 별, 매번 실행 시마다** 저장되어야 한다. 
* 학습 시의 Loss $L$ 은 다음과 같이 계산한다. 
  * $L = CE(\hat{y}, S(A(x)))$ (단, $S$ 는 Student Model)
  * 이때 $CE$ 는 [Cross Entropy Loss](../../AI%20Basics/Deep%20Learning%20Basics/딥러닝_기초_Loss_function.md#2-5-categorical-cross-entropy-loss) 를 의미한다.
* 이때, **Fast Training Distillation 을 통해서는 아래와 같이 Soft Label 을 생성** 하므로, **Ground-truth Label 에 대한 정보는 불필요** 하다.

### 2-3. Soft Labels

Soft Labels 는 **이미지 분류 모델은 기본적으로 Class 의 개수만큼의 logit 을 생성하지만, 여기서는 그 중 중요한 몇 개만 저장** 하는 아이디어이다.

* logit 의 값들 중 **상위 K 개의 값들 ($\hat{y}_{I(k)}, k=1,2,...,K$) 만 저장** 한다.
* 나머지의 경우는 상위 K 개의 logit 까지 포함한 전체 logit 값의 합이 1이 되도록 모두 동일한 확률 값을 할당한다.

이를 수식으로 표현하면 다음과 같다.

![image](../images/Vision_TinyViT_3.PNG)

[(출처)](https://arxiv.org/pdf/2207.10666v1) : Kan Wu and Jinnian Zhang et al., "TinyViT: Fast Pretraining Distillation for Small Vision Transformers"

### 2-4. Data Augmentation Encoding

**Data Augmentation Encoding** 은 말 그대로 **Data Augmentation 의 파라미터를 인코딩-디코딩** 하는 것으로, 핵심 아이디어는 다음과 같다.

* Data Augmentation 방법론을 실제 적용할 때 사용하는 파라미터의 집합 $d$ 를 **하나의 파라미터 $d_0 = E(d)$ 로 인코딩** 한다.
  * 이는 $d$ 의 파라미터의 값이 **매 이미지마다, 매 학습 iteration 마다 서로 다르기** 때문이다.
* $d = E^{-1}(d_0)$ 을 이용한 디코딩으로, Data Augmentation 을 정확히 표현할 수 있다.

## 3. 모델 상세 구조

### 3-1. 계층적인 Vision Transformer

### 3-2. Contraction Factors

### 3-3. 모델 설정값 (하이퍼파라미터 등)

## 4. 모델에 대한 질문거리

### 4-1. 작은 모델이 큰 데이터를 학습하기 어려운 원인은?

### 4-2. Distillation 은 이것을 어떻게 극복하는가?

## 5. 실험 결과

### 5-1. Ablation Study

### 5-2. ImageNet 분류 실험 결과

### 5-3. Object Detection 실험 결과