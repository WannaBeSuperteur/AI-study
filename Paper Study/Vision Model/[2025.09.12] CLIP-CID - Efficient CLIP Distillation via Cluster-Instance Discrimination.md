
## 목차

* [1. CLIP-CID 의 핵심 아이디어](#1-clip-cid-의-핵심-아이디어)
* [2. CLIP-CID 의 방법론](#2-clip-cid-의-방법론)
  * [2-1. Image 의 Semantic Balance](#2-1-image-의-semantic-balance)
  * [2-2. Cluster-Level Distillation](#2-2-cluster-level-distillation)
  * [2-3. Instance-Level Distillation](#2-3-instance-level-distillation)
* [3. 실험 및 그 결과](#3-실험-및-그-결과)
  * [3-1. 실험 설정](#3-1-실험-설정)
  * [3-2. 실험 결과](#3-2-실험-결과)
  * [3-3. Ablation Study](#3-3-ablation-study)
  * [3-4. 실험 결론](#3-4-실험-결론)

## 논문 소개

* Kaicheng Yang and Tiancheng Gu et al., "CLIP-CID: Efficient CLIP Distillation via Cluster-Instance Discrimination"
* [AAAI Download Link](https://ojs.aaai.org/index.php/AAAI/article/download/35505/37660)

## 1. CLIP-CID 의 핵심 아이디어

CLIP-CID 의 핵심 아이디어 컨셉은 다음과 같다.

* [Contrastive Language-Image Pre-training (CLIP)](%5B2025.09.07%5D%20CLIPArTT%20-%20Adaption%20of%20CLIP%20to%20New%20Domains%20at%20Test%20Time.md#1-1-기존-clip-방법-및-그-문제점) 에 대한 **새로운 [Knowledge Distillation](../../AI%20Basics/Deep%20Learning%20Basics/딥러닝_기초_Knowledge_Distillation.md) 방법** 제안 
* 다음을 결합하여 **보다 효과적인 Knowledge Distillation** 이 될 수 있게 함
  * Cluster Discrimination
  * Instance Discrimination

## 2. CLIP-CID 의 방법론

**CLIP-CID (CLIP Cluster-Instance Distillation)** 의 방법론은 다음과 같다.

| 방법론                         | 설명                                                                                                                                                                                                                          |
|-----------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Image 의 Semantic Balance    | 간단하지만 효과적인 **image semantic balance 방법론** 을 제시한다.                                                                                                                                                                           |
| Cluster-level Distillation  | **Cluster Discrimination 기반 Knowledge Distillation** 을 통해 학습 데이터의 **잠재적인 semantic structure** 를 파악한다.                                                                                                                       |
| Instance-level Distillation | - Cluster-Level Distillation 은 **Student Image Encoder 가 학습 데이터의 복잡한 패턴을 찾아낼 수 있게** 한다.<br>- 그러나, **Fine-grained Semantic Detail** 의 뉘앙스가 일정 부분 학습되지 못할 수 있다.<br>- 이를 해결하기 위해, **Instance-Level Distillation Loss** 를 적용한다. |

### 2-1. Image 의 Semantic Balance

### 2-2. Cluster-Level Distillation

### 2-3. Instance-Level Distillation

## 3. 실험 및 그 결과

### 3-1. 실험 설정

### 3-2. 실험 결과

### 3-3. Ablation Study

### 3-4. 실험 결론
