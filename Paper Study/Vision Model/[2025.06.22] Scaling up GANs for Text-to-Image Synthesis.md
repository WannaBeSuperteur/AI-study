## 목차

* [1. GigaGAN 핵심 아이디어](#1-gigagan-핵심-아이디어)
* [2. GigaGAN 의 전체 구조](#2-gigagan-의-전체-구조)
* [3. Generator](#3-generator)
  * [3-1. Text and latent-code conditioning](#3-1-text-and-latent-code-conditioning)
  * [3-2. Synthesis Network (with adaptive kernel selection)](#3-2-synthesis-network-with-adaptive-kernel-selection)
* [4. Discriminator](#4-discriminator)
  * [4-1. Text conditioning & Multi-scale image processing](#4-1-text-conditioning--multi-scale-image-processing)
  * [4-2. Loss Term 1 (Multi-scale input, multi-scale output adversarial)](#4-2-loss-term-1-multi-scale-input-multi-scale-output-adversarial)
  * [4-3. Loss Term 2 (Matching-aware Loss)](#4-3-loss-term-2-matching-aware-loss)
  * [4-4. Loss Term 3 (CLIP contrastive Loss)](#4-4-loss-term-3-clip-contrastive-loss)
* [5. GAN-based Upsampler](#5-gan-based-upsampler)
* [6. 실험 결과](#6-실험-결과)

## 논문 소개

* Minguk Kang and Jun-Yan Zhu et al., "Scaling up GANs for Text-to-Image Synthesis", 2023
* [arXiv Link](https://arxiv.org/pdf/2303.05511)
* 개인적으로는 Figure 1 최상단의 ```a human growing colorful flowers from her hair``` 의 컨셉을 Oh-LoRA 향후 버전에 유사하게 적용해 보고 싶음

## 1. GigaGAN 핵심 아이디어

* 기존 StyleGAN 구조의 문제점
  * **StyleGAN 구조의 성능 및 학습 데이터셋 규모만을 향상시키는 것** 은 한계가 있다. (unstable)
* GigaGAN 의 특징
  * **텍스트 - 이미지 합성**
  * latent interpolation, style mixing, 벡터 연산 등 **다양한 latent space 편집 기술** 적용 가능
* GigaGAN 의 핵심 아이디어

| 핵심 아이디어              | 설명                                                                                             |
|----------------------|------------------------------------------------------------------------------------------------|
| Multi-scale Training | image-text alignment 및 low-frequency detail 향상<br>(**낮은 해상도를 나타내는 block** 의 파라미터를 효과적으로 사용 가능) |
| Multi-stage Approach | 처음에 64 x 64 이미지 생성 → 512 x 512 로 upsampling                                                    |

## 2. GigaGAN 의 전체 구조

| 구성 요소                                         | 설명                                                                                                                                                                                                                                                  |
|-----------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| [Generator](#3-generator)                     | - **StyleGAN2** 기반<br>- **mapping network** : input → style vector $w$, $w = M(z, t_{global})$<br>- output 생성에 사용되는 방법은 [Convolution](../../Image%20Processing/Basics_CNN.md)<br>- Synthesis Network 에 **Sample-adaptive Kernel Selection** 적용      |
| [Discrinimator](#4-discriminator)             | - **'text branch' 와 'image branch'** 의 2개의 branch 로 구성<br>- **text conditioning** : text $c$ → text description $t_D$<br>- multi-scale image processing : generator 의 'pyramid' 구조의 각 level 을 **서로 독립적으로** 처리<br>- Loss Function 은 총 3개의 term 으로 구성 |
| [GAN-based Upsampler](#5-gan-based-upsampler) | - Synthesis Network 는 **비대칭적 [U-Net](../../Image%20Processing/Model_U-Net.md) 구조** 로 64 x 64 → 512 x 512 로 upsampling                                                                                                                               |

## 3. Generator

![image](../images/GigaGAN_1.PNG)

[(출처)](https://arxiv.org/pdf/2303.05511) : Minguk Kang and Jun-Yan Zhu et al., "Scaling up GANs for Text-to-Image Synthesis"

GigaGAN 의 Generator 의 구조는 다음과 같다.

| 구성 요소                                                                                                       | 설명                                                                                                                                                                                                               |
|-------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Baseline                                                                                                    | StyleGAN2 기반                                                                                                                                                                                                     |
| [Text and latent-code conditioning](#3-1-text-and-latent-code-conditioning)                                 | - **Frozen CLIP** feature extractor<br>- $T$ (additional **learned text encoder**)<br>- $M$ (MLP mapping network)                                                                                                |
| [Synthesis Network (with adaptive kernel selection)](#3-2-synthesis-network-with-adaptive-kernel-selection) | 입력:<br>- $f_l$ (feature at layer $l$)<br>- $t_{local}$ (text embedding 중 prompt 와 관련된 부분)<br>- $w$ (style vector)<br>출력: 각 layer $l$ 에 대해, $f_{l+1} = g_{xa}^l(g_{attn}^l(g_{adaconv}^l(f_l, w), w), t_{local})$ |

### 3-1. Text and latent-code conditioning

![image](../images/GigaGAN_2.PNG)

[(출처)](https://arxiv.org/pdf/2303.05511) : Minguk Kang and Jun-Yan Zhu et al., "Scaling up GANs for Text-to-Image Synthesis"

**Text and latent-code conditioning** 의 프로세스는 다음과 같다.

* 기본 프로세스
  * **StyleGAN2** 기반

* text input → text embedding $t$
  * text input 에 대한 **conditional vector** $c \in R^{C \times 768}$ 생성
  * $c$ 를 **Pre-trained CLIP text encoder** 로 forward 시켜서 $\epsilon_{txt}(c)$ 를 생성
  * $\epsilon_{txt}(c)$ 를 **Learned text encoder ($T$)** 로 forward 시켜서 text embedding $t \in R^{C \times 768}$ 를 생성

* text embedding $t$ → style vector $w$
  * $t$ 를 $t_{local} \in R^{(C - 1) \times 768}$ 과 $t_{global} \in R^{768}$ 로 분할
  * $z \sim N(0, 1)$ 을 생성
  * $t_{global}$ 과 $z$ 를 **MLP mapping network ($M$)** 으로 forward 시켜서 style vector $w = M(z, t_{global})$ 를 생성

* style vector $w$ → generated image $x$
  * $t_{local}$ 과 $w$ 를 **Synthesis Network ($\tilde{G}$)** 으로 forward 시켜서 image $x$ 를 생성
  * generated image $x = \tilde{G}(w, t_{local})$

### 3-2. Synthesis Network (with adaptive kernel selection)

![image](../images/GigaGAN_3.PNG)

[(출처)](https://arxiv.org/pdf/2303.05511) : Minguk Kang and Jun-Yan Zhu et al., "Scaling up GANs for Text-to-Image Synthesis"

**1. Generator ($\tilde{G}$) 의 구성**

| 구성 요소                                                                                                                           | 설명                                                                                                |
|---------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------|
| Convolutional block<br>($g_{adaconv}^l$)                                                                                        |                                                                                                   |
| [Self-Attention](../../Natural%20Language%20Processing/Basics_트랜스포머%20모델.md#3-1-encoder-self-attention) block<br>($g_{attn}^l$) | Convolutional Layer 가 **receptive field 기반으로 처리** 하므로 **멀리 떨어진 부분을 contextualize 하기 어렵다** 는 문제 해결 |
| Cross-Attention block<br>($g_{xa}^l$)                                                                                           | 개별 word embedding 처리 목적으로 사용                                                                      |

**2. 이미지 생성 프로세스**

* Synthesis Network 는 **upsampling convolutional layer 의 피라미드 구조** 로 되어 있음
  * 즉, feature $f_l$ 을 다음 layer 의 feature $f_{l+1}$ 으로 변환하는 **여러 개의 level** 로 구성
* 각 level 에서 다음과 같이 처리됨
  * feature $f_l$ 은 $g_{adaconv}^l$, $g_{attn}^l$, $g_{xa}^l$ 순서대로 network forward 됨
  * 이를 수식으로 나타내면 $f_{l+1} = g_{xa}^l(g_{attn}^l(g_{adaconv}^l(f_l, w), w), t_{local})$ 이 됨
* 최종적으로 다음과 같이 **피라미드 구조로 여러 이미지가 생성** 됨
  * $x_0$ (4 x 4), $x_1$ (8 x 8), $x_2$ (16 x 16), $x_3$ (32 x 32), $x_4$ (64 x 64) 

**3. Sample-adaptive kernel selection**

Sample-adaptive kernel selection 은 **학습 데이터 (인터넷상의 이미지) 의 특징이 매우 다양** 하므로, 이를 처리하기 위해 **Convolution Kernel 의 표현 능력을 "text-conditioning 기반 on-the-fly 방법"으로 향상** 시키기 위한 것이다.

* $N$ 개의 filter 를 저장하는 Filter Bank 를 초기화
  * 각 filter 는 $K_i \in R^{C_{in} \times C_{out} \times K \times K}$
* 이 Filter Bank 의 filter 들을 **weighted average** 하여 **aggregated filter** 생성
  * $K \in R^{C_{in} \times C_{out} \times K \times K}$
  * $\displaystyle K = \Sigma_{i=1}^N K_i · softmax(W_{filter}^T w + b_{filter})_i$
  * 해당 필터는 **StyleGAN2의 convolution pipeline** 에 사용
* 최종 modulated weights $g_{adaconv}(f, w)$ 생성
  * $g_{adaconv}(f, w) = ((W_{mod}^T w + b_{mod}) ⊗ K) * f$
  * **Convolutional block 의 최종 출력값** 에 해당

| ⊗               | *           |
|-----------------|-------------|
| (de-)modulation | convolution |

## 4. Discriminator

### 4-1. Text conditioning & Multi-scale image processing

### 4-2. Loss Term 1 (Multi-scale input, multi-scale output adversarial)

### 4-3. Loss Term 2 (Matching-aware Loss)

### 4-4. Loss Term 3 (CLIP contrastive Loss)

## 5. GAN-based Upsampler

## 6. 실험 결과