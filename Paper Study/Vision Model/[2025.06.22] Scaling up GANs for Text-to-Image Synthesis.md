## 목차

* [1. GigaGAN 핵심 아이디어](#1-gigagan-핵심-아이디어)
* [2. GigaGAN 의 전체 구조](#2-gigagan-의-전체-구조)
* [3. Generator](#3-generator)
  * [3-1. Text and latent-code conditioning](#3-1-text-and-latent-code-conditioning)
  * [3-2. Synthesis Network (with adaptive kernel selection)](#3-2-synthesis-network-with-adaptive-kernel-selection)
* [4. Discriminator](#4-discriminator)
  * [4-1. Text conditioning & Multi-scale image processing](#4-1-text-conditioning--multi-scale-image-processing)
  * [4-2. Loss Term 1 (Multi-scale input, multi-scale output adversarial)](#4-2-loss-term-1-multi-scale-input-multi-scale-output-adversarial)
  * [4-3. Loss Term 2 (Matching-aware Loss)](#4-3-loss-term-2-matching-aware-loss)
  * [4-4. Loss Term 3 (CLIP contrastive Loss)](#4-4-loss-term-3-clip-contrastive-loss)
* [5. GAN-based Upsampler](#5-gan-based-upsampler)
* [6. 실험 결과](#6-실험-결과)

## 논문 소개

* Minguk Kang and Jun-Yan Zhu et al., "Scaling up GANs for Text-to-Image Synthesis", 2023
* [arXiv Link](https://arxiv.org/pdf/2303.05511)
* 개인적으로는 Figure 1 최상단의 ```a human growing colorful flowers from her hair``` 의 컨셉을 Oh-LoRA 향후 버전에 유사하게 적용해 보고 싶음

## 1. GigaGAN 핵심 아이디어

* 기존 StyleGAN 구조의 문제점
  * **StyleGAN 구조의 성능 및 학습 데이터셋 규모만을 향상시키는 것** 은 한계가 있다. (unstable)
* GigaGAN 의 특징
  * **텍스트 - 이미지 합성**
  * latent interpolation, style mixing, 벡터 연산 등 **다양한 latent space 편집 기술** 적용 가능
* GigaGAN 의 핵심 아이디어

| 핵심 아이디어              | 설명                                                                                             |
|----------------------|------------------------------------------------------------------------------------------------|
| Multi-scale Training | image-text alignment 및 low-frequency detail 향상<br>(**낮은 해상도를 나타내는 block** 의 파라미터를 효과적으로 사용 가능) |
| Multi-stage Approach | 처음에 64 x 64 이미지 생성 → 512 x 512 로 upsampling                                                    |

## 2. GigaGAN 의 전체 구조

[(출처)](https://arxiv.org/pdf/2303.05511) : Minguk Kang and Jun-Yan Zhu et al., "Scaling up GANs for Text-to-Image Synthesis"

| 구성 요소                                         | 설명                                                                                                                                                                                                                                                                                     |
|-----------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| [Generator](#3-generator)                     | - **StyleGAN2** 기반<br>- **mapping network** : input → style vector $w$, $w = M(z, c)$<br>- output 생성에 사용되는 방법은 [Convolution](https://github.com/WannaBeSuperteur/AI-study/blob/main/Image%20Processing/Basics_CNN.md)<br>- Synthesis Network 에 **Sample-adaptive Kernel Selection** 적용 |
| [Discrinimator](#4-discriminator)             | - **'text branch' 와 'image branch'** 의 2개의 branch 로 구성<br>- **text conditioning** : text $c$ → text description $t_D$<br>- multi-scale image processing : generator 의 'pyramid' 구조의 각 level 을 **서로 독립적으로** 처리<br>- Loss Function 은 총 3개의 term 으로 구성                                    |
| [GAN-based Upsampler](#5-gan-based-upsampler) | - Synthesis Network 는 **비대칭적 [U-Net](https://github.com/WannaBeSuperteur/AI-study/blob/main/Image%20Processing/Model_U-Net.md) 구조** 로 64 x 64 → 512 x 512 로 upsampling                                                                                                                 |

## 3. Generator

![image](../images/GigaGAN_1.PNG)

GigaGAN 의 Generator 의 구조는 다음과 같다.

| 구성 요소                                                                                                       | 설명                                                                                                                                                                                                                 |
|-------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Baseline                                                                                                    | StyleGAN2 기반                                                                                                                                                                                                       |
| [Text and latent-code conditioning](#3-1-text-and-latent-code-conditioning)                                 | - **Frozen CLIP** feature extractor<br>- $T$ (additional **learned text encoder**)<br>- $M$ (MLP mapping network)                                                                                                  |
| [Synthesis Network (with adaptive kernel selection)](#3-2-synthesis-network-with-adaptive-kernel-selection) | 입력:<br>- $f_l$ (feature at layer $l$)<br>- $t_{local}$ (text embedding 중 prompt 와 관련된 부분)<br>- $w$ (style vector)<br>- 출력: 각 layer $l$ 에 대해, $f_{l+1} = g_{xa}^l(g_{attn}^l(g_{adaconv}^l(f_l, w), w), t_{local})$ |

### 3-1. Text and latent-code conditioning

### 3-2. Synthesis Network (with adaptive kernel selection)

## 4. Discriminator

### 4-1. Text conditioning & Multi-scale image processing

### 4-2. Loss Term 1 (Multi-scale input, multi-scale output adversarial)

### 4-3. Loss Term 2 (Matching-aware Loss)

### 4-4. Loss Term 3 (CLIP contrastive Loss)

## 5. GAN-based Upsampler

## 6. 실험 결과