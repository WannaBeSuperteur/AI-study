## ëª©ì°¨

* [1. GigaGAN í•µì‹¬ ì•„ì´ë””ì–´](#1-gigagan-í•µì‹¬-ì•„ì´ë””ì–´)
* [2. GigaGAN ì˜ ì „ì²´ êµ¬ì¡°](#2-gigagan-ì˜-ì „ì²´-êµ¬ì¡°)
* [3. Generator](#3-generator)
  * [3-1. Text and latent-code conditioning](#3-1-text-and-latent-code-conditioning)
  * [3-2. Synthesis Network (with adaptive kernel selection)](#3-2-synthesis-network-with-adaptive-kernel-selection)
* [4. Discriminator](#4-discriminator)
  * [4-1. Text conditioning & Multi-scale image processing](#4-1-text-conditioning--multi-scale-image-processing)
  * [4-2. Loss Function Overview](#4-2-loss-function-overview)
  * [4-3. Loss Term 1 (GAN Loss & Matching-aware Loss)](#4-3-loss-term-1-gan-loss--matching-aware-loss)
  * [4-4. Loss Term 2 (CLIP contrastive Loss)](#4-4-loss-term-2-clip-contrastive-loss)
  * [4-5. Loss Term 3 (Vision-aided adversarial loss)](#4-5-loss-term-3-vision-aided-adversarial-loss)
* [5. GAN-based Upsampler](#5-gan-based-upsampler)
* [6. ì‹¤í—˜ ê²°ê³¼](#6-ì‹¤í—˜-ê²°ê³¼)

## ë…¼ë¬¸ ì†Œê°œ

* Minguk Kang and Jun-Yan Zhu et al., "Scaling up GANs for Text-to-Image Synthesis", 2023
* [arXiv Link](https://arxiv.org/pdf/2303.05511)
* ê°œì¸ì ìœ¼ë¡œëŠ” Figure 1 ìµœìƒë‹¨ì˜ ```a human growing colorful flowers from her hair``` ì˜ ì»¨ì…‰ì„ Oh-LoRA í–¥í›„ ë²„ì „ì— ìœ ì‚¬í•˜ê²Œ ì ìš©í•´ ë³´ê³  ì‹¶ìŒ

## 1. GigaGAN í•µì‹¬ ì•„ì´ë””ì–´

* ê¸°ì¡´ StyleGAN êµ¬ì¡°ì˜ ë¬¸ì œì 
  * **StyleGAN êµ¬ì¡°ì˜ ì„±ëŠ¥ ë° í•™ìŠµ ë°ì´í„°ì…‹ ê·œëª¨ë§Œì„ í–¥ìƒì‹œí‚¤ëŠ” ê²ƒ** ì€ í•œê³„ê°€ ìˆë‹¤. (unstable)
* GigaGAN ì˜ íŠ¹ì§•
  * **í…ìŠ¤íŠ¸ - ì´ë¯¸ì§€ í•©ì„±**
  * latent interpolation, style mixing, ë²¡í„° ì—°ì‚° ë“± **ë‹¤ì–‘í•œ latent space í¸ì§‘ ê¸°ìˆ ** ì ìš© ê°€ëŠ¥
* GigaGAN ì˜ í•µì‹¬ ì•„ì´ë””ì–´

| í•µì‹¬ ì•„ì´ë””ì–´              | ì„¤ëª…                                                                                             |
|----------------------|------------------------------------------------------------------------------------------------|
| Multi-scale Training | image-text alignment ë° low-frequency detail í–¥ìƒ<br>(**ë‚®ì€ í•´ìƒë„ë¥¼ ë‚˜íƒ€ë‚´ëŠ” block** ì˜ íŒŒë¼ë¯¸í„°ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥) |
| Multi-stage Approach | ì²˜ìŒì— 64 x 64 ì´ë¯¸ì§€ ìƒì„± â†’ 512 x 512 ë¡œ upsampling                                                    |

## 2. GigaGAN ì˜ ì „ì²´ êµ¬ì¡°

| êµ¬ì„± ìš”ì†Œ                                         | ì„¤ëª…                                                                                                                                                                                                                                                  |
|-----------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| [Generator](#3-generator)                     | - **StyleGAN2** ê¸°ë°˜<br>- **mapping network** : input â†’ style vector $w$, $w = M(z, t_{global})$<br>- output ìƒì„±ì— ì‚¬ìš©ë˜ëŠ” ë°©ë²•ì€ [Convolution](../../Image%20Processing/Basics_CNN.md)<br>- Synthesis Network ì— **Sample-adaptive Kernel Selection** ì ìš©      |
| [Discrinimator](#4-discriminator)             | - **'text branch' ì™€ 'image branch'** ì˜ 2ê°œì˜ branch ë¡œ êµ¬ì„±<br>- **text conditioning** : text $c$ â†’ text description $t_D$<br>- multi-scale image processing : generator ì˜ 'pyramid' êµ¬ì¡°ì˜ ê° level ì„ **ì„œë¡œ ë…ë¦½ì ìœ¼ë¡œ** ì²˜ë¦¬<br>- Loss Function ì€ ì´ 3ê°œì˜ term ìœ¼ë¡œ êµ¬ì„± |
| [GAN-based Upsampler](#5-gan-based-upsampler) | - Synthesis Network ëŠ” **ë¹„ëŒ€ì¹­ì  [U-Net](../../Image%20Processing/Model_U-Net.md) êµ¬ì¡°** ë¡œ 64 x 64 â†’ 512 x 512 ë¡œ upsampling                                                                                                                               |

## 3. Generator

![image](../images/GigaGAN_1.PNG)

[(ì¶œì²˜)](https://arxiv.org/pdf/2303.05511) : Minguk Kang and Jun-Yan Zhu et al., "Scaling up GANs for Text-to-Image Synthesis"

GigaGAN ì˜ Generator ì˜ êµ¬ì¡°ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

| êµ¬ì„± ìš”ì†Œ                                                                                                       | ì„¤ëª…                                                                                                                                                                                                               |
|-------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Baseline                                                                                                    | StyleGAN2 ê¸°ë°˜                                                                                                                                                                                                     |
| [Text and latent-code conditioning](#3-1-text-and-latent-code-conditioning)                                 | - **Frozen CLIP** feature extractor<br>- $T$ (additional **learned text encoder**)<br>- $M$ (MLP mapping network)                                                                                                |
| [Synthesis Network (with adaptive kernel selection)](#3-2-synthesis-network-with-adaptive-kernel-selection) | ì…ë ¥:<br>- $f_l$ (feature at layer $l$)<br>- $t_{local}$ (text embedding ì¤‘ prompt ì™€ ê´€ë ¨ëœ ë¶€ë¶„)<br>- $w$ (style vector)<br>ì¶œë ¥: ê° layer $l$ ì— ëŒ€í•´, $f_{l+1} = g_{xa}^l(g_{attn}^l(g_{adaconv}^l(f_l, w), w), t_{local})$ |

### 3-1. Text and latent-code conditioning

![image](../images/GigaGAN_2.PNG)

[(ì¶œì²˜)](https://arxiv.org/pdf/2303.05511) : Minguk Kang and Jun-Yan Zhu et al., "Scaling up GANs for Text-to-Image Synthesis"

**Text and latent-code conditioning** ì˜ í”„ë¡œì„¸ìŠ¤ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

* ê¸°ë³¸ í”„ë¡œì„¸ìŠ¤
  * **StyleGAN2** ê¸°ë°˜

* text input â†’ text embedding $t$
  * text input ì— ëŒ€í•œ **conditional vector** $c \in R^{C \times 768}$ ìƒì„±
  * $c$ ë¥¼ **Pre-trained CLIP text encoder** ë¡œ forward ì‹œì¼œì„œ $\epsilon_{txt}(c)$ ë¥¼ ìƒì„±
  * $\epsilon_{txt}(c)$ ë¥¼ **Learned text encoder ($T$)** ë¡œ forward ì‹œì¼œì„œ text embedding $t \in R^{C \times 768}$ ë¥¼ ìƒì„±

* text embedding $t$ â†’ style vector $w$
  * $t$ ë¥¼ $t_{local} \in R^{(C - 1) \times 768}$ ê³¼ $t_{global} \in R^{768}$ ë¡œ ë¶„í• 
  * $z \sim N(0, 1)$ ì„ ìƒì„±
  * $t_{global}$ ê³¼ $z$ ë¥¼ **MLP mapping network ($M$)** ìœ¼ë¡œ forward ì‹œì¼œì„œ style vector $w = M(z, t_{global})$ ë¥¼ ìƒì„±

* style vector $w$ â†’ generated image $x$
  * $t_{local}$ ê³¼ $w$ ë¥¼ **Synthesis Network ($\tilde{G}$)** ìœ¼ë¡œ forward ì‹œì¼œì„œ image $x$ ë¥¼ ìƒì„±
  * generated image $x = \tilde{G}(w, t_{local})$

### 3-2. Synthesis Network (with adaptive kernel selection)

![image](../images/GigaGAN_3.PNG)

[(ì¶œì²˜)](https://arxiv.org/pdf/2303.05511) : Minguk Kang and Jun-Yan Zhu et al., "Scaling up GANs for Text-to-Image Synthesis"

**1. Generator ($\tilde{G}$) ì˜ êµ¬ì„±**

| êµ¬ì„± ìš”ì†Œ                                                                                                                           | ì„¤ëª…                                                                                                |
|---------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------|
| Convolutional block<br>($g_{adaconv}^l$)                                                                                        |                                                                                                   |
| [Self-Attention](../../Natural%20Language%20Processing/Basics_íŠ¸ëœìŠ¤í¬ë¨¸%20ëª¨ë¸.md#3-1-encoder-self-attention) block<br>($g_{attn}^l$) | Convolutional Layer ê°€ **receptive field ê¸°ë°˜ìœ¼ë¡œ ì²˜ë¦¬** í•˜ë¯€ë¡œ **ë©€ë¦¬ ë–¨ì–´ì§„ ë¶€ë¶„ì„ contextualize í•˜ê¸° ì–´ë µë‹¤** ëŠ” ë¬¸ì œ í•´ê²° |
| Cross-Attention block<br>($g_{xa}^l$)                                                                                           | ê°œë³„ word embedding ì²˜ë¦¬ ëª©ì ìœ¼ë¡œ ì‚¬ìš©                                                                      |

**2. ì´ë¯¸ì§€ ìƒì„± í”„ë¡œì„¸ìŠ¤**

* Synthesis Network ëŠ” **upsampling convolutional layer ì˜ í”¼ë¼ë¯¸ë“œ êµ¬ì¡°** ë¡œ ë˜ì–´ ìˆìŒ
  * ì¦‰, feature $f_l$ ì„ ë‹¤ìŒ layer ì˜ feature $f_{l+1}$ ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” **ì—¬ëŸ¬ ê°œì˜ level** ë¡œ êµ¬ì„±
* ê° level ì—ì„œ ë‹¤ìŒê³¼ ê°™ì´ ì²˜ë¦¬ë¨
  * feature $f_l$ ì€ $g_{adaconv}^l$, $g_{attn}^l$, $g_{xa}^l$ ìˆœì„œëŒ€ë¡œ network forward ë¨
  * ì´ë¥¼ ìˆ˜ì‹ìœ¼ë¡œ ë‚˜íƒ€ë‚´ë©´ $f_{l+1} = g_{xa}^l(g_{attn}^l(g_{adaconv}^l(f_l, w), w), t_{local})$ ì´ ë¨
* ìµœì¢…ì ìœ¼ë¡œ ë‹¤ìŒê³¼ ê°™ì´ **í”¼ë¼ë¯¸ë“œ êµ¬ì¡°ë¡œ ì—¬ëŸ¬ ì´ë¯¸ì§€ê°€ ìƒì„±** ë¨
  * $x_0$ (4 x 4), $x_1$ (8 x 8), $x_2$ (16 x 16), $x_3$ (32 x 32), $x_4$ (64 x 64) 

**3. Sample-adaptive kernel selection**

Sample-adaptive kernel selection ì€ **í•™ìŠµ ë°ì´í„° (ì¸í„°ë„·ìƒì˜ ì´ë¯¸ì§€) ì˜ íŠ¹ì§•ì´ ë§¤ìš° ë‹¤ì–‘** í•˜ë¯€ë¡œ, ì´ë¥¼ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ **Convolution Kernel ì˜ í‘œí˜„ ëŠ¥ë ¥ì„ "text-conditioning ê¸°ë°˜ on-the-fly ë°©ë²•"ìœ¼ë¡œ í–¥ìƒ** ì‹œí‚¤ê¸° ìœ„í•œ ê²ƒì´ë‹¤.

* $N$ ê°œì˜ filter ë¥¼ ì €ì¥í•˜ëŠ” Filter Bank ë¥¼ ì´ˆê¸°í™”
  * ê° filter ëŠ” $K_i \in R^{C_{in} \times C_{out} \times K \times K}$
* ì´ Filter Bank ì˜ filter ë“¤ì„ **weighted average** í•˜ì—¬ **aggregated filter** ìƒì„±
  * $K \in R^{C_{in} \times C_{out} \times K \times K}$
  * $\displaystyle K = \Sigma_{i=1}^N K_i Â· softmax(W_{filter}^T w + b_{filter})_i$
  * í•´ë‹¹ í•„í„°ëŠ” **StyleGAN2ì˜ convolution pipeline** ì— ì‚¬ìš©
* ìµœì¢… modulated weights $g_{adaconv}(f, w)$ ìƒì„±
  * $g_{adaconv}(f, w) = ((W_{mod}^T w + b_{mod}) âŠ— K) * f$
  * **Convolutional block ì˜ ìµœì¢… ì¶œë ¥ê°’** ì— í•´ë‹¹

| âŠ—               | *           |
|-----------------|-------------|
| (de-)modulation | convolution |

## 4. Discriminator

![image](../images/GigaGAN_4.PNG)

[(ì¶œì²˜)](https://arxiv.org/pdf/2303.05511) : Minguk Kang and Jun-Yan Zhu et al., "Scaling up GANs for Text-to-Image Synthesis"

**GigaGAN Discriminator** ëŠ” ë‹¤ìŒê³¼ ê°™ì€ í”„ë¡œì„¸ìŠ¤ë¡œ êµ¬ì„±ëœë‹¤.

| í”„ë¡œì„¸ìŠ¤                         | ì„¤ëª…                                                                    |
|------------------------------|-----------------------------------------------------------------------|
| Text Conditioning            | text $c$ â†’ text descriptor $t_D$                                      |
| Multi-scale image processing | ì—¬ëŸ¬ scale ì˜ feature vector ë¥¼ ì¶”ì¶œ (resoution: ```32 â†’ 16 â†’ 8 â†’ 4 â†’ 1```) |

**GigaGAN ì˜ Loss Function** ì€ ë‹¤ìŒê³¼ ê°™ì´ êµ¬ì„±ëœë‹¤.

* Matching-aware Loss
* CLIP contrastive Loss
* Vision-aided adversarial Loss

### 4-1. Text conditioning & Multi-scale image processing

**1. Text Conditioning**

* ì›ë³¸ í…ìŠ¤íŠ¸ $c$ ë¥¼ **text descriptor $t_D$** ë¡œ mapping ì‹œí‚¨ë‹¤.
  * ì´ë•Œ, **CLIP ê³¼ ê°™ì€ text encoder + trainable attention layer** ë¥¼ ì´ìš©í•œë‹¤.

**2. Multi-scale image processing**

* ìœ„ ê·¸ë¦¼ê³¼ ê°™ì€ **í”¼ë¼ë¯¸ë“œ êµ¬ì¡°** ì—ì„œ, ê° resolution level ì„ **ì„œë¡œ ë…ë¦½ì ìœ¼ë¡œ ì²˜ë¦¬** í•œë‹¤.
* ì´ 5ê°œì˜ feature extractor $\phi_{iâ†’j} : R^{X_i \times X_i \times 3} â†’ R^{X_j^D \times X_j^D \times C_j}$ ë¥¼ ì •ì˜í•œë‹¤.
  * ê° feature extractor ëŠ” **Convolutional Layer + Self-Attention Layer** ë¡œ êµ¬ì„±ëœë‹¤.
  * ê° feature extractor ì˜ output ì˜ resolution ì€ **32 â†’ 16 â†’ 8 â†’ 4 â†’ 1** ìˆœìœ¼ë¡œ, ìµœì¢… ì¶œë ¥ ê²°ê³¼ë¬¼ì˜ ì°¨ì›ì€ $R^{1 \times 1 \times C_5}$ ì´ë‹¤.

### 4-2. Loss Function Overview

**GigaGAN ì˜ [Loss Function](../../AI%20Basics/Deep%20Learning%20Basics/ë”¥ëŸ¬ë‹_ê¸°ì´ˆ_Loss_function.md)** ì€ ë‹¤ìŒê³¼ ê°™ì´ êµ¬ì„±ë˜ë©°, **ì „ì²´ Loss ëŠ” ì´ë“¤ì˜ í•©** ìœ¼ë¡œ ì •ì˜ëœë‹¤.

* GigaGAN **Total Loss** : $V(G, D) = V_{MS-I/O}(G, D) + L_{CLIP}(G) + L_{Vision}(G)$
* Gen = Generator, Dis = Discriminator

| Loss Term                                                                                      | ì ìš© ëª¨ë¸                      | ì„¤ëª…                                                                                                                                                                                                                              |
|------------------------------------------------------------------------------------------------|----------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| [GAN Loss & Matching-aware Loss](#4-3-loss-term-1-gan-loss--matching-aware-loss), $V_{MS-I/O}$ | Gen, Dis                   | - **GAN Loss** : [ì›ë˜ í‘œì¤€ GAN ì˜ Loss Function](../../Generative%20AI/Basics_GAN.md#2-ganì˜-êµ¬ì¡°-ë°-ì‘ë™-ì›ë¦¬)<br>- **Matching-aware Loss** : Discriminator ê°€ 'ì¡°ê±´'ì„ ì˜ ë°˜ì˜í•˜ë„ë¡ í•˜ê¸° ìœ„í•´, **ëœë¤í•˜ê²Œ ìƒì„±ëœ condition $\hat{c}$ ë¥¼ ì´ìš©** í•˜ì—¬ fake pair ë¥¼ ìƒì„± |
| [CLIP contrastive Loss](#4-4-loss-term-2-clip-contrastive-loss), $L_{CLIP}$                    | Gen                        | **Pre-trained CLIP encoder** ë¥¼ ì´ìš©í•œ **contrastive loss**                                                                                                                                                                         |
| [Vision-aided adversarial loss](#4-5-loss-term-3-vision-aided-adversarial-loss), $L_{vision}$  | Gen **(+ Additional Dis)** | **Vision-Aided GAN** ì˜ Discriminator (CLIP ëª¨ë¸ ì‚¬ìš©) ë¥¼ ì¶”ê°€ë¡œ ì‚¬ìš©                                                                                                                                                                      |

### 4-3. Loss Term 1 (GAN Loss & Matching-aware Loss)

**1. $V_{MS-I/O}$ Loss Term: Multi-scale input, multi-scale output adversarial loss**

* $V_{MS-I/O}$ Loss Term ì€ ë‹¤ìŒê³¼ ê°™ì´ **[GAN Loss](../../Generative%20AI/Basics_GAN.md#2-ganì˜-êµ¬ì¡°-ë°-ì‘ë™-ì›ë¦¬) ($V_{GAN}$) + Matching-aware Loss ($V_{match}$)** ì˜ í•©ì„ ì˜ë¯¸í•œë‹¤.

![image](../images/GigaGAN_5.PNG)

[(ì¶œì²˜)](https://arxiv.org/pdf/2303.05511) : Minguk Kang and Jun-Yan Zhu et al., "Scaling up GANs for Text-to-Image Synthesis"

| notation            | ì„¤ëª…                                         |
|---------------------|--------------------------------------------|
| $\psi_j$            | 4-layer 1 $\times$ 1 modulated Convolution |
| $\phi_{iâ†’j}$        | Discriminator ì˜ Feature Extractor          |
| $Conv_{1 \times 1}$ | skip connection ìœ¼ë¡œ ì¶”ê°€ëœ ë¶€ë¶„                  |

**2. Matching-aware Loss ($V_{match}$)**

ê¸°ì¡´ **GAN Loss** ì™¸ì—, **Matching-aware Loss** ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì—­í• ì„ í•œë‹¤.

* **ëœë¤í•˜ê²Œ ìƒì„±ëœ condition $\hat{c}$** ì„ ì´ìš©í•˜ì—¬ fake pair ë¥¼ ìƒì„±
* í•´ë‹¹ fake pair ë¥¼ í•™ìŠµí•¨ìœ¼ë¡œì¨ **Discriminator ê°€ 'ì¡°ê±´'ì„ ë” ì˜ ë°˜ì˜í•  ìˆ˜ ìˆìŒ**

Matching-aware Loss ëŠ” [ê¸°ì¡´ GAN Loss](../../Generative%20AI/Basics_GAN.md#2-ganì˜-êµ¬ì¡°-ë°-ì‘ë™-ì›ë¦¬) ì™€ ìœ ì‚¬í•œ ì ì´ ìˆë‹¤.

![image](../images/GigaGAN_6.PNG)

[(ì¶œì²˜)](https://arxiv.org/pdf/2303.05511) : Minguk Kang and Jun-Yan Zhu et al., "Scaling up GANs for Text-to-Image Synthesis"

### 4-4. Loss Term 2 (CLIP contrastive Loss)

**CLIP contrastive Loss** ëŠ” **Pre-trained CLIP image & text encoder** ë¥¼ ì´ìš©í•˜ì—¬ ì„ë² ë”©í•œ ê²°ê³¼ë¬¼ì— ëŒ€í•œ **Contrastive [Cross-Entropy Loss](../../AI%20Basics/Deep%20Learning%20Basics/ë”¥ëŸ¬ë‹_ê¸°ì´ˆ_Loss_function.md#2-5-categorical-cross-entropy-loss)** ì´ë‹¤.

* Contrastive Learning ì´ë€ **ê°€ê¹Œì´ ìœ„ì¹˜í•˜ë„ë¡ í•˜ëŠ” ê²ƒì€ feature space ìƒì—ì„œ ê±°ë¦¬ë¥¼ ê°€ê¹ê²Œ, ë©€ë¦¬ ìœ„ì¹˜í•˜ë„ë¡ í•˜ëŠ” ê²ƒì€ feature space ìƒì—ì„œ ê±°ë¦¬ë¥¼ ë©€ê²Œ** í•˜ë„ë¡ í•™ìŠµì‹œí‚¤ëŠ” ê²ƒì´ë‹¤.
  * ì¦‰, [S-BERT (Sentence BERT)](../../Natural%20Language%20Processing/Basics_BERT,%20SBERT%20ëª¨ë¸.md#sbert-ëª¨ë¸) ì˜ í•™ìŠµ ë°©ë²•ê³¼ ì»¨ì…‰ì´ ì¼ì • ë¶€ë¶„ ìœ ì‚¬í•˜ë‹¤.

![image](../images/GigaGAN_7.PNG)

[(ì¶œì²˜)](https://arxiv.org/pdf/2303.05511) : Minguk Kang and Jun-Yan Zhu et al., "Scaling up GANs for Text-to-Image Synthesis"

| notation             | ì„¤ëª…                                  |
|----------------------|-------------------------------------|
| $\eta_{img}$         | CLIP image encoder                  |
| $\eta_{txt}$         | CLIP text encoder                   |
| $c_0, c_1, ..., c_n$ | training data ì—ì„œ sampling ëœ caption |

### 4-5. Loss Term 3 (Vision-aided adversarial loss)

**Vision-aided adversarial loss ( $L_{Vision}(G)$ )** ì˜ ì•„ì´ë””ì–´ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

* **Vision-Aided GAN ì˜ Discriminator (CLIP ëª¨ë¸ ì‚¬ìš©)** ë¥¼ ì¶”ê°€ Discriminator ë¡œ ì‚¬ìš©
  * í•´ë‹¹ Discriminator ì˜ **CLIP image encoder ë¥¼ freeze** í•œ í›„, intermediate layer ë¡œë¶€í„° feature extraction ì‹¤ì‹œ
  * í•´ë‹¹ feature extraction ì— ëŒ€í•´ **real/fake íŒì •** ì‹¤ì‹œ
* í•™ìŠµ ì•ˆì •í™”ë¥¼ ìœ„í•´ì„œ, **Projected GAN** ì´ë¼ëŠ” random projection layer ë¥¼ ì¶”ê°€

## 5. GAN-based Upsampler

**GAN-based Upsampler** ì˜ Synthesis Network ì˜ êµ¬ì„±ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.

* **ë¹„ëŒ€ì¹­ì  [U-Net](../../Image%20Processing/Model_U-Net.md) êµ¬ì¡°**
* ìµœì¢…ì ìœ¼ë¡œ, ì´ë¯¸ì§€ë¥¼ 64 x 64 â†’ 512 x 512 ë¡œ upsampling

| êµ¬ì¡°                    | ì„¤ëª…                                                               |
|-----------------------|------------------------------------------------------------------|
| **downsampling** part | 3 ê°œì˜ downsampling residual block ìœ¼ë¡œ êµ¬ì„±                           |
| **upsampling** part   | 6 ê°œì˜ upsampling residual block **(with attention layers)** ìœ¼ë¡œ êµ¬ì„± |

## 6. ì‹¤í—˜ ê²°ê³¼

**1. Prompt Interpolation**

í•œ ì†ì„±ì´ ë°”ë€” ë•Œ ì´ì™€ ê´€ë ¨ ì—†ëŠ” ë‹¤ë¥¸ ì†ì„±ë“¤ë„ **ì•½ê°„ì”©ì€ ë°”ë€ŒëŠ”** ë“¯í•˜ë‹¤. ê·¸ëŸ¬ë‚˜, ë³¸ì¸ì˜ ê°œì¸ í”„ë¡œì íŠ¸ì¸ ì—¬ì„± ê°€ìƒ ì¸ê°„ **Oh-LoRA ğŸ‘±â€â™€ï¸ (ì˜¤ë¡œë¼)** ì˜ í–¥í›„ ë²„ì „ì— ì´ ë°©ë²•ì„ ì ìš©í•œë‹¤ë©´ **ì¡°ê¸ˆ ë” ë‚˜ì€ Oh-LoRA í‘œì •/ëª¸ì§“ ë³€í™”** ë¥¼ ì–»ì„ ìˆ˜ ìˆì„ ê²ƒìœ¼ë¡œ ì˜ˆìƒí•œë‹¤.

* ```A {modern|victorian} mansion``` & ```in a {sunny day|sunset}``` (1)

![image](../images/GigaGAN_8.PNG)

[(ì¶œì²˜)](https://arxiv.org/pdf/2303.05511) : Minguk Kang and Jun-Yan Zhu et al., "Scaling up GANs for Text-to-Image Synthesis"

* ```A {modern|victorian} mansion``` & ```in a {sunny day|sunset}``` (2)

![image](../images/GigaGAN_9.PNG)

[(ì¶œì²˜)](https://arxiv.org/pdf/2303.05511) : Minguk Kang and Jun-Yan Zhu et al., "Scaling up GANs for Text-to-Image Synthesis"

* ```{Roses|Sunflowers}.``` & ```{oil painting|photograph}```

![image](../images/GigaGAN_10.PNG)

[(ì¶œì²˜)](https://arxiv.org/pdf/2303.05511) : Minguk Kang and Jun-Yan Zhu et al., "Scaling up GANs for Text-to-Image Synthesis"

**2. Style Mixing (Human Face)**

![image](../images/GigaGAN_11.PNG)

[(ì¶œì²˜)](https://arxiv.org/pdf/2303.05511) : Minguk Kang and Jun-Yan Zhu et al., "Scaling up GANs for Text-to-Image Synthesis"
