각 모델에 대한 자세한 정보는 ```experiment_log_cvae_GPU.md``` 파일 참고.

## MODEL 28, 29, 30 차이점

||MODEL 28|MODEL 29|MODEL 30|
|---|---|---|---|
|HIDDEN_DIMS|231|231|149|
|decoder 60 x 60, 120 x 120 feature direct connection|O|X|O|

## MODEL 30
**학습 로그**
```
Epoch 1/60
2024-04-21 14:54:49.338377: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8907
2024-04-21 14:54:50.293885: W tensorflow/stream_executor/gpu/asm_compiler.cc:111] *** WARNING *** You are using ptxas 11.0.194, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.

You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.
16864/16864 [==============================] - 130s 8ms/sample - loss: 4755.0363 - lr: 4.0000e-04
Epoch 2/60
16864/16864 [==============================] - 129s 8ms/sample - loss: 3360.1639 - lr: 4.0000e-04
Epoch 3/60
16864/16864 [==============================] - 130s 8ms/sample - loss: 3120.6261 - lr: 4.0000e-04
Epoch 4/60
16864/16864 [==============================] - 131s 8ms/sample - loss: 2970.7720 - lr: 4.0000e-04
Epoch 5/60
16864/16864 [==============================] - 131s 8ms/sample - loss: 2874.3569 - lr: 3.9000e-04
Epoch 6/60
16864/16864 [==============================] - 132s 8ms/sample - loss: 2788.2695 - lr: 3.8025e-04
Epoch 7/60
16864/16864 [==============================] - 132s 8ms/sample - loss: 2718.0864 - lr: 3.7074e-04
Epoch 8/60
16864/16864 [==============================] - 132s 8ms/sample - loss: 2663.5662 - lr: 3.6148e-04
Epoch 9/60
16864/16864 [==============================] - 132s 8ms/sample - loss: 2602.2131 - lr: 3.5244e-04
Epoch 10/60
16864/16864 [==============================] - 132s 8ms/sample - loss: 2558.6862 - lr: 3.4363e-04
Epoch 11/60
16864/16864 [==============================] - 132s 8ms/sample - loss: 2502.8397 - lr: 3.3504e-04
Epoch 12/60
16864/16864 [==============================] - 132s 8ms/sample - loss: 2456.4720 - lr: 3.2666e-04
Epoch 13/60
16864/16864 [==============================] - 132s 8ms/sample - loss: 2409.2965 - lr: 3.1849e-04
Epoch 14/60
16864/16864 [==============================] - 132s 8ms/sample - loss: 2367.0115 - lr: 3.1053e-04
Epoch 15/60
16864/16864 [==============================] - 132s 8ms/sample - loss: 2325.4493 - lr: 3.0277e-04
Epoch 16/60
16864/16864 [==============================] - 132s 8ms/sample - loss: 2286.2942 - lr: 2.9520e-04
Epoch 17/60
16864/16864 [==============================] - 132s 8ms/sample - loss: 2240.0588 - lr: 2.8782e-04
Epoch 18/60
16864/16864 [==============================] - 132s 8ms/sample - loss: 2200.0304 - lr: 2.8062e-04
Epoch 19/60
16864/16864 [==============================] - 132s 8ms/sample - loss: 2162.3508 - lr: 2.7361e-04
Epoch 20/60
16864/16864 [==============================] - 133s 8ms/sample - loss: 2122.2130 - lr: 2.6677e-04
Epoch 21/60
16864/16864 [==============================] - 133s 8ms/sample - loss: 2087.2146 - lr: 2.6010e-04
Epoch 22/60
16864/16864 [==============================] - 132s 8ms/sample - loss: 2049.2267 - lr: 2.5360e-04
Epoch 23/60
16864/16864 [==============================] - 132s 8ms/sample - loss: 2011.4648 - lr: 2.4726e-04
Epoch 24/60
16864/16864 [==============================] - 132s 8ms/sample - loss: 1980.7810 - lr: 2.4108e-04
Epoch 25/60
16864/16864 [==============================] - 132s 8ms/sample - loss: 1944.9631 - lr: 2.3505e-04
Epoch 26/60
16864/16864 [==============================] - 132s 8ms/sample - loss: 1910.4221 - lr: 2.2917e-04
Epoch 27/60
16864/16864 [==============================] - 132s 8ms/sample - loss: 1875.5178 - lr: 2.2344e-04
Epoch 28/60
16864/16864 [==============================] - 132s 8ms/sample - loss: 1849.1796 - lr: 2.1786e-04
Epoch 29/60
16864/16864 [==============================] - 132s 8ms/sample - loss: 1822.5898 - lr: 2.1241e-04
Epoch 30/60
16864/16864 [==============================] - 132s 8ms/sample - loss: 1794.1397 - lr: 2.0710e-04
Epoch 31/60
16864/16864 [==============================] - 132s 8ms/sample - loss: 1768.3969 - lr: 2.0192e-04
Epoch 32/60
16864/16864 [==============================] - 132s 8ms/sample - loss: 1745.7649 - lr: 1.9687e-04
Epoch 33/60
16864/16864 [==============================] - 131s 8ms/sample - loss: 1717.9128 - lr: 1.9195e-04
Epoch 34/60
16864/16864 [==============================] - 131s 8ms/sample - loss: 1698.0928 - lr: 1.8715e-04
Epoch 35/60
16864/16864 [==============================] - 131s 8ms/sample - loss: 1676.6619 - lr: 1.8247e-04
Epoch 36/60
16864/16864 [==============================] - 132s 8ms/sample - loss: 1654.5372 - lr: 1.7791e-04
Epoch 37/60
16864/16864 [==============================] - 132s 8ms/sample - loss: 1635.0138 - lr: 1.7347e-04
Epoch 38/60
16864/16864 [==============================] - 131s 8ms/sample - loss: 1610.0323 - lr: 1.6913e-04
Epoch 39/60
16864/16864 [==============================] - 131s 8ms/sample - loss: 1594.5154 - lr: 1.6490e-04
Epoch 40/60
16864/16864 [==============================] - 131s 8ms/sample - loss: 1572.3686 - lr: 1.6078e-04
Epoch 41/60
16864/16864 [==============================] - 131s 8ms/sample - loss: 1555.6286 - lr: 1.5676e-04
Epoch 42/60
16864/16864 [==============================] - 131s 8ms/sample - loss: 1541.9175 - lr: 1.5284e-04
Epoch 43/60
16864/16864 [==============================] - 131s 8ms/sample - loss: 1523.9654 - lr: 1.4902e-04
Epoch 44/60
16864/16864 [==============================] - 131s 8ms/sample - loss: 1509.3555 - lr: 1.4529e-04
Epoch 45/60
16864/16864 [==============================] - 131s 8ms/sample - loss: 1493.9440 - lr: 1.4166e-04
Epoch 46/60
16864/16864 [==============================] - 131s 8ms/sample - loss: 1477.4304 - lr: 1.3812e-04
Epoch 47/60
16864/16864 [==============================] - 131s 8ms/sample - loss: 1465.1736 - lr: 1.3467e-04
Epoch 48/60
16864/16864 [==============================] - 132s 8ms/sample - loss: 1450.7437 - lr: 1.3130e-04
Epoch 49/60
16864/16864 [==============================] - 131s 8ms/sample - loss: 1441.3151 - lr: 1.2802e-04
Epoch 50/60
16864/16864 [==============================] - 131s 8ms/sample - loss: 1428.4416 - lr: 1.2482e-04
Epoch 51/60
16864/16864 [==============================] - 131s 8ms/sample - loss: 1418.2368 - lr: 1.2170e-04
Epoch 52/60
16864/16864 [==============================] - 131s 8ms/sample - loss: 1406.1589 - lr: 1.1865e-04
Epoch 53/60
16864/16864 [==============================] - 131s 8ms/sample - loss: 1396.6022 - lr: 1.1569e-04
Epoch 54/60
16864/16864 [==============================] - 132s 8ms/sample - loss: 1385.0001 - lr: 1.1280e-04
Epoch 55/60
16864/16864 [==============================] - 132s 8ms/sample - loss: 1375.3753 - lr: 1.0998e-04
Epoch 56/60
16864/16864 [==============================] - 132s 8ms/sample - loss: 1365.0408 - lr: 1.0723e-04
Epoch 57/60
16864/16864 [==============================] - 132s 8ms/sample - loss: 1356.3636 - lr: 1.0455e-04
Epoch 58/60
16864/16864 [==============================] - 132s 8ms/sample - loss: 1346.6184 - lr: 1.0193e-04
Epoch 59/60
16864/16864 [==============================] - 132s 8ms/sample - loss: 1339.0214 - lr: 9.9383e-05
Epoch 60/60
16864/16864 [==============================] - 132s 8ms/sample - loss: 1330.5528 - lr: 9.9383e-05
```

**Loss 그래프**

![cvae_train_result](https://github.com/WannaBeSuperteur/AI-study/assets/32893014/cffb258f-ffeb-4dbd-8b5f-13d0917a42b1)

**60 epochs 생성 이미지**
* Final Loss : **1330.5528**

![생성이미지](https://github.com/WannaBeSuperteur/AI-study/assets/32893014/45252dff-6c7d-4506-92d8-cd233606e32b)

**24 epochs 생성 이미지**
* Final Loss : **2007.7193**

![image](https://github.com/WannaBeSuperteur/AI-study/assets/32893014/46a39880-724f-4f1b-8c71-5500affa3b2a)

## MODEL 29
**학습 로그**
```
Epoch 1/60
2024-04-21 00:20:48.955028: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8907
2024-04-21 00:20:49.901253: W tensorflow/stream_executor/gpu/asm_compiler.cc:111] *** WARNING *** You are using ptxas 11.0.194, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.

You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.
16864/16864 [==============================] - 98s 6ms/sample - loss: 4586.3311 - lr: 6.0000e-04
Epoch 2/60
16864/16864 [==============================] - 96s 6ms/sample - loss: 2998.4116 - lr: 6.0000e-04
Epoch 3/60
16864/16864 [==============================] - 96s 6ms/sample - loss: 2765.0989 - lr: 6.0000e-04
Epoch 4/60
16864/16864 [==============================] - 97s 6ms/sample - loss: 2640.8741 - lr: 6.0000e-04
Epoch 5/60
16864/16864 [==============================] - 97s 6ms/sample - loss: 2546.9036 - lr: 5.8050e-04
Epoch 6/60
16864/16864 [==============================] - 101s 6ms/sample - loss: 2464.8724 - lr: 5.6163e-04
Epoch 7/60
16864/16864 [==============================] - 98s 6ms/sample - loss: 2398.8944 - lr: 5.4338e-04
Epoch 8/60
16864/16864 [==============================] - 98s 6ms/sample - loss: 2335.2740 - lr: 5.2572e-04
Epoch 9/60
16864/16864 [==============================] - 98s 6ms/sample - loss: 2280.4274 - lr: 5.0863e-04
Epoch 10/60
16864/16864 [==============================] - 98s 6ms/sample - loss: 2224.9886 - lr: 4.9210e-04
Epoch 11/60
16864/16864 [==============================] - 98s 6ms/sample - loss: 2172.2823 - lr: 4.7611e-04
Epoch 12/60
16864/16864 [==============================] - 98s 6ms/sample - loss: 2117.0965 - lr: 4.6064e-04
Epoch 13/60
16864/16864 [==============================] - 98s 6ms/sample - loss: 2072.7761 - lr: 4.4567e-04
Epoch 14/60
16864/16864 [==============================] - 98s 6ms/sample - loss: 2018.3441 - lr: 4.3118e-04
Epoch 15/60
16864/16864 [==============================] - 98s 6ms/sample - loss: 1972.2839 - lr: 4.1717e-04
Epoch 16/60
16864/16864 [==============================] - 98s 6ms/sample - loss: 1923.4866 - lr: 4.0361e-04
Epoch 17/60
16864/16864 [==============================] - 98s 6ms/sample - loss: 1881.6224 - lr: 3.9049e-04
Epoch 18/60
16864/16864 [==============================] - 98s 6ms/sample - loss: 1841.0156 - lr: 3.7780e-04
Epoch 19/60
16864/16864 [==============================] - 98s 6ms/sample - loss: 1805.0519 - lr: 3.6552e-04
Epoch 20/60
16864/16864 [==============================] - 98s 6ms/sample - loss: 1765.6020 - lr: 3.5364e-04
Epoch 21/60
16864/16864 [==============================] - 98s 6ms/sample - loss: 1734.0044 - lr: 3.4215e-04
Epoch 22/60
16864/16864 [==============================] - 98s 6ms/sample - loss: 1698.5078 - lr: 3.3103e-04
Epoch 23/60
16864/16864 [==============================] - 98s 6ms/sample - loss: 1669.3223 - lr: 3.2027e-04
Epoch 24/60
16864/16864 [==============================] - 98s 6ms/sample - loss: 1642.2402 - lr: 3.0986e-04
Epoch 25/60
16864/16864 [==============================] - 98s 6ms/sample - loss: 1615.3547 - lr: 2.9979e-04
Epoch 26/60
16864/16864 [==============================] - 98s 6ms/sample - loss: 1592.1864 - lr: 2.9005e-04
Epoch 27/60
16864/16864 [==============================] - 98s 6ms/sample - loss: 1564.7816 - lr: 2.8062e-04
Epoch 28/60
16864/16864 [==============================] - 98s 6ms/sample - loss: 1546.4153 - lr: 2.7150e-04
Epoch 29/60
16864/16864 [==============================] - 98s 6ms/sample - loss: 1530.6921 - lr: 2.6268e-04
Epoch 30/60
16864/16864 [==============================] - 98s 6ms/sample - loss: 1506.7940 - lr: 2.5414e-04
Epoch 31/60
16864/16864 [==============================] - 98s 6ms/sample - loss: 1491.3829 - lr: 2.4588e-04
Epoch 32/60
16864/16864 [==============================] - 98s 6ms/sample - loss: 1472.9533 - lr: 2.3789e-04
Epoch 33/60
16864/16864 [==============================] - 98s 6ms/sample - loss: 1456.1488 - lr: 2.3016e-04
Epoch 34/60
16864/16864 [==============================] - 98s 6ms/sample - loss: 1441.9898 - lr: 2.2268e-04
Epoch 35/60
16864/16864 [==============================] - 98s 6ms/sample - loss: 1427.7318 - lr: 2.1544e-04
Epoch 36/60
16864/16864 [==============================] - 98s 6ms/sample - loss: 1415.3203 - lr: 2.0844e-04
Epoch 37/60
16864/16864 [==============================] - 98s 6ms/sample - loss: 1402.5945 - lr: 2.0167e-04
Epoch 38/60
16864/16864 [==============================] - 98s 6ms/sample - loss: 1387.2478 - lr: 1.9511e-04
Epoch 39/60
16864/16864 [==============================] - 98s 6ms/sample - loss: 1377.5018 - lr: 1.8877e-04
Epoch 40/60
16864/16864 [==============================] - 98s 6ms/sample - loss: 1364.3710 - lr: 1.8264e-04
Epoch 41/60
16864/16864 [==============================] - 98s 6ms/sample - loss: 1353.9707 - lr: 1.7670e-04
Epoch 42/60
16864/16864 [==============================] - 98s 6ms/sample - loss: 1343.4802 - lr: 1.7096e-04
Epoch 43/60
16864/16864 [==============================] - 98s 6ms/sample - loss: 1334.7965 - lr: 1.6540e-04
Epoch 44/60
16864/16864 [==============================] - 98s 6ms/sample - loss: 1325.4129 - lr: 1.6003e-04
Epoch 45/60
16864/16864 [==============================] - 98s 6ms/sample - loss: 1315.4391 - lr: 1.5483e-04
Epoch 46/60
16864/16864 [==============================] - 98s 6ms/sample - loss: 1306.9672 - lr: 1.4979e-04
Epoch 47/60
16864/16864 [==============================] - 98s 6ms/sample - loss: 1300.4252 - lr: 1.4492e-04
Epoch 48/60
16864/16864 [==============================] - 98s 6ms/sample - loss: 1293.0366 - lr: 1.4021e-04
Epoch 49/60
16864/16864 [==============================] - 98s 6ms/sample - loss: 1284.5246 - lr: 1.3566e-04
Epoch 50/60
16864/16864 [==============================] - 98s 6ms/sample - loss: 1277.3071 - lr: 1.3125e-04
Epoch 51/60
16864/16864 [==============================] - 98s 6ms/sample - loss: 1269.7002 - lr: 1.2698e-04
Epoch 52/60
16864/16864 [==============================] - 98s 6ms/sample - loss: 1263.8733 - lr: 1.2286e-04
Epoch 53/60
16864/16864 [==============================] - 98s 6ms/sample - loss: 1258.6293 - lr: 1.1886e-04
Epoch 54/60
16864/16864 [==============================] - 98s 6ms/sample - loss: 1250.5405 - lr: 1.1500e-04
Epoch 55/60
16864/16864 [==============================] - 98s 6ms/sample - loss: 1245.4476 - lr: 1.1126e-04
Epoch 56/60
16864/16864 [==============================] - 98s 6ms/sample - loss: 1240.6858 - lr: 1.0765e-04
Epoch 57/60
16864/16864 [==============================] - 98s 6ms/sample - loss: 1234.4995 - lr: 1.0415e-04
Epoch 58/60
16864/16864 [==============================] - 98s 6ms/sample - loss: 1230.6404 - lr: 1.0076e-04
Epoch 59/60
16864/16864 [==============================] - 98s 6ms/sample - loss: 1224.3601 - lr: 9.7489e-05
Epoch 60/60
16864/16864 [==============================] - 98s 6ms/sample - loss: 1220.6575 - lr: 9.7489e-05
```

**Loss 그래프**

![cvae_train_result](https://github.com/WannaBeSuperteur/AI-study/assets/32893014/63350f1e-5956-4d60-8140-653d28b88381)

**60 epochs 생성 이미지**
* Final Loss : **1220.6575**

![생성이미지](https://github.com/WannaBeSuperteur/AI-study/assets/32893014/d750f94a-4b4f-4498-8c65-74f1cfa31ba7)

**24 epochs 생성 이미지**
* Final Loss : **1606.3852**

![image](https://github.com/WannaBeSuperteur/AI-study/assets/32893014/ff82a6fa-2cf2-48db-97e2-94223029c640)

## MODEL 28
**학습 로그**
```
Epoch 1/60
2024-04-21 10:33:53.392776: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8907
2024-04-21 10:33:54.386731: W tensorflow/stream_executor/gpu/asm_compiler.cc:111] *** WARNING *** You are using ptxas 11.0.194, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.

You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.
16864/16864 [==============================] - 159s 9ms/sample - loss: 4582.4155 - lr: 6.0000e-04
Epoch 2/60
16864/16864 [==============================] - 156s 9ms/sample - loss: 3250.5305 - lr: 6.0000e-04
Epoch 3/60
16864/16864 [==============================] - 157s 9ms/sample - loss: 2988.8389 - lr: 6.0000e-04
Epoch 4/60
16864/16864 [==============================] - 157s 9ms/sample - loss: 2825.9994 - lr: 6.0000e-04
Epoch 5/60
16864/16864 [==============================] - 157s 9ms/sample - loss: 2702.4441 - lr: 5.8050e-04
Epoch 6/60
16864/16864 [==============================] - 158s 9ms/sample - loss: 2611.5923 - lr: 5.6163e-04
Epoch 7/60
16864/16864 [==============================] - 158s 9ms/sample - loss: 2521.8533 - lr: 5.4338e-04
Epoch 8/60
16864/16864 [==============================] - 158s 9ms/sample - loss: 2443.4194 - lr: 5.2572e-04
Epoch 9/60
16864/16864 [==============================] - 158s 9ms/sample - loss: 2366.0158 - lr: 5.0863e-04
Epoch 10/60
16864/16864 [==============================] - 158s 9ms/sample - loss: 2297.8629 - lr: 4.9210e-04
Epoch 11/60
16864/16864 [==============================] - 158s 9ms/sample - loss: 2216.2333 - lr: 4.7611e-04
Epoch 12/60
16864/16864 [==============================] - 158s 9ms/sample - loss: 2146.8827 - lr: 4.6064e-04
Epoch 13/60
16864/16864 [==============================] - 158s 9ms/sample - loss: 2082.7499 - lr: 4.4567e-04
Epoch 14/60
16864/16864 [==============================] - 158s 9ms/sample - loss: 2012.1959 - lr: 4.3118e-04
Epoch 15/60
16864/16864 [==============================] - 159s 9ms/sample - loss: 1953.7546 - lr: 4.1717e-04
Epoch 16/60
16864/16864 [==============================] - 160s 10ms/sample - loss: 1903.1536 - lr: 4.0361e-04
Epoch 17/60
16864/16864 [==============================] - 159s 9ms/sample - loss: 1848.5752 - lr: 3.9049e-04
Epoch 18/60
16864/16864 [==============================] - 159s 9ms/sample - loss: 1800.7940 - lr: 3.7780e-04
Epoch 19/60
16864/16864 [==============================] - 159s 9ms/sample - loss: 1754.9224 - lr: 3.6552e-04
Epoch 20/60
16864/16864 [==============================] - 159s 9ms/sample - loss: 1711.4150 - lr: 3.5364e-04
Epoch 21/60
16864/16864 [==============================] - 159s 9ms/sample - loss: 1672.0486 - lr: 3.4215e-04
Epoch 22/60
16864/16864 [==============================] - 159s 9ms/sample - loss: 1634.2881 - lr: 3.3103e-04
Epoch 23/60
16864/16864 [==============================] - 159s 9ms/sample - loss: 1602.0704 - lr: 3.2027e-04
Epoch 24/60
16864/16864 [==============================] - 160s 9ms/sample - loss: 1572.5690 - lr: 3.0986e-04
Epoch 25/60
16864/16864 [==============================] - 159s 9ms/sample - loss: 1539.0814 - lr: 2.9979e-04
Epoch 26/60
16864/16864 [==============================] - 159s 9ms/sample - loss: 1511.7593 - lr: 2.9005e-04
Epoch 27/60
16864/16864 [==============================] - 159s 9ms/sample - loss: 1485.5082 - lr: 2.8062e-04
Epoch 28/60
16864/16864 [==============================] - 160s 9ms/sample - loss: 1464.0146 - lr: 2.7150e-04
Epoch 29/60
16864/16864 [==============================] - 159s 9ms/sample - loss: 1436.3549 - lr: 2.6268e-04
Epoch 30/60
16864/16864 [==============================] - 160s 9ms/sample - loss: 1414.1416 - lr: 2.5414e-04
Epoch 31/60
16864/16864 [==============================] - 159s 9ms/sample - loss: 1391.6258 - lr: 2.4588e-04
Epoch 32/60
16864/16864 [==============================] - 159s 9ms/sample - loss: 1372.8395 - lr: 2.3789e-04
Epoch 33/60
16864/16864 [==============================] - 159s 9ms/sample - loss: 1354.2918 - lr: 2.3016e-04
Epoch 34/60
16864/16864 [==============================] - 159s 9ms/sample - loss: 1335.2689 - lr: 2.2268e-04
Epoch 35/60
16864/16864 [==============================] - 158s 9ms/sample - loss: 1317.6205 - lr: 2.1544e-04
Epoch 36/60
16864/16864 [==============================] - 159s 9ms/sample - loss: 1302.6564 - lr: 2.0844e-04
Epoch 37/60
16864/16864 [==============================] - 159s 9ms/sample - loss: 1286.9626 - lr: 2.0167e-04
Epoch 38/60
16864/16864 [==============================] - 159s 9ms/sample - loss: 1272.7377 - lr: 1.9511e-04
Epoch 39/60
16864/16864 [==============================] - 159s 9ms/sample - loss: 1258.9075 - lr: 1.8877e-04
Epoch 40/60
16864/16864 [==============================] - 160s 9ms/sample - loss: 1244.3913 - lr: 1.8264e-04
Epoch 41/60
16864/16864 [==============================] - 159s 9ms/sample - loss: 1229.9452 - lr: 1.7670e-04
Epoch 42/60
16864/16864 [==============================] - 159s 9ms/sample - loss: 1222.1500 - lr: 1.7096e-04
Epoch 43/60
16864/16864 [==============================] - 159s 9ms/sample - loss: 1207.6972 - lr: 1.6540e-04
Epoch 44/60
16864/16864 [==============================] - 159s 9ms/sample - loss: 1196.5454 - lr: 1.6003e-04
Epoch 45/60
16864/16864 [==============================] - 159s 9ms/sample - loss: 1185.1895 - lr: 1.5483e-04
Epoch 46/60
16864/16864 [==============================] - 158s 9ms/sample - loss: 1176.2498 - lr: 1.4979e-04
Epoch 47/60
16864/16864 [==============================] - 159s 9ms/sample - loss: 1167.2738 - lr: 1.4492e-04
Epoch 48/60
16864/16864 [==============================] - 158s 9ms/sample - loss: 1159.3958 - lr: 1.4021e-04
Epoch 49/60
16864/16864 [==============================] - 159s 9ms/sample - loss: 1149.6307 - lr: 1.3566e-04
Epoch 50/60
16864/16864 [==============================] - 158s 9ms/sample - loss: 1140.8626 - lr: 1.3125e-04
Epoch 51/60
16864/16864 [==============================] - 159s 9ms/sample - loss: 1133.0214 - lr: 1.2698e-04
Epoch 52/60
16864/16864 [==============================] - 159s 9ms/sample - loss: 1125.1854 - lr: 1.2286e-04
Epoch 53/60
16864/16864 [==============================] - 159s 9ms/sample - loss: 1117.7048 - lr: 1.1886e-04
Epoch 54/60
16864/16864 [==============================] - 159s 9ms/sample - loss: 1110.1573 - lr: 1.1500e-04
Epoch 55/60
16864/16864 [==============================] - 159s 9ms/sample - loss: 1103.5845 - lr: 1.1126e-04
Epoch 56/60
16864/16864 [==============================] - 159s 9ms/sample - loss: 1097.7344 - lr: 1.0765e-04
Epoch 57/60
16864/16864 [==============================] - 159s 9ms/sample - loss: 1093.1725 - lr: 1.0415e-04
Epoch 58/60
16864/16864 [==============================] - 158s 9ms/sample - loss: 1085.1602 - lr: 1.0076e-04
Epoch 59/60
16864/16864 [==============================] - 158s 9ms/sample - loss: 1079.1429 - lr: 9.7489e-05
Epoch 60/60
16864/16864 [==============================] - 158s 9ms/sample - loss: 1075.6455 - lr: 9.7489e-05
```

**Loss 그래프**

![cvae_train_result](https://github.com/WannaBeSuperteur/AI-study/assets/32893014/a26e2fde-0032-4325-91f6-97c3db036684)

**60 epochs 생성 이미지**
* Final Loss : **1075.6455**

![생성이미지](https://github.com/WannaBeSuperteur/AI-study/assets/32893014/6a6fb353-d697-47af-a112-2e2320a3431e)

**24 epochs 생성 이미지**
* Final Loss : **1499.3069**

![image](https://github.com/WannaBeSuperteur/AI-study/assets/32893014/e5c6ed7a-085d-48f3-910f-be68faf68a3c)

## MODEL 29 info change 실험 결과

* 실험 결론 : 다음의 info change에 대해 모두 **의도한 대로 이미지가 잘 생성된다.**
  * 머리 색 (```hair_color```) 변경
  * 입을 벌린 정도 (```mouth```) 변경
  * 눈을 뜬 정도 (```eyes```) 변경
  * 성별 (```male_prob```, ```female_prob```) 변경
* 아래 캡처에서 선택한 이미지는 **여성 가상인간으로 사용하기에 큰 무리가 없는 이미지**

![image](https://github.com/WannaBeSuperteur/AI-study/assets/32893014/ecc5daa3-e436-473b-a8a8-1e4321934d1f)